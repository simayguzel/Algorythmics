title,authors,doi,abstract,categories,keywords,journal,volume,references,figures,tables,year
"Performance, workload, and usability in a multiscreen, multi-device, information-rich environment","Jason J., Saleem; Dustin T., Weiler",10.7717/peerj-cs.162,"Potential benefits of multiscreen and multiple device environments were assessed using three different computing environments. A single factor, within-subject study was conducted with 18 engineering students in a laboratory experiment. Three levels for the computing environment factor included one with a desktop computer with a single monitor (control, condition A); one with a desktop with dual monitors, as well as a single tablet computer (condition B); and one with a desktop with a single monitor, as well as two tablet computers (condition C). There was no statistically significant difference in efficiency or workload when completing scenarios for the three computing environments. However, a dual monitor desktop with a single tablet computer (B) was the ideal computing environment for the information-rich engineering problem given to participants, supported by significantly fewer errors compared to condition C and significantly higher usability ratings compared to conditions A and C. A single desktop monitor with two tablet computers (C) did not provide any advantage compared to a single desktop monitor (A).",Human-Computer Interaction; Mobile and Ubiquitous Computing,Multiscreen; Dual monitors; Tablet computers; Human-computer interaction,PeerJ Computer Science,4,27,1,3,2018
Large-scale comparative visualisation of sets of multidimensional data,"Dany, Vohl; David G., Barnes; Christopher J., Fluke; Govinda, Poudel; Nellie, Georgiou-Karistianis; Amr H., Hassan; Yuri, Benovitski; Tsz Ho, Wong; Owen L., Kaluza; Toan D., Nguyen; C. Paul, Bonnington",10.7717/peerj-cs.88,"We present encube—a qualitative, quantitative and comparative visualisation and analysis system, with application to high-resolution, immersive three-dimensional environments and desktop displays. encube extends previous comparative visualisation systems by considering: (1) the integration of comparative visualisation and analysis into a unified system; (2) the documentation of the discovery process; and (3) an approach that enables scientists to continue the research process once back at their desktop. Our solution enables tablets, smartphones or laptops to be used as interaction units for manipulating, organising, and querying data. We highlight the modularity of encube, allowing additional functionalities to be included as required. Additionally, our approach supports a high level of collaboration within the physical environment. We show how our implementation of encube operates in a large-scale, hybrid visualisation and supercomputing environment using the CAVE2 at Monash University, and on a local desktop, making it a versatile solution. We discuss how our approach can help accelerate the discovery rate in a variety of research scenarios.",Graphics; Scientific Computing and Simulation; Visual Analytics,Scientific data visualisation; Tiled-display; Comparative Visualisation; Immersive environments; CAVE2,PeerJ Computer Science,2,84,11,5,2016
Incorporating popularity in a personalized news recommender system,"Nirmal, Jonnalagedda; Susan, Gauch; Kevin, Labille; Sultan, Alfarhood",10.7717/peerj-cs.63,"Online news reading has become a widely popular way to read news articles from news sources around the globe. With the enormous amount of news articles available, users are easily overwhelmed by information of little interest to them. News recommender systems help users manage this flood by recommending articles based on user interests rather than presenting articles in order of their occurrence. We present our research on developing personalized news recommendation system with the help of a popular micro-blogging service, “Twitter.” News articles are ranked based on the popularity of the article identified from Twitter’s public timeline. In addition, users construct profiles based on their interests and news articles are also ranked based on their match to the user profile. By integrating these two approaches, we present a hybrid news recommendation model that recommends interesting news articles to the user based on their popularity as well as their relevance to the user profile.",Agents and Multi-Agent Systems; World Wide Web and Web Science,Twitter; Personalized news recommendation; News recommender systems; User profile,PeerJ Computer Science,2,29,6,8,2016
Uncertainty modeling process for semantic technology,"Rommel N., Carvalho; Kathryn B., Laskey; Paulo C.G. Da, Costa",10.7717/peerj-cs.77,"The ubiquity of uncertainty across application domains generates a need for principled support for uncertainty management in semantically aware systems. A probabilistic ontology provides constructs for representing uncertainty in domain ontologies. While the literature has been growing on formalisms for representing uncertainty in ontologies, there remains little guidance in the knowledge engineering literature for how to design probabilistic ontologies. To address the gap, this paper presents the Uncertainty Modeling Process for Semantic Technology (UMP-ST), a new methodology for modeling probabilistic ontologies. To explain how the methodology works and to verify that it can be applied to different scenarios, this paper describes step-by-step the construction of a proof-of-concept probabilistic ontology. The resulting domain model can be used to support identification of fraud in public procurements in Brazil. While the case study illustrates the development of a probabilistic ontology in the PR-OWL probabilistic ontology language, the methodology is applicable to any ontology formalism that properly integrates uncertainty with domain semantics.",Artificial Intelligence; World Wide Web and Web Science; Software Engineering,PR-OWL; MEBN; UP; Methodology; UMP-ST; Semantic Web; Bayesian networks; Uncertainty; Modeling; Semantic technology,PeerJ Computer Science,2,78,10,2,2016
Complexity curve: a graphical measure of data complexity and classifier performance,"Julian, Zubek; Dariusz M., Plewczynski",10.7717/peerj-cs.76,"We describe a method for assessing data set complexity based on the estimation of the underlining probability distribution and Hellinger distance. In contrast to some popular complexity measures, it is not focused on the shape of a decision boundary in a classification task but on the amount of available data with respect to the attribute structure. Complexity is expressed in terms of graphical plot, which we call complexity curve. It demonstrates the relative increase of available information with the growth of sample size. We perform theoretical and experimental examination of properties of the introduced complexity measure and show its relation to the variance component of classification error. We then compare it with popular data complexity measures on 81 diverse data sets and show that it can contribute to explaining performance of specific classifiers on these sets. We also apply our methodology to a panel of simple benchmark data sets, demonstrating how it can be used in practice to gain insights into data characteristics. Moreover, we show that the complexity curve is an effective tool for reducing the size of the training set (data pruning), allowing to significantly speed up the learning process without compromising classification accuracy. The associated code is available to download at: https://github.com/zubekj/complexity_curve (open source Python implementation).",Algorithms and Analysis of Algorithms; Artificial Intelligence; Data Mining and Machine Learning,Learning curves; Data complexity; Data pruning; Hellinger distance; Bias-variance decomposition; Performance measures,PeerJ Computer Science,2,31,9,5,2016
Software process improvement: a systematic mapping study on the state of the art,"Marco, Kuhrmann; Philipp, Diebold; Jürgen, Münch",10.7717/peerj-cs.62,"Software process improvement (SPI) has been around for decades: frameworks are proposed, success factors are studied, and experiences have been reported. However, the sheer mass of concepts, approaches, and standards published over the years overwhelms practitioners as well as researchers. What is out there? Are there new trends and emerging approaches? What are open issues? Still, we struggle to answer these questions about the current state of SPI and related research. In this article, we present results from an updated systematic mapping study to shed light on the field of SPI, to develop a big picture of the state of the art, and to draw conclusions for future research directions. An analysis of 769 publications draws a big picture of SPI-related research of the past quarter-century. Our study shows a high number of solution proposals, experience reports, and secondary studies, but only few theories and models on SPI in general. In particular, standard SPI models like CMMI and ISO/IEC 15,504 are analyzed, enhanced, and evaluated for applicability in practice, but these standards are also critically discussed, e.g., from the perspective of SPI in small-to-medium-sized companies, which leads to new specialized frameworks. New and specialized frameworks account for the majority of the contributions found (approx. 38%). Furthermore, we find a growing interest in success factors (approx. 16%) to aid companies in conducting SPI and in adapting agile principles and practices for SPI (approx. 10%). Beyond these specific topics, the study results also show an increasing interest into secondary studies with the purpose of aggregating and structuring SPI-related knowledge. Finally, the present study helps directing future research by identifying under-researched topics awaiting further investigation.",Software Engineering,SPI; Software process; Systematic mapping study; Software process improvement,PeerJ Computer Science,2,47,16,8,2016
TCP adaptation with network coding and opportunistic data forwarding in multi-hop wireless networks,"Chen, Zhang; Yuanzhu, Chen; Cheng, Li",10.7717/peerj-cs.89,"Opportunistic data forwarding significantly increases the throughput in multi-hop wireless mesh networks by utilizing the broadcast nature of wireless transmissions and the fluctuation of link qualities. Network coding strengthens the robustness of data transmissions over unreliable wireless links. However, opportunistic data forwarding and network coding are rarely incorporated with TCP because the frequent occurrences of out-of-order packets in opportunistic data forwarding and long decoding delay in network coding overthrow TCP’s congestion control. In this paper, we propose a solution dubbed TCPFender, which supports opportunistic data forwarding and network coding in TCP. Our solution adds an adaptation layer to mask the packet loss caused by wireless link errors and provides early positive feedbacks to trigger a larger congestion window for TCP. This adaptation layer functions over the network layer and reduces the delay of ACKs for each coded packet. The simulation results show that TCPFender significantly outperforms TCP/IP in terms of the network throughput in different topologies of wireless networks.",Computer Networks and Communications; Network Science and Online Social Networks,TCP; Network coding; Opportunistic data forwarding; Multi-hop wireless networks,PeerJ Computer Science,2,32,11,1,2016
Towards computational reproducibility: researcher perspectives on the use and sharing of software,"Yasmin, AlNoamany; John A., Borghi",10.7717/peerj-cs.163,"Research software, which includes both source code and executables used as part of the research process, presents a significant challenge for efforts aimed at ensuring reproducibility. In order to inform such efforts, we conducted a survey to better understand the characteristics of research software as well as how it is created, used, and shared by researchers. Based on the responses of 215 participants, representing a range of research disciplines, we found that researchers create, use, and share software in a wide variety of forms for a wide variety of purposes, including data collection, data analysis, data visualization, data cleaning and organization, and automation. More participants indicated that they use open source software than commercial software. While a relatively small number of programming languages (e.g., Python, R, JavaScript, C++, MATLAB) are used by a large number, there is a long tail of languages used by relatively few. Between-group comparisons revealed that significantly more participants from computer science write source code and create executables than participants from other disciplines. Differences between researchers from computer science and other disciplines related to the knowledge of best practices of software creation and sharing were not statistically significant. While many participants indicated that they draw a distinction between the sharing and preservation of software, related practices and perceptions were often not aligned with those of the broader scholarly communications community.",Digital Libraries,Software sustainability; Reproducibility; Research software; Code; Finding software; Sharing software,PeerJ Computer Science,4,68,9,2,2018
An integrated platform for intuitive mathematical programming modeling using LaTeX,"Charalampos P., Triantafyllidis; Lazaros G., Papageorgiou",10.7717/peerj-cs.161,"This paper presents a novel prototype platform that uses the same LaTeX mark-up language, commonly used to typeset mathematical content, as an input language for modeling optimization problems of various classes. The platform converts the LaTeX model into a formal Algebraic Modeling Language (AML) representation based on Pyomo through a parsing engine written in Python and solves by either via NEOS server or locally installed solvers, using a friendly Graphical User Interface (GUI). The distinct advantages of our approach can be summarized in (i) simplification and speed-up of the model design and development process (ii) non-commercial character (iii) cross-platform support (iv) easier typo and logic error detection in the description of the models and (v) minimization of working knowledge of programming and AMLs to perform mathematical programming modeling. Overall, this is a presentation of a complete workable scheme on using LaTeX for mathematical programming modeling which assists in furthering our ability to reproduce and replicate scientific work.",Optimization Theory and Computation; Scientific Computing and Simulation; Programming Languages; Software Engineering,Pyomo; Python; Algebraic Modeling Languages; Mathematical programming; Optimization; LaTeX,PeerJ Computer Science,4,31,4,0,2018
AutoWIG: automatic generation of python bindings for C++ libraries,"Pierre, Fernique; Christophe, Pradal",10.7717/peerj-cs.149,"Most of Python and R scientific packages incorporate compiled scientific libraries to speed up the code and reuse legacy libraries. While several semi-automatic solutions exist to wrap these compiled libraries, the process of wrapping a large library is cumbersome and time consuming. In this paper, we introduce AutoWIG, a Python package that wraps automatically compiled libraries into high-level languages using LLVM/Clang technologies and the Mako templating engine. Our approach is automatic, extensible, and applies to complex C++ libraries, composed of thousands of classes or incorporating modern meta-programming constructs.",Data Science; Scientific Computing and Simulation; Programming Languages; Software Engineering,C++; Python; Automatic bindings generation,PeerJ Computer Science,4,36,5,3,2018
A computational framework for colour metrics and colour space transforms,"Ivar, Farup",10.7717/peerj-cs.48,"An object-oriented computational framework for the transformation of colour data and colour metric tensors is presented. The main idea of the design is to represent the transforms between spaces as compositions of objects from a class hierarchy providing the methods for both the transforms themselves and the corresponding Jacobian matrices. In this way, new colour spaces can be implemented on the fly by transforming from any existing colour space, and colour data in various formats as well as colour metric tensors and colour difference data can easily be transformed between the colour spaces. This reduces what normally requires several days of coding to a few lines of code without introducing a significant computational overhead. The framework is implemented in the Python programming language.",Computer Vision; Graphics; Optimization Theory and Computation; Scientific Computing and Simulation; Software Engineering,Colour metrics; Colour space; Transform; Object-oriented; Python,PeerJ Computer Science,2,22,8,0,2016
A software for parameter optimization with Differential Evolution Entirely Parallel method,"Konstantin, Kozlov; Alexander M., Samsonov; Maria, Samsonova",10.7717/peerj-cs.74,"Summary. Differential Evolution Entirely Parallel (DEEP) package is a software for finding unknown real and integer parameters in dynamical models of biological processes by minimizing one or even several objective functions that measure the deviation of model solution from data. Numerical solutions provided by the most efficient global optimization methods are often problem-specific and cannot be easily adapted to other tasks. In contrast, DEEP allows a user to describe both mathematical model and objective function in any programming language, such as R, Octave or Python and others. Being implemented in C, DEEP demonstrates as good performance as the top three methods from CEC-2014 (Competition on evolutionary computation) benchmark and was successfully applied to several biological problems.Availability. DEEP method is an open source and free software distributed under the terms of GPL licence version 3. The sources are available at http://deepmethod.sourceforge.net/ and binary packages for Fedora GNU/Linux are provided for RPM package manager at https://build.opensuse.org/project/repositories/home:mackoel:compbio.",Computational Biology; Distributed and Parallel Computing; Optimization Theory and Computation,Differential Evolution; Parameter optimization; Mathematical modeling; Parallelization; Bioinformatics; Open source software,PeerJ Computer Science,2,40,3,1,2016
Toward physiological indices of emotional state driving future ebook interactivity,"Jan B.F., van Erp; Maarten A., Hogervorst; Ysbrand D., van der Werf",10.7717/peerj-cs.60,"Ebooks of the future may respond to the emotional experience of the reader. (Neuro-) physiological measures could capture a reader’s emotional state and use this to enhance the reading experience by adding matching sounds or to change the storyline therewith creating a hybrid art form in between literature and gaming. We describe the theoretical foundation of the emotional and creative brain and review the neurophysiological indices that can be used to drive future ebook interactivity in a real life situation. As a case study, we report the neurophysiological measurements of a bestselling author during nine days of writing which can potentially be used later to compare them to those of the readers. In designated calibration blocks, the artist wrote emotional paragraphs for emotional (IAPS) pictures. Analyses showed that we can reliably distinguish writing blocks from resting but we found no reliable differences related to the emotional content of the writing. The study shows that measurements of EEG, heart rate (variability), skin conductance, facial expression and subjective ratings can be done over several hours a day and for several days in a row. In follow-up phases, we will measure 300 readers with a similar setup.",Brain-Computer Interface; Human-Computer Interaction; Multimedia,Creativity; Reading; Emotion; Neurophysiology; Brain–computer interfaces; Ebook; Interactivity; EEG; Multimedia; Human–computer interaction,PeerJ Computer Science,2,86,5,2,2016
BioAssay templates for the semantic web,"Alex M., Clark; Nadia K., Litterman; Janice E., Kranz; Peter, Gund; Kellan, Gregory; Barry A., Bunin",10.7717/peerj-cs.61,"Annotation of bioassay protocols using semantic web vocabulary is a way to make experiment descriptions machine-readable. Protocols are communicated using concise scientific English, which precludes most kinds of analysis by software algorithms. Given the availability of a sufficiently expressive ontology, some or all of the pertinent information can be captured by asserting a series of facts, expressed as semantic web triples (subject, predicate, object). With appropriate annotation, assays can be searched, clustered, tagged and evaluated in a multitude of ways, analogous to other segments of drug discovery informatics. The BioAssay Ontology (BAO) has been previously designed for this express purpose, and provides a layered hierarchy of meaningful terms which can be linked to. Currently the biggest challenge is the issue of content creation: scientists cannot be expected to use the BAO effectively without having access to software tools that make it straightforward to use the vocabulary in a canonical way. We have sought to remove this barrier by: (1) defining a BioAssay Template (BAT) data model; (2) creating a software tool for experts to create or modify templates to suit their needs; and (3) designing a common assay template (CAT) to leverage the most value from the BAO terms. The CAT was carefully assembled by biologists in order to find a balance between the maximum amount of information captured vs. low degrees of freedom in order to keep the user experience as simple as possible. The data format that we use for describing templates and corresponding annotations is the native format of the semantic web (RDF triples), and we demonstrate some of the ways that generated content can be meaningfully queried using the SPARQL language. We have made all of these materials available as open source (http://github.com/cdd/bioassay-template), in order to encourage community input and use within diverse projects, including but not limited to our own commercial electronic lab notebook products.",Bioinformatics; Computational Biology; Human-Computer Interaction; Data Science,Assay protocols; Semantic web; BioAssay Ontology; Common Assay Template; Machine learning,PeerJ Computer Science,2,18,11,1,2016
Are you ashamed? Can a gaze tracker tell?,"Rytis, Maskeliunas; Vidas, Raudonis",10.7717/peerj-cs.75,"Our aim was to determine the possibility of detecting cognitive emotion information (neutral, disgust, shameful, “sensory pleasure”) by using a remote eye tracker within an approximate range of 1 meter. Our implementation was based on a self-learning ANN used for profile building, emotion status identification and recognition. Participants of the experiment were provoked with audiovisual stimuli (videos with sounds) to measure the emotional feedback. The proposed system was able to classify each felt emotion with an average of 90% accuracy (2 second measuring interval).",Human-Computer Interaction; Artificial Intelligence; Computer Vision,Cognitive; Recognition; Emotions; Gaze-tracking,PeerJ Computer Science,2,35,12,4,2016
How are functionally similar code clones syntactically different? An empirical study and a benchmark,"Stefan, Wagner; Asim, Abdulkhaleq; Ivan, Bogicevic; Jan-Peter, Ostberg; Jasmin, Ramadani",10.7717/peerj-cs.49,"Background. Today, redundancy in source code, so-called “clones” caused by copy&paste can be found reliably using clone detection tools. Redundancy can arise also independently, however, not caused by copy&paste. At present, it is not clear how only functionally similar clones (FSC) differ from clones created by copy&paste. Our aim is to understand and categorise the syntactical differences in FSCs that distinguish them from copy&paste clones in a way that helps clone detection research.Methods. We conducted an experiment using known functionally similar programs in Java and C from coding contests. We analysed syntactic similarity with traditional detection tools and explored whether concolic clone detection can go beyond syntax. We ran all tools on 2,800 programs and manually categorised the differences in a random sample of 70 program pairs.Results. We found no FSCs where complete files were syntactically similar. We could detect a syntactic similarity in a part of the files in <16% of the program pairs. Concolic detection found 1 of the FSCs. The differences between program pairs were in the categories algorithm, data structure, OO design, I/O and libraries. We selected 58 pairs for an openly accessible benchmark representing these categories.Discussion. The majority of differences between functionally similar clones are beyond the capabilities of current clone detection approaches. Yet, our benchmark can help to drive further clone detection research.",Programming Languages; Software Engineering,Code clone; Functionally similar clone; Empirical study; Benchmark,PeerJ Computer Science,2,33,4,12,2016
Indexing labeled sequences,"Tatiana, Rocher; Mathieu, Giraud; Mikaël, Salson",10.7717/peerj-cs.148,"Labels are a way to add some information on a text, such as functional annotations such as genes on a DNA sequences. V(D)J recombinations are DNA recombinations involving two or three short genes in lymphocytes. Sequencing this short region (500 bp or less) produces labeled sequences and brings insight in the lymphocyte repertoire for onco-hematology or immunology studies.We present two indexes for a text with non-overlapping labels. They store the text in a Burrows–Wheeler transform (BWT) and a compressed label sequence in a Wavelet Tree. The label sequence is taken in the order of the text (TL-index) or in the order of the BWT (TLBW-index). Both indexes need a space related to the entropy of the labeled text.These indexes allow efficient text–label queries to count and find labeled patterns. The TLBW-index has an overhead on simple label queries but is very efficient on combined pattern–label queries. We implemented the indexes in C++ and compared them against a baseline solution on pseudo-random as well as on V(D)J labeled texts.New indexes such as the ones we proposed improve the way we index and query labeled texts as, for instance, lymphocyte repertoire for hematological and immunological studies.",Bioinformatics; Computational Biology; Algorithms and Analysis of Algorithms,Data structures; Text indexing; Burrows–Wheeler transform; Wavelet Tree; V(D)J recombination,PeerJ Computer Science,4,21,8,3,2018
Parallelisation of equation-based simulation programs on heterogeneous computing systems,"Dragan D., Nikolić",10.7717/peerj-cs.160,"Numerical solutions of equation-based simulations require computationally intensive tasks such as evaluation of model equations, linear algebra operations and solution of systems of linear equations. The focus in this work is on parallel evaluation of model equations on shared memory systems such as general purpose processors (multi-core CPUs and manycore devices), streaming processors (Graphics Processing Units and Field Programmable Gate Arrays) and heterogeneous systems. The current approaches for evaluation of model equations are reviewed and their capabilities and shortcomings analysed. Since stream computing differs from traditional computing in that the system processes a sequential stream of elements, equations must be transformed into a data structure suitable for both types. The postfix notation expression stacks are recognised as a platform and programming language independent method to describe, store in computer memory and evaluate general systems of differential and algebraic equations of any size. Each mathematical operation and its operands are described by a specially designed data structure, and every equation is transformed into an array of these structures (a Compute Stack). Compute Stacks are evaluated by a stack machine using a Last In First Out queue. The stack machine is implemented in the DAE Tools modelling software in the C99 language using two Application Programming Interface (APIs)/frameworks for parallelism. The Open Multi-Processing (OpenMP) API is used for parallelisation on general purpose processors, and the Open Computing Language (OpenCL) framework is used for parallelisation on streaming processors and heterogeneous systems. The performance of the sequential Compute Stack approach is compared to the direct C++ implementation and to the previous approach that uses evaluation trees. The new approach is 45% slower than the C++ implementation and more than five times faster than the previous one. The OpenMP and OpenCL implementations are tested on three medium-scale models using a multi-core CPU, a discrete GPU, an integrated GPU and heterogeneous computing setups. Execution times are compared and analysed and the advantages of the OpenCL implementation running on a discrete GPU and heterogeneous systems are discussed. It is found that the evaluation of model equations using the parallel OpenCL implementation running on a discrete GPU is up to twelve times faster than the sequential version while the overall simulation speed-up gained is more than three times.",Distributed and Parallel Computing; Scientific Computing and Simulation,Modelling; Simulation; Heterogeneous computing; Parallel computing; Differential-algebraic equations; Equation-based; Streaming processors; OpenCL; OpenMP,PeerJ Computer Science,4,18,6,17,2018
Verifiability in computer-aided research: the role of digital scientific notations at the human-computer interface,"Konrad, Hinsen",10.7717/peerj-cs.158,"Most of today’s scientific research relies on computers and software for processing scientific information. Examples of such computer-aided research are the analysis of experimental data or the simulation of phenomena based on theoretical models. With the rapid increase of computational power, scientific software has integrated more and more complex scientific knowledge in a black-box fashion. As a consequence, its users do not know, and do not even have a chance of finding out, which assumptions and approximations their computations are based on. This black-box nature of scientific software has made the verification of much computer-aided research close to impossible. The present work starts with an analysis of this situation from the point of view of human-computer interaction in scientific research. It identifies the key role of digital scientific notations at the human-computer interface, reviews the most popular ones in use today, and describes a proof-of-concept implementation of Leibniz, a language designed as a verifiable digital scientific notation for models formulated as mathematical equations.",Human-Computer Interaction; Digital Libraries; Scientific Computing and Simulation,Digital scientific notations; Human-computer interaction; Computational science; Verification; Validation; Computational documents,PeerJ Computer Science,4,49,6,0,2018
"20 GB in 10 minutes: a case for linking major biodiversity databases using an open socio-technical infrastructure and a pragmatic, cross-institutional collaboration","Anne E., Thessen; Jorrit H., Poelen; Matthew, Collins; Jen, Hammock",10.7717/peerj-cs.164,"Biodiversity information is made available through numerous databases that each have their own data models, web services, and data types. Combining data across databases leads to new insights, but is not easy because each database uses its own system of identifiers. In the absence of stable and interoperable identifiers, databases are often linked using taxonomic names. This labor intensive, error prone, and lengthy process relies on accessible versions of nomenclatural authorities and fuzzy-matching algorithms. To approach the challenge of linking diverse data, more than technology is needed. New social collaborations like the Global Unified Open Data Architecture (GUODA) that combines skills from diverse groups of computer engineers from iDigBio, server resources from the Advanced Computing and Information Systems (ACIS) Lab, global-scale data presentation from EOL, and independent developers and researchers are what is needed to make concrete progress on finding relationships between biodiversity datasets. This paper will discuss a technical solution developed by the GUODA collaboration for faster linking across databases with a use case linking Wikidata and the Global Biotic Interactions database (GloBI). The GUODA infrastructure is a 12-node, high performance computing cluster made up of about 192 threads with 12 TB of storage and 288 GB memory. Using GUODA, 20 GB of compressed JSON from Wikidata was processed and linked to GloBI in about 10–11 min. Instead of comparing name strings or relying on a single identifier, Wikidata and GloBI were linked by comparing graphs of biodiversity identifiers external to each system. This method resulted in adding 119,957 Wikidata links in GloBI, an increase of 13.7% of all outgoing name links in GloBI. Wikidata and GloBI were compared to Open Tree of Life Reference Taxonomy to examine consistency and coverage. The process of parsing Wikidata, Open Tree of Life Reference Taxonomy and GloBI archives and calculating consistency metrics was done in minutes on the GUODA platform. As a model collaboration, GUODA has the potential to revolutionize biodiversity science by bringing diverse technically minded people together with high performance computing resources that are accessible from a laptop or desktop. However, participating in such a collaboration still requires basic programming skills.",Bioinformatics; Databases,Biodiversity; Collaboration; Identifiers; Wikidata; Graph; Linking,PeerJ Computer Science,4,25,6,2,2018
Information theoretic alignment free variant calling,"Justin, Bedo; Benjamin, Goudey; Jeremy, Wazny; Zeyu, Zhou",10.7717/peerj-cs.71,"While traditional methods for calling variants across whole genome sequence data rely on alignment to an appropriate reference sequence, alternative techniques are needed when a suitable reference does not exist. We present a novel alignment and assembly free variant calling method based on information theoretic principles designed to detect variants have strong statistical evidence for their ability to segregate samples in a given dataset. Our method uses the context surrounding a particular nucleotide to define variants. Given a set of reads, we model the probability of observing a given nucleotide conditioned on the surrounding prefix and suffixes of length k as a multinomial distribution. We then estimate which of these contexts are stable intra-sample and varying inter-sample using a statistic based on the Kullback–Leibler divergence.The utility of the variant calling method was evaluated through analysis of a pair of bacterial datasets and a mouse dataset. We found that our variants are highly informative for supervised learning tasks with performance similar to standard reference based calls and another reference free method (DiscoSNP++). Comparisons against reference based calls showed our method was able to capture very similar population structure on the bacterial dataset. The algorithm’s focus on discriminatory variants makes it suitable for many common analysis tasks for organisms that are too diverse to be mapped back to a single reference sequence.",Bioinformatics; Computational Biology,Alignment free; Variant; Assembly free; Genome; Acteria; Feature extraction,PeerJ Computer Science,2,17,5,3,2016
Fuzzy based binary feature profiling for modus operandi analysis,"Mahawaga Arachchige Pathum, Chamikara; Akalanka, Galappaththi; Roshan Dharshana, Yapa; Ruwan Dharshana, Nawarathna; Saluka Ranasinghe, Kodituwakku; Jagath, Gunatilake; Aththanapola Arachchilage Chathranee Anumitha, Jayathilake; Liwan, Liyanage",10.7717/peerj-cs.65,"It is a well-known fact that some criminals follow perpetual methods of operations known as modi operandi. Modus operandi is a commonly used term to describe the habits in committing crimes. These modi operandi are used in relating criminals to crimes for which the suspects have not yet been recognized. This paper presents the design, implementation and evaluation of a new method to find connections between crimes and criminals using modi operandi. The method involves generating a feature matrix for a particular criminal based on the flow of events of his/her previous convictions. Then, based on the feature matrix, two representative modi operandi are generated: complete modus operandi and dynamic modus operandi. These two representative modi operandi are compared with the flow of events of the crime at hand, in order to generate two other outputs: completeness probability (CP) and deviation probability (DP). CP and DP are used as inputs to a fuzzy inference system to generate a score which is used in providing a measurement for the similarity between the suspect and the crime at hand. The method was evaluated using actual crime data and ten other open data sets. In addition, comparison with nine other classification algorithms showed that the proposed method performs competitively with other related methods proving that the performance of the new method is at an acceptable level.",Algorithms and Analysis of Algorithms; Artificial Intelligence; Data Mining and Machine Learning,Modus operandi analysis; Fuzzy inference systems; Binary feature analysis; Classification; Association rule mining,PeerJ Computer Science,2,72,15,12,2016
A technology prototype system for rating therapist empathy from audio recordings in addiction counseling,"Bo, Xiao; Chewei, Huang; Zac E., Imel; David C., Atkins; Panayiotis, Georgiou; Shrikanth S., Narayanan",10.7717/peerj-cs.59,"Scaling up psychotherapy services such as for addiction counseling is a critical societal need. One challenge is ensuring quality of therapy, due to the heavy cost of manual observational assessment. This work proposes a speech technology-based system to automate the assessment of therapist empathy—a key therapy quality index—from audio recordings of the psychotherapy interactions. We designed a speech processing system that includes voice activity detection and diarization modules, and an automatic speech recognizer plus a speaker role matching module to extract the therapist’s language cues. We employed Maximum Entropy models, Maximum Likelihood language models, and a Lattice Rescoring method to characterize high vs. low empathic language. We estimated therapy-session level empathy codes using utterance level evidence obtained from these models. Our experiments showed that the fully automated system achieved a correlation of 0.643 between expert annotated empathy codes and machine-derived estimations, and an accuracy of 81% in classifying high vs. low empathy, in comparison to a 0.721 correlation and 86% accuracy in the oracle setting using manual transcripts. The results show that the system provides useful information that can contribute to automatic quality insurance and therapist training.",Computational Linguistics; Emerging Technologies; Human-Computer Interaction; Natural Language and Speech,Prototype system; Speech processing; Language modeling; Addiction counseling; Empathy modeling; Automatic speech recognition,PeerJ Computer Science,2,47,3,12,2016
Matrix Depot: an extensible test matrix collection for Julia,"Weijian, Zhang; Nicholas J., Higham",10.7717/peerj-cs.58,"Matrix Depot is a Julia software package that provides easy access to a large and diverse collection of test matrices. Its novelty is threefold. First, it is extensible by the user, and so can be adapted to include the user’s own test problems. In doing so, it facilitates experimentation and makes it easier to carry out reproducible research. Second, it amalgamates in a single framework two different types of existing matrix collections, comprising parametrized test matrices (including Hansen’s set of regularization test problems and Higham’s Test Matrix Toolbox) and real-life sparse matrix data (giving access to the University of Florida sparse matrix collection). Third, it fully exploits the Julia language. It uses multiple dispatch to help provide a simple interface and, in particular, to allow matrices to be generated in any of the numeric data types supported by the language.",Algorithms and Analysis of Algorithms; Data Science; Scientific Computing and Simulation,Julia; Software package; Test matrices; Matrix algorithm.; Test problems,PeerJ Computer Science,2,24,1,1,2016
Enriching scientific publications with interactive 3D PDF: an integrated toolbox for creating ready-to-publish figures,"Axel, Newe",10.7717/peerj-cs.64,"Three-dimensional (3D) data of many kinds is produced at an increasing rate throughout all scientific disciplines. The Portable Document Format (PDF) is the de-facto standard for the exchange of electronic documents and allows for embedding three-dimensional models. Therefore, it is a well-suited medium for the visualization and the publication of this kind of data. The generation of the appropriate files has been cumbersome so far. This article presents the first release of a software toolbox which integrates the complete workflow for generating 3D model files and ready-to-publish 3D PDF documents for scholarly publications in a consolidated working environment. It can be used out-of-the-box as a simple working tool or as a basis for specifically tailored solutions. A comprehensive documentation, an example project and a project wizard facilitate the customization. It is available royalty-free and for Windows, MacOS and Linux.",Computer Vision; Data Science; Digital Libraries; Emerging Technologies; Scientific Computing and Simulation,Portable Document Format; 3D-PDF; PDF; U3D; Universal 3D; Application,PeerJ Computer Science,2,38,5,4,2016
Audio segmentation using Flattened Local Trimmed Range for ecological acoustic space analysis,"Giovany, Vega; Carlos J., Corrada-Bravo; T. Mitchell, Aide",10.7717/peerj-cs.70,"The acoustic space in a given environment is filled with footprints arising from three processes: biophony, geophony and anthrophony. Bioacoustic research using passive acoustic sensors can result in thousands of recordings. An important component of processing these recordings is to automate signal detection. In this paper, we describe a new spectrogram-based approach for extracting individual audio events. Spectrogram-based audio event detection (AED) relies on separating the spectrogram into background (i.e., noise) and foreground (i.e., signal) classes using a threshold such as a global threshold, a per-band threshold, or one given by a classifier. These methods are either too sensitive to noise, designed for an individual species, or require prior training data. Our goal is to develop an algorithm that is not sensitive to noise, does not need any prior training data and works with any type of audio event. To do this, we propose: (1) a spectrogram filtering method, the Flattened Local Trimmed Range (FLTR) method, which models the spectrogram as a mixture of stationary and non-stationary energy processes and mitigates the effect of the stationary processes, and (2) an unsupervised algorithm that uses the filter to detect audio events. We measured the performance of the algorithm using a set of six thoroughly validated audio recordings and obtained a sensitivity of 94% and a positive predictive value of 89%. These sensitivity and positive predictive values are very high, given that the validated recordings are diverse and obtained from field conditions. The algorithm was then used to extract audio events in three datasets. Features of these audio events were plotted and showed the unique aspects of the three acoustic communities.",Bioinformatics; Computational Biology,Audio event detection; Flattened Local Trimmed Range; Bioacoustic,PeerJ Computer Science,2,23,7,3,2016
Tools4MSP: an open source software package to support Maritime Spatial Planning,"Stefano, Menegon; Alessandro, Sarretta; Daniel, Depellegrin; Giulio, Farella; Chiara, Venier; Andrea, Barbanti",10.7717/peerj-cs.165,"This paper presents the Tools4MSP software package, a Python-based Free and Open Source Software (FOSS) for geospatial analysis in support of Maritime Spatial Planning (MSP) and marine environmental management. The suite was initially developed within the ADRIPLAN data portal, that has been recently upgraded into the Tools4MSP Geoplatform (data.tools4msp.eu), an integrated web platform that supports MSP through the application of different tools, e.g., collaborative geospatial modelling of cumulative effects assessment (CEA) and marine use conflict (MUC) analysis. The package can be used as stand-alone library or as collaborative webtool, providing user-friendly interfaces appropriate to decision-makers, regional authorities, academics and MSP stakeholders. An effective MSP-oriented integrated system of web-based software, users and services is proposed. It includes four components: the Tools4MSP Geoplatform for interoperable and collaborative sharing of geospatial datasets and for MSP-oriented analysis, the Tools4MSP package as stand-alone library for advanced geospatial and statistical analysis, the desktop applications to simplify data curation and the third party data repositories for multidisciplinary and multilevel geospatial datasets integration. The paper presents an application example of the Tools4MSP GeoNode plugin and an example of Tools4MSP stand-alone library for CEA in the Adriatic Sea. The Tools4MSP and the developed software have been released as FOSS under the GPL 3 license and are currently under further development.",Data Science; Scientific Computing and Simulation; Spatial and Geographic Information Systems,Python; SDI; Tools4MSP software; Open Source; Cumulative Effects Assessment; Maritime spatial planning; GeoNode,PeerJ Computer Science,4,37,5,1,2018
Temporal constrained objects for modelling neuronal dynamics,"Manjusha, Nair; Jinesh, Manchan Kannimoola; Bharat, Jayaraman; Bipin, Nair; Shyam, Diwakar",10.7717/peerj-cs.159,"Several new programming languages and technologies have emerged in the past few decades in order to ease the task of modelling complex systems. Modelling the dynamics of complex systems requires various levels of abstractions and reductive measures in representing the underlying behaviour. This also often requires making a trade-off between how realistic a model should be in order to address the scientific questions of interest and the computational tractability of the model.In this paper, we propose a novel programming paradigm, called temporal constrained objects, which facilitates a principled approach to modelling complex dynamical systems. Temporal constrained objects are an extension of constrained objects with a focus on the analysis and prediction of the dynamic behaviour of a system. The structural aspects of a neuronal system are represented using objects, as in object-oriented languages, while the dynamic behaviour of neurons and synapses are modelled using declarative temporal constraints. Computation in this paradigm is a process of constraint satisfaction within a time-based simulation.We identified the feasibility and practicality in automatically mapping different kinds of neuron and synapse models to the constraints of temporal constrained objects. Simple neuronal networks were modelled by composing circuit components, implicitly satisfying the internal constraints of each component and interface constraints of the composition. Simulations show that temporal constrained objects provide significant conciseness in the formulation of these models. The underlying computational engine employed here automatically finds the solutions to the problems stated, reducing the code for modelling and simulation control. All examples reported in this paper have been programmed and successfully tested using the prototype language called TCOB. The code along with the programming environment are available at http://github.com/compneuro/TCOB_Neuron.Temporal constrained objects provide powerful capabilities for modelling the structural and dynamic aspects of neural systems. Capabilities of the constraint programming paradigm, such as declarative specification, the ability to express partial information and non-directionality, and capabilities of the object-oriented paradigm especially aggregation and inheritance, make this paradigm the right candidate for complex systems and computational modelling studies. With the advent of multi-core parallel computer architectures and techniques or parallel constraint-solving, the paradigm of temporal constrained objects lends itself to highly efficient execution which is necessary for modelling and simulation of large brain circuits.",Computational Biology; Scientific Computing and Simulation; Programming Languages,Temporal constrained objects; Constraint programming; Object-oriented languages; Declarative modelling; Neuron models,PeerJ Computer Science,4,58,14,0,2018
Convolutional ensembles for Arabic Handwritten Character and Digit Recognition,"Iam, Palatnik de Sousa",10.7717/peerj-cs.167,"A learning algorithm is proposed for the task of Arabic Handwritten Character and Digit recognition. The architecture consists on an ensemble of different Convolutional Neural Networks. The proposed training algorithm uses a combination of adaptive gradient descent on the first epochs and regular stochastic gradient descent in the last epochs, to facilitate convergence. Different validation strategies are tested, namely Monte Carlo Cross-Validation and K-fold Cross Validation. Hyper-parameter tuning was done by using the MADbase digits dataset. State of the art validation and testing classification accuracies were achieved, with average values of 99.74% and 99.47% respectively. The same algorithm was then trained and tested with the AHCD character dataset, also yielding state of the art validation and testing classification accuracies: 98.60% and 98.42% respectively.",Algorithms and Analysis of Algorithms; Artificial Intelligence; Computer Vision; Data Mining and Machine Learning,Offline character recognition; Arabic Handwriting Recognition; Convolutional Neural Networks; Deep learning,PeerJ Computer Science,4,18,3,1,2018
Smart Brix—a continuous evolution framework for container application deployments,"Johannes M., Schleicher; Michael, Vögler; Christian, Inzinger; Schahram, Dustdar",10.7717/peerj-cs.66,"Container-based application deployments have received significant attention in recent years. Operating system virtualization based on containers as a mechanism to deploy and manage complex, large-scale software systems has become a popular mechanism for application deployment and operation. Packaging application components into self-contained artifacts has brought substantial flexibility to developers and operation teams alike. However, this flexibility comes at a price. Practitioners need to respect numerous constraints ranging from security and compliance requirements, to specific regulatory conditions. Fulfilling these requirements is especially challenging in specialized domains with large numbers of stakeholders. Moreover, the rapidly growing number of container images to be managed due to the introduction of new or updated applications and respective components, leads to significant challenges for container management and adaptation. In this paper, we introduce Smart Brix, a framework for continuous evolution of container application deployments that tackles these challenges. Smart Brix integrates and unifies concepts of continuous integration, runtime monitoring, and operational analytics. Furthermore, it allows practitioners to define generic analytics and compensation pipelines composed of self-assembling processing components to autonomously validate and verify containers to be deployed. We illustrate the feasibility of our approach by evaluating our framework using a case study from the smart city domain. We show that Smart Brix is horizontally scalable and runtime of the implemented analysis and compensation pipelines scales linearly with the number of container application packages.",Adaptive and Self-Organizing Systems; Distributed and Parallel Computing; Software Engineering,Containers; Container evolution; Container adaptation; DevOps; Infrastructure as Code,PeerJ Computer Science,2,28,10,1,2016
Alternating guided image filtering,"Alexander, Toet",10.7717/peerj-cs.72,"Edge preserving filters aim to simplify the representation of images (e.g., by reducing noise or eliminating irrelevant detail) while preserving their most significant edges. These filters are typically nonlinear and locally smooth the image structure while minimizing both blurring and over-sharpening of visually important edges. Here we present the Alternating Guided Filter (AGF) that achieves edge preserving smoothing by combining two recently introduced filters: the Rolling Guided Filter (RGF) and the Smooth and iteratively Restore Filter (SiR). We show that the integration of RGF and SiR in an alternating iterative framework results in a new smoothing operator that preserves significant image edges while effectively eliminating small scale details. The AGF combines the large scale edge and local intensity preserving properties of the RGF with the edge restoring properties of the SiR while eliminating the drawbacks of both previous methods (i.e., edge curvature smoothing by RGF and local intensity reduction and restoration of small scale details near large scale edges by SiR). The AGF is simple to implement and efficient, and produces high-quality results. We demonstrate the effectiveness of AGF on a variety of images, and provide a public code to facilitate future studies.",Algorithms and Analysis of Algorithms; Computer Vision,Guided Filter; Edge preserving filter; Bilateral filter; Alternating Guided Filter; Rolling Guided Filter; Image enhancement; Noise reduction; Edge enhancement; Image smoothing; Image filtering,PeerJ Computer Science,2,9,9,3,2016
What should we know to develop an information robot?,"Satoru, Satake; Keita, Nakatani; Kotaro, Hayashi; Takyuki, Kanda; Michita, Imai",10.7717/peerj-cs.8,"This paper is aimed at identifying the required knowledge for information robots. We addressed two aspects of this knowledge, ‘what should it know’ and ‘what should it do.’ The first part of this study was devoted to the former aspect. We investigated what information staff know and what people expect from information robots. We found that there are a lot of similarities. Based on this, we developed a knowledge structure about an environment to be used to provide information. The developed knowledge structure worked well. In the field study we confirmed that the robot was able to answer most of the requests (96.6%). However, regarding the latter aspect, although we initially replicated what human staff members do, the robot did not serve well. Many users hesitated to speak, and remained quiet. Here, we found that the knowledge for facilitating interaction was missing. We further designed the interaction flow to accommodate people who tend to be quiet. Finally, our field study revealed that the improved interaction flow increased the success ratio of information providing from 54.4% to 84.5%.",Robotics,Information-providing; Direction giving; Belief about robots,PeerJ Computer Science,1,19,11,2,2015
Pylogeny: an open-source Python framework for phylogenetic tree reconstruction and search space heuristics,"Alexander, Safatli; Christian, Blouin",10.7717/peerj-cs.9,"Summary. Pylogeny is a cross-platform library for the Python programming language that provides an object-oriented application programming interface for phylogenetic heuristic searches. Its primary function is to permit both heuristic search and analysis of the phylogenetic tree search space, as well as to enable the design of novel algorithms to search this space. To this end, the framework supports the structural manipulation of phylogenetic trees, in particular using rearrangement operators such as NNI, SPR, and TBR, the scoring of trees using parsimony and likelihood methods, the construction of a tree search space graph, and the programmatic execution of a few existing heuristic programs. The library supports a range of common phylogenetic file formats and can be used for both nucleotide and protein data. Furthermore, it is also capable of supporting GPU likelihood calculation on nucleotide character data through the BEAGLE library.Availability. Existing development and source code is available for contribution and for download by the public from GitHub (http://github.com/AlexSafatli/Pylogeny). A stable release of this framework is available for download through PyPi (Python Package Index) at http://pypi.python.org/pypi/pylogeny.",Bioinformatics; Computational Biology,Phylogenetic; Python; Heuristic; Alignment; Maximum likelihood; Library; Combinatorial; Programming; Parsimony,PeerJ Computer Science,1,14,0,1,2015
Software development: do good manners matter?,"Giuseppe, Destefanis; Marco, Ortu; Steve, Counsell; Stephen, Swift; Michele, Marchesi; Roberto, Tonelli",10.7717/peerj-cs.73,"A successful software project is the result of a complex process involving, above all, people. Developers are the key factors for the success of a software development process, not merely as executors of tasks, but as protagonists and core of the whole development process. This paper investigates social aspects among developers working on software projects developed with the support of Agile tools. We studied 22 open-source software projects developed using the Agile board of the JIRA repository. All comments committed by developers involved in the projects were analyzed and we explored whether the politeness of comments affected the number of developers involved and the time required to fix any given issue. Our results showed that the level of politeness in the communication process among developers does have an effect on the time required to fix issues and, in the majority of the analysed projects, it had a positive correlation with attractiveness of the project to both active and potential developers. The more polite developers were, the less time it took to fix an issue.",Data Mining and Machine Learning; Data Science; Software Engineering,Social and human aspects; Politeness; Mining software repositories; Issue fixing time; Software development,PeerJ Computer Science,2,77,11,10,2016
"OncoLnc: linking TCGA survival data to mRNAs, miRNAs, and lncRNAs","Jordan, Anaya",10.7717/peerj-cs.67,"OncoLnc is a tool for interactively exploring survival correlations, and for downloading clinical data coupled to expression data for mRNAs, miRNAs, or long noncoding RNAs (lncRNAs). OncoLnc contains survival data for 8,647 patients from 21 cancer studies performed by The Cancer Genome Atlas (TCGA), along with RNA-SEQ expression for mRNAs and miRNAs from TCGA, and lncRNA expression from MiTranscriptome beta. Storing this data gives users the ability to separate patients by gene expression, and then create publication-quality Kaplan-Meier plots or download the data for further analyses. OncoLnc also stores precomputed survival analyses, allowing users to quickly explore survival correlations for up to 21 cancers in a single click. This resource allows researchers studying a specific gene to quickly investigate if it may have a role in cancer, and the supporting data allows researchers studying a specific cancer to identify the mRNAs, miRNAs, and lncRNAs most correlated with survival, and researchers looking for a novel lncRNA involved with cancer lists of potential candidates. OncoLnc is available at http://www.oncolnc.org.",Bioinformatics; Computational Biology; Databases,TCGA; Cancer; RNA-SEQ; mRNA; miRNA; lncRNA; Kaplan-Meier; Cox regression; Database,PeerJ Computer Science,2,19,2,3,2016
SNF: synthesizing high performance NFV service chains,"Georgios P., Katsikas; Marcel, Enguehard; Maciej, Kuźniar; Gerald Q., Maguire Jr; Dejan, Kostić",10.7717/peerj-cs.98,"In this paper we introduce SNF, a framework that synthesizes (S) network function (NF) service chains by eliminating redundant I/O and repeated elements, while consolidating stateful cross layer packet operations across the chain. SNF uses graph composition and set theory to determine traffic classes handled by a service chain composed of multiple elements. It then synthesizes each traffic class using a minimal set of new elements that apply single-read-single-write and early-discard operations. Our SNF prototype takes a baseline state of the art network functions virtualization (NFV) framework to the level of performance required for practical NFV service deployments. Software-based SNF realizes long (up to 10 NFs) and stateful service chains that achieve line-rate 40 Gbps throughput (up to 8.5x greater than the baseline NFV framework). Hardware-assisted SNF, using a commodity OpenFlow switch, shows that our approach scales at 40 Gbps for Internet Service Provider-level NFV deployments.",Computer Networks and Communications,NFV; Service chains; Synthesis; Single-read-single-write; Line-rate; 40 Gbps,PeerJ Computer Science,2,42,11,0,2016
Adaptive automation: automatically (dis)engaging automation during visually distracted driving,"Christopher D.D., Cabrall; Nico M., Janssen; Joost C.F., de Winter",10.7717/peerj-cs.166,"Automated driving is often proposed as a solution to human errors. However, fully automated driving has not yet reached the point where it can be implemented in real traffic. This study focused on adaptively allocating steering control either to the driver or to an automated pilot based on momentary driver distraction measured from an eye tracker.Participants (N = 31) steered a simulated vehicle with a fixed speed, and at specific moments were required to perform a visual secondary task (i.e., changing a CD). Three conditions were tested: (1) Manual driving (Manual), in which participants steered themselves. (2) An automated backup (Backup) condition, consisting of manual steering except during periods of visual distraction, where the driver was backed up by automated steering. (3) A forced manual drive (Forced) condition, consisting of automated steering except during periods of visual distraction, where the driver was forced into manual steering. In all three conditions, the speed of the vehicle was automatically kept at 70 km/h throughout the drive.The Backup condition showed a decrease in mean and maximum absolute lateral error compared to the Manual condition. The Backup condition also showed the lowest self-reported workload ratings and yielded a higher acceptance rating than the Forced condition. The Forced condition showed a higher maximum absolute lateral error than the Backup condition.In conclusion, the Backup condition was well accepted, and significantly improved performance when compared to the Manual and Forced conditions. Future research could use a higher level of simulator fidelity and a higher-quality eye-tracker.",Human-Computer Interaction; Multimedia,Automated driving; Adaptive automation; Eye tracking; Driver distraction; Driving simulator; Dual task; Human–machine interaction; Car driving,PeerJ Computer Science,4,59,10,0,2018
Continuously revised assurance cases with stakeholders’ cross-validation: a DEOS experience,"Kimio, Kuramitsu",10.7717/peerj-cs.101,"Recently, assurance cases have received much attention in the field of software-based computer systems and IT services. However, software changes very often, and there are no strong regulations for software. These facts are two main challenges to be addressed in the development of software assurance cases. We propose a method of developing assurance cases by means of continuous revision at every stage of the system life cycle, including in operation and service recovery in failure cases. Instead of a regulator, dependability arguments are validated by multiple stakeholders competing with each other. This paper reported our experience with the proposed method in the case of Aspen education service. The case study demonstrates that continuous revisions enable stakeholders to share dependability problems across software life cycle stages, which will lead to the long-term improvement of service dependability.",Security and Privacy; Software Engineering,Assurance cases; GSN; DEOS process; Experience report; Service dependability,PeerJ Computer Science,2,21,8,0,2016
Modeling and management of usage-aware distributed datasets for global Smart City Application Ecosystems,"Johannes M., Schleicher; Michael, Vögler; Christian, Inzinger; Schahram, Dustdar",10.7717/peerj-cs.115,"The ever-growing amount of data produced by and in today’s smart cities offers significant potential for novel applications created by city stakeholders as well as third parties. Current smart city application models mostly assume that data is exclusively managed by and bound to its original application and location. We argue that smart city data must not be constrained to such data silos so that future smart city applications can seamlessly access and integrate data from multiple sources across multiple cities. In this paper, we present a methodology and toolset to model available smart city data sources and enable efficient, distributed data access in smart city environments. We introduce a modeling abstraction to describe the structure and relevant properties, such as security and compliance constraints, of smart city data sources along with independently accessible subsets in a technology-agnostic way. Based on this abstraction, we present a middleware toolset for efficient and seamless data access through autonomous relocation of relevant subsets of available data sources to improve Quality of Service for smart city applications based on a configurable mechanism. We evaluate our approach using a case study in the context of a distributed city infrastructure decision support system and show that selective relocation of data subsets can significantly reduce application response times.",Distributed and Parallel Computing; Software Engineering,Smart city application engineering; Data management; Data migration; Quality of service,PeerJ Computer Science,3,23,8,2,2017
DOSCHEDA: a web application for interactive chemoproteomics data analysis,"Bruno, Contrino; Eric, Miele; Ronald, Tomlinson; M. Paola, Castaldi; Piero, Ricchiuto",10.7717/peerj-cs.129,"Mass Spectrometry (MS) based chemoproteomics has recently become a main tool to identify and quantify cellular target protein interactions with ligands/drugs in drug discovery. The complexity associated with these new types of data requires scientists with a limited computational background to perform systematic data quality controls as well as to visualize the results derived from the analysis to enable rapid decision making. To date, there are no readily accessible platforms specifically designed for chemoproteomics data analysis.We developed a Shiny-based web application named DOSCHEDA (Down Stream Chemoproteomics Data Analysis) to assess the quality of chemoproteomics experiments, to filter peptide intensities based on linear correlations between replicates, and to perform statistical analysis based on the experimental design. In order to increase its accessibility, DOSCHEDA is designed to be used with minimal user input and it does not require programming knowledge. Typical inputs can be protein fold changes or peptide intensities obtained from Proteome Discover, MaxQuant or other similar software. DOSCHEDA aggregates results from bioinformatics analyses performed on the input dataset into a dynamic interface, it encompasses interactive graphics and enables customized output reports.DOSCHEDA is implemented entirely in R language. It can be launched by any system with R installed, including Windows, Mac OS and Linux distributions. DOSCHEDA is hosted on a shiny-server at https://doscheda.shinyapps.io/doscheda and is also available as a Bioconductor package (http://www.bioconductor.org/).",Bioinformatics,Quantitative Chemoproteomics; Statistical Models; Web interface; Shiny; TMT; iTRAQ; Proteomics; Quantitative Chemical biology; Drug dose-response; Protein drug profiling,PeerJ Computer Science,3,23,4,3,2017
The rise of Chrome,"Jonathan, Tamary; Dror G., Feitelson",10.7717/peerj-cs.28,"Since Chrome’s initial release in 2008 it has grown in market share, and now controls roughly half of the desktop browsers market. In contrast with Internet Explorer, the previous dominant browser, this was not achieved by marketing practices such as bundling the browser with a pre-loaded operating system. This raises the question of how Chrome achieved this remarkable feat, while other browsers such as Firefox and Opera were left behind. We show that both the performance of Chrome and its conformance with relevant standards are typically better than those of the two main contending browsers, Internet Explorer and Firefox. In addition, based on a survey of the importance of 25 major features, Chrome product managers seem to have made somewhat better decisions in selecting where to put effort. Thus the rise of Chrome is consistent with technical superiority over the competition.",World Wide Web and Web Science; Software Engineering,Web browser; Market share; Benchmark results; Google Chrome; Feature selection,PeerJ Computer Science,1,16,13,9,2015
A survey of Chinese interpreting studies: who influences who …and why?,"Ziyun, Xu; Leonid B., Pekelis",10.7717/peerj-cs.14,"This paper describes how scholars in Chinese Interpreting Studies (CIS) interact with each other and form discrete circles of influence. It also discusses what it means to be an influential scholar in the community, and the relationship between an author’s choice of research topic and his academic influence. The study examines an all-but-exhaustive collection of 59,303 citations from 1,289 MA theses, 32 doctoral dissertations and 2,909 research papers, combining traditional citation analysis with the newer Social Network Analysis to paint a panorama of CIS. It concludes that the community cannot be broadly divided into Liberal Arts and Empirical Science camps; rather, it comprises several distinct communities with various defining features. The analysis also reveals that the top Western influencers have an array of academic backgrounds and research interests across many different disciplines, whereas their Chinese counterparts are predominantly focused on Interpreting Studies. Last but not least, there is found to be a positive correlation between choosing non-mainstream research topics and having a high level of academic influence in the community.",Data Mining and Machine Learning; Data Science; Digital Libraries; Social Computing,Chinese Interpreting Studies; Academic influence; Social network analysis; Research topic selection,PeerJ Computer Science,1,98,7,24,2015
D4V: a peer-to-peer architecture for data dissemination in smartphone-based vehicular applications,"Marco, Picone; Michele, Amoretti; Gianluigi, Ferrari; Francesco, Zanichelli",10.7717/peerj-cs.15,"Vehicular data collection applications are emerging as an appealing technology to monitor urban areas, where a high concentration of connected vehicles with onboard sensors is a near future scenario. In this context, smartphones are, on one side, effective enablers of Vehicle-to-Vehicle (V2V) and Vehicle-to-Infrastructure (V2I) applications and, on the other side, highly sophisticated sensing platforms. In this paper, we introduce an effective and efficient system, denoted as D4V, to disseminate vehicle-related information and sensed data using smartphones as V2I devices. D4V relies on a Peer-to-Peer (P2P) overlay scheme, denoted as Distributed Geographic Table (DGT), which unifies the concepts of physical and virtual neighborhoods in a scalable and robust infrastructure for application-level services. First, we investigate the discovery procedure of the DGT overlay network, through analytical and simulation results. Then, we present and discuss an extensive simulation-based performance evaluation (considering relevant performance indicators) of the D4V system, in a 4G wireless communication scenario. The simulation methodology combines DEUS (an application-level simulation tool for the study of large-scale systems) with ns-3 (a well-known network simulator, which takes into account lower layers), in order to provide a D4V proof-of-concept. The observed results show that D4V-based information sharing among vehicles allows to significantly reduce risks and nuisances (e.g., due to road defects and congestions).",Computer Networks and Communications; Distributed and Parallel Computing; Mobile and Ubiquitous Computing,Vehicular Sensor Networks (VSNs); Smartphones; Peer-to-Peer (P2P); Vehicle-to-Infrastructure (V2I); Localization,PeerJ Computer Science,1,59,13,1,2015
A framework for cut-over management,"Guido, Nageldinger",10.7717/peerj-cs.29,"The purpose of this paper is to provide a governance structure for IT-related projects in order to assure a safeguarded and timely transition to a productive environment. This transitioning, which rarely exceeds a weekend, is colloquially called ‘cut-over’, ‘rollout’ or ‘deployment’. The governance structure is defined in accordance with a set of project-specific deliverables for a cascade-type procedural project-management model, which is integrated within an Information Technology Infrastructure Library (ITIL)-orientated service organization. This integration is illustrated by the use of a semi-agile release model. Due to the release model selected, which is particularly characterized by its bundling of projects for a release-specific rollout (as it is referred to in the project documentation), a new definition and interpretation of deployment from a generic ITIL perspective is required. The facilitated release model requires a distinction between a project-specific cut-over and a release-specific rollout. This separation gives rise to two types of go-live scenarios: one for each participating project and one for each release. Additionally, an interplay between cut-over planning for a project and rollout planning for a release becomes apparent. Projects should already incorporate cut-over related deliverables in the initial planning phase. Even though consulting methodologies such as ASAP (Accelerated SAP), recommend scattered, project-specific deliverables useful for cut-over planning, this publication offers an integrated approach on how to prepare systematically for a project-specific cut-over with all required deliverables. The framework provided maps out ITIL’s release and deployment process by means of IT projects; furthermore it allows IT projects to interface easily with the ITIL change-management process.",Computer Architecture; Theory and Formal Methods; Software Engineering,Release; Project-specific cut-over; Release-specific rollout; Go-live preparation; Go-live; Deployment; Application-specific cut-over; ITIL; IT Service Management; IT Project Management,PeerJ Computer Science,1,17,13,3,2015
"A method for creating interactive, user-resembling avatars","Igor, Macedo Silva; Renan C., Moioli",10.7717/peerj-cs.128,"Virtual reality (VR) applications have disseminated throughout several fields, with a special quest for immersion. The avatar is one of the key constituents of immersive applications, and avatar resemblance can provoke diverse emotional responses from the user. Yet a lot a virtual reality systems struggle to implement real life-like avatars. In this work, we propose a novel method for creating interactive, user-resembling avatars using available commercial hardware and software. Avatar visualization is possible with a point-cloud or a contiguous polygon surface, and avatar interactions with the virtual scenario happens through a body joint-approximation for contact. In addition, the implementation could be easily extended to other systems and its modular architecture admits improvement both on visualization and physical interactions. The code is under Apache License 2.0 and is freely available as Supplemental Information 1 in this article.",Human-Computer Interaction; Emerging Technologies,Virtual reality; Immersion; Oculus rift; Kinect; Avatar embodiment,PeerJ Computer Science,3,50,8,2,2017
A survey of secure middleware for the Internet of Things,"Paul, Fremantle; Philip, Scott",10.7717/peerj-cs.114,"The rapid growth of small Internet connected devices, known as the Internet of Things (IoT), is creating a new set of challenges to create secure, private infrastructures. This paper reviews the current literature on the challenges and approaches to security and privacy in the Internet of Things, with a strong focus on how these aspects are handled in IoT middleware. We focus on IoT middleware because many systems are built from existing middleware and these inherit the underlying security properties of the middleware framework. The paper is composed of three main sections. Firstly, we propose a matrix of security and privacy threats for IoT. This matrix is used as the basis of a widespread literature review aimed at identifying requirements on IoT platforms and middleware. Secondly, we present a structured literature review of the available middleware and how security is handled in these middleware approaches. We utilise the requirements from the first phase to evaluate. Finally, we draw a set of conclusions and identify further work in this area.",Computer Networks and Communications; Embedded Computing; Real-Time and Embedded Systems; Security and Privacy; World Wide Web and Web Science,Internet of Things; Security; Privacy; IoT; Middleware; Survey,PeerJ Computer Science,3,200,1,2,2017
Learning structural bioinformatics and evolution with a snake puzzle,"Gonzalo S., Nido; Ludovica, Bachschmid-Romano; Ugo, Bastolla; Alberto, Pascual-García",10.7717/peerj-cs.100,"We propose here a working unit for teaching basic concepts of structural bioinformatics and evolution through the example of a wooden snake puzzle, strikingly similar to toy models widely used in the literature of protein folding. In our experience, developed at a Master’s course at the Universidad Autónoma de Madrid (Spain), the concreteness of this example helps to overcome difficulties caused by the interdisciplinary nature of this field and its high level of abstraction, in particular for students coming from traditional disciplines. The puzzle will allow us discussing a simple algorithm for finding folded solutions, through which we will introduce the concept of the configuration space and the contact matrix representation. This is a central tool for comparing protein structures, for studying simple models of protein energetics, and even for a qualitative discussion of folding kinetics, through the concept of the Contact Order. It also allows a simple representation of misfolded conformations and their free energy. These concepts will motivate evolutionary questions, which we will address by simulating a structurally constrained model of protein evolution, again modelled on the snake puzzle. In this way, we can discuss the analogy between evolutionary concepts and statistical mechanics that facilitates the understanding of both concepts. The proposed examples and literature are accessible, and we provide supplementary material (see ‘Data Availability’) to reproduce the numerical experiments. We also suggest possible directions to expand the unit. We hope that this work will further stimulate the adoption of games in teaching practice.",Bioinformatics; Computational Biology; Computer Education; Scientific Computing and Simulation,Structural bioinformatics; Education; Protein folding; Statistical mechanics; Contact matrix; Protein structure alignment; Designability; Evolution; Contact order; Protein classification,PeerJ Computer Science,2,67,12,4,2016
AliTV—interactive visualization of whole genome comparisons,"Markus J., Ankenbrand; Sonja, Hohlfeld; Thomas, Hackl; Frank, Förster",10.7717/peerj-cs.116,"Whole genome alignments and comparative analysis are key methods in the quest of unraveling the dynamics of genome evolution. Interactive visualization and exploration of the generated alignments, annotations, and phylogenetic data are important steps in the interpretation of the initial results. Limitations of existing software inspired us to develop our new tool AliTV, which provides interactive visualization of whole genome alignments. AliTV reads multiple whole genome alignments or automatically generates alignments from the provided data. Optional feature annotations and phylo- genetic information are supported. The user-friendly, web-browser based and highly customizable interface allows rapid exploration and manipulation of the visualized data as well as the export of publication-ready high-quality figures. AliTV is freely available at https://github.com/AliTVTeam/AliTV.",Bioinformatics; Computational Biology,Comparative genomics; Alignment; Visualization,PeerJ Computer Science,3,38,1,1,2017
A PCA-based bio-motion generator to synthesize new patterns of human running,"José María, Baydal-Bertomeu; Juan Vicente, Durá-Gil; Ana, Piérola-Orcero; Eduardo, Parrilla Bernabé; Alfredo, Ballester; Sandra, Alemany-Munt",10.7717/peerj-cs.102,"Synthesizing human movement is useful for most applications where the use of avatars is required. These movements should be as realistic as possible and thus must take into account anthropometric characteristics (weight, height, etc.), gender, and the performance of the activity being developed. The aim of this study is to develop a new methodology based on the combination of principal component analysis and partial least squares regression model that can generate realistic motion from a set of data (gender, anthropometry and performance). A total of 18 volunteer runners have participated in the study. The joint angles of the main body joints were recorded in an experimental study using 3D motion tracking technology. A five-step methodology has been employed to develop a model capable of generating a realistic running motion. The described model has been validated for running motion, showing a highly realistic motion which fits properly with the real movements measured. The described methodology could be applied to synthesize any type of motion: walking, going up and down stairs, etc. In future work, we want to integrate the motion in realistic body shapes, generated with a similar methodology and from the same simple original data.",Data Mining and Machine Learning; Graphics; Scientific Computing and Simulation,Synthesizing motion; Motion analysis; PLS; Running; PCA,PeerJ Computer Science,2,26,6,2,2016
PAME: plasmonic assay modeling environment,"Adam, Hughes; Zhaowen, Liu; Mark E., Reeves",10.7717/peerj-cs.17,"Plasmonic assays are an important class of optical sensors that measure biomolecular interactions in real-time without the need for labeling agents, making them especially well-suited for clinical applications. Through the incorporation of nanoparticles and fiberoptics, these sensing systems have been successfully miniaturized and show great promise for in-situ probing and implantable devices, yet it remains challenging to derive meaningful, quantitative information from plasmonic responses. This is in part due to a lack of dedicated modeling tools, and therefore we introduce PAME, an open-source Python application for modeling plasmonic systems of bulk and nanoparticle-embedded metallic films. PAME combines aspects of thin-film solvers, nanomaterials and fiber-optics into an intuitive graphical interface. Some of PAME’s features include a simulation mode, a database of hundreds of materials, and an object-oriented framework for designing complex nanomaterials, such as a gold nanoparticles encased in a protein shell. An overview of PAME’s theory and design is presented, followed by example simulations of a fiberoptic refractometer, as well as protein binding to a multiplexed sensor composed of a mixed layer of gold and silver colloids. These results provide new insights into observed responses in reflectance biosensors.",Computational Biology; Scientific Computing and Simulation; Software Engineering,Assays; Bioengineering; Python; Modeling; Biosensing; Simulation; Software; Plasmonics; Nanoparticles; Fiberoptics; Thin films,PeerJ Computer Science,1,114,7,0,2015
Workload assessment for mental arithmetic tasks using the task-evoked pupillary response,"Gerhard, Marquart; Joost, de Winter",10.7717/peerj-cs.16,"Pupillometry is a promising method for assessing mental workload and could be helpful in the optimization of systems that involve human–computer interaction. The present study focuses on replicating the studies by Ahern (1978) and Klingner (2010), which found that for three levels of difficulty of mental multiplications, the more difficult multiplications yielded larger dilations of the pupil. Using a remote eye tracker, our research expands upon these two previous studies by statistically testing for each 1.5 s interval of the calculation period (1) the mean absolute pupil diameter (MPD), (2) the mean pupil diameter change (MPDC) with respect to the pupil diameter during the pre-stimulus accommodation period, and (3) the mean pupil diameter change rate (MPDCR). An additional novelty of our research is that we compared the pupil diameter measures with a self-report measure of workload, the NASA Task Load Index (NASA-TLX), and with the mean blink rate (MBR). The results showed that the findings of Ahern and Klingner were replicated, and that the MPD and MPDC discriminated just as well between the lowest and highest difficulty levels as did the NASA-TLX. The MBR, on the other hand, did not differentiate between the difficulty levels. Moderate to strong correlations were found between the MPDC and the proportion of incorrect responses, indicating that the MPDC was higher for participants with a poorer performance. For practical applications, validity could be improved by combining pupillometry with other physiological techniques.",Human-Computer Interaction,Pupillometry; Human factors; Pupil diameter; Cognitive load,PeerJ Computer Science,1,36,10,3,2015
SymPy: symbolic computing in Python,"Aaron, Meurer; Christopher P., Smith; Mateusz, Paprocki; Ondřej, Čertík; Sergey B., Kirpichev; Matthew, Rocklin; AMiT, Kumar; Sergiu, Ivanov; Jason K., Moore; Sartaj, Singh; Thilina, Rathnayake; Sean, Vig; Brian E., Granger; Richard P., Muller; Francesco, Bonazzi; Harsh, Gupta; Shivam, Vats; Fredrik, Johansson; Fabian, Pedregosa; Matthew J., Curry; Andy R., Terrel; Štěpán, Roučka; Ashutosh, Saboo; Isuru, Fernando; Sumith, Kulal; Robert, Cimrman; Anthony, Scopatz",10.7717/peerj-cs.103,"SymPy is an open source computer algebra system written in pure Python. It is built with a focus on extensibility and ease of use, through both interactive and programmatic applications. These characteristics have led SymPy to become a popular symbolic library for the scientific Python ecosystem. This paper presents the architecture of SymPy, a description of its features, and a discussion of select submodules. The supplementary material provide additional examples and further outline details of the architecture and features of SymPy.",Scientific Computing and Simulation; Software Engineering,Python; Computer algebra system; Symbolics,PeerJ Computer Science,3,66,1,3,2017
A probabilistic model to recover individual genomes from metagenomes,"Johannes, Dröge; Alexander, Schönhuth; Alice C., McHardy",10.7717/peerj-cs.117,"Shotgun metagenomics of microbial communities reveal information about strains of relevance for applications in medicine, biotechnology and ecology. Recovering their genomes is a crucial but very challenging step due to the complexity of the underlying biological system and technical factors. Microbial communities are heterogeneous, with oftentimes hundreds of present genomes deriving from different species or strains, all at varying abundances and with different degrees of similarity to each other and reference data. We present a versatile probabilistic model for genome recovery and analysis, which aggregates three types of information that are commonly used for genome recovery from metagenomes. As potential applications we showcase metagenome contig classification, genome sample enrichment and genome bin comparisons. The open source implementation MGLEX is available via the Python Package Index and on GitHub and can be embedded into metagenome analysis workflows and programs.",Bioinformatics; Computational Biology; Data Science,Binning; Metagenomics,PeerJ Computer Science,3,32,6,3,2017
Species-specific audio detection: a comparison of three template-based detection algorithms using random forests,"Carlos J., Corrada Bravo; Rafael, Álvarez Berríos; T. Mitchell, Aide",10.7717/peerj-cs.113,"We developed a web-based cloud-hosted system that allow users to archive, listen, visualize, and annotate recordings. The system also provides tools to convert these annotations into datasets that can be used to train a computer to detect the presence or absence of a species. The algorithm used by the system was selected after comparing the accuracy and efficiency of three variants of a template-based detection. The algorithm computes a similarity vector by comparing a template of a species call with time increments across the spectrogram. Statistical features are extracted from this vector and used as input for a Random Forest classifier that predicts presence or absence of the species in the recording. The fastest algorithm variant had the highest average accuracy and specificity; therefore, it was implemented in the ARBIMON web-based system.",Bioinformatics; Computational Biology; Data Mining and Machine Learning,Acoustic monitoring; Machine learning; Animal vocalizations; Recording visualization; Recording annotation; Generic species algorithm; Web-based cloud-hosted system; Random Forest classifier; Species prediction; Species-specific audio detection,PeerJ Computer Science,3,32,9,5,2017
Exploring Twitter communication dynamics with evolving community analysis,"Konstantinos, Konstantinidis; Symeon, Papadopoulos; Yiannis, Kompatsiaris",10.7717/peerj-cs.107,"Online Social Networks (OSNs) have been widely adopted as a means of news dissemination, event reporting, opinion expression and discussion. As a result, news and events are being constantly reported and discussed online through OSNs such as Twitter. However, the variety and scale of all the information renders manual analysis extremely cumbersome, and therefore creating a storyline for an event or news story is an effort-intensive task. The main challenge pertains to the magnitude of data to be analyzed. To this end, we propose a framework for ranking the resulting communities and their metadata on the basis of structural, contextual and evolutionary characteristics such as community centrality, textual entropy, persistence and stability. We apply the proposed framework on three Twitter datasets and demonstrate that the analysis that followed enables the extraction of new insights with respect to influential user accounts, topics of discussion and emerging trends. These insights could primarily assist the work of social and political analysis scientists and the work of journalists in their own story telling, but also highlight the limitations of existing analysis methods and pose new research questions. To our knowledge, this study is the first to investigate the ranking of dynamic communities. In addition, our findings suggest future work regarding the determination of the general context of the communities based on structure and evolutionary behavior alone.",Data Mining and Machine Learning; Network Science and Online Social Networks; Social Computing,Online social networks; Community evolution detection; Community ranking; Data mining,PeerJ Computer Science,3,51,13,8,2017
Mining usage patterns for the Android API,"Hudson S., Borges; Marco Tulio, Valente",10.7717/peerj-cs.12,"API methods are not used alone, but in groups and following patterns. However, despite being a key information for API users, most usage patterns are not described in official API documents. In this article, we report a study that evaluates the feasibility of automatically enriching API documents with information on usage patterns. For this purpose, we mine and analyze 1,952 usage patterns, from a set of 396 Android applications. As part of our findings, we report that the Android API has many undocumented and non-trivial usage patterns, which can be inferred using association rule mining algorithms. We also describe a field study where a version of the original Android documentation is instrumented with the extracted usage patterns. During 17 months, this documentation received 77,863 visits from professional Android developers.",Software Engineering,Application programming interfaces; Usage patterns; Android,PeerJ Computer Science,1,19,6,3,2015
Auditory interfaces in automated driving: an international survey,"Pavlo, Bazilinskyy; Joost, de Winter",10.7717/peerj-cs.13,"This study investigated peoples’ opinion on auditory interfaces in contemporary cars and their willingness to be exposed to auditory feedback in automated driving. We used an Internet-based survey to collect 1,205 responses from 91 countries. The respondents stated their attitudes towards two existing auditory driver assistance systems, a parking assistant (PA) and a forward collision warning system (FCWS), as well as towards a futuristic augmented sound system (FS) proposed for fully automated driving. The respondents were positive towards the PA and FCWS, and rated the willingness to have automated versions of these systems as 3.87 and 3.77, respectively (on a scale from 1 = disagree strongly to 5 = agree strongly). The respondents tolerated the FS (the mean willingness to use it was 3.00 on the same scale). The results showed that among the available response options, the female voice was the most preferred feedback type for takeover requests in highly automated driving, regardless of whether the respondents’ country was English speaking or not. The present results could be useful for designers of automated vehicles and other stakeholders.",Human-Computer Interaction; Autonomous Systems; Multimedia,Driverless car; Crowdsourcing; Survey; Questionnaire; Fully automated driving; Highly automated driving; Auditory interface; Auditory feedback; Warning,PeerJ Computer Science,1,98,9,3,2015
Finding melanoma drugs through a probabilistic knowledge graph,"James P., McCusker; Michel, Dumontier; Rui, Yan; Sylvia, He; Jonathan S., Dordick; Deborah L., McGuinness",10.7717/peerj-cs.106,"Metastatic cutaneous melanoma is an aggressive skin cancer with some progression-slowing treatments but no known cure. The omics data explosion has created many possible drug candidates; however, filtering criteria remain challenging, and systems biology approaches have become fragmented with many disconnected databases. Using drug, protein and disease interactions, we built an evidence-weighted knowledge graph of integrated interactions. Our knowledge graph-based system, ReDrugS, can be used via an application programming interface or web interface, and has generated 25 high-quality melanoma drug candidates. We show that probabilistic analysis of systems biology graphs increases drug candidate quality compared to non-probabilistic methods. Four of the 25 candidates are novel therapies, three of which have been tested with other cancers. All other candidates have current or completed clinical trials, or have been studied in in vivo or in vitro. This approach can be used to identify candidate therapies for use in research or personalized medicine.",Bioinformatics; Computational Biology; Data Science; World Wide Web and Web Science,Melanoma; Knowledge graphs; Drug repositioning; Uncertainty reasoning,PeerJ Computer Science,3,81,7,2,2017
Formatting Open Science: agilely creating multiple document formats for academic manuscripts with Pandoc Scholar,"Albert, Krewinkel; Robert, Winkler",10.7717/peerj-cs.112,"The timely publication of scientific results is essential for dynamic advances in science. The ubiquitous availability of computers which are connected to a global network made the rapid and low-cost distribution of information through electronic channels possible. New concepts, such as Open Access publishing and preprint servers are currently changing the traditional print media business towards a community-driven peer production. However, the cost of scientific literature generation, which is either charged to readers, authors or sponsors, is still high. The main active participants in the authoring and evaluation of scientific manuscripts are volunteers, and the cost for online publishing infrastructure is close to negligible. A major time and cost factor is the formatting of manuscripts in the production stage. In this article we demonstrate the feasibility of writing scientific manuscripts in plain markdown (MD) text files, which can be easily converted into common publication formats, such as PDF, HTML or EPUB, using Pandoc. The simple syntax of Markdown assures the long-term readability of raw files and the development of software and workflows. We show the implementation of typical elements of scientific manuscripts—formulas, tables, code blocks and citations—and present tools for editing, collaborative writing and version control. We give an example on how to prepare a manuscript with distinct output formats, a DOCX file for submission to a journal, and a LATEX/PDF version for deposition as a PeerJ preprint. Further, we implemented new features for supporting ‘semantic web’ applications, such as the ‘journal article tag suite’—JATS, and the ‘citation typing ontology’—CiTO standard. Reducing the work spent on manuscript formatting translates directly to time and cost savings for writers, publishers, readers and sponsors. Therefore, the adoption of the MD format contributes to the agile production of open science literature. Pandoc Scholar is freely available from https://github.com/pandoc-scholar.",Computer Education; Computer Networks and Communications; Digital Libraries; Human-Computer Interaction; World Wide Web and Web Science,Open science; Markdown; Latex; Publishing; Typesetting; Document formats,PeerJ Computer Science,3,45,8,4,2017
Solving the inverse heat conduction problem using NVLink capable Power architecture,"Sándor, Szénási",10.7717/peerj-cs.138,"The accurate knowledge of Heat Transfer Coefficients is essential for the design of precise heat transfer operations. The determination of these values requires Inverse Heat Transfer Calculations, which are usually based on heuristic optimisation techniques, like Genetic Algorithms or Particle Swarm Optimisation. The main bottleneck of these heuristics is the high computational demand of the cost function calculation, which is usually based on heat transfer simulations producing the thermal history of the workpiece at given locations. This Direct Heat Transfer Calculation is a well parallelisable process, making it feasible to implement an efficient GPU kernel for this purpose. This paper presents a novel step forward: based on the special requirements of the heuristics solving the inverse problem (executing hundreds of simulations in a parallel fashion at the end of each iteration), it is possible to gain a higher level of parallelism using multiple graphics accelerators. The results show that this implementation (running on 4 GPUs) is about 120 times faster than a traditional CPU implementation using 20 cores. The latest developments of the GPU-based High Power Computations area were also analysed, like the new NVLink connection between the host and the devices, which tries to solve the long time existing data transfer handicap of GPU programming.",Distributed and Parallel Computing; Graphics; Scientific Computing and Simulation; Software Engineering,GPU; CUDA; Inverse heat conduction problem; Heat transfer; Parallelisation; Data-parallel algorithm; Simulation; NVLink; Graphics accelerator; Optimisation,PeerJ Computer Science,3,17,8,5,2017
Bracken: estimating species abundance in metagenomics data,"Jennifer, Lu; Florian P., Breitwieser; Peter, Thielen; Steven L., Salzberg",10.7717/peerj-cs.104,"Metagenomic experiments attempt to characterize microbial communities using high-throughput DNA sequencing. Identification of the microorganisms in a sample provides information about the genetic profile, population structure, and role of microorganisms within an environment. Until recently, most metagenomics studies focused on high-level characterization at the level of phyla, or alternatively sequenced the 16S ribosomal RNA gene that is present in bacterial species. As the cost of sequencing has fallen, though, metagenomics experiments have increasingly used unbiased shotgun sequencing to capture all the organisms in a sample. This approach requires a method for estimating abundance directly from the raw read data. Here we describe a fast, accurate new method that computes the abundance at the species level using the reads collected in a metagenomics experiment. Bracken (Bayesian Reestimation of Abundance after Classification with KrakEN) uses the taxonomic assignments made by Kraken, a very fast read-level classifier, along with information about the genomes themselves to estimate abundance at the species level, the genus level, or above. We demonstrate that Bracken can produce accurate species- and genus-level abundance estimates even when a sample contains multiple near-identical species.",Bioinformatics; Computational Biology,Metagenomics; Species abundance; Microbiome; Bayesian estimation,PeerJ Computer Science,3,26,5,0,2017
Interoperability and FAIRness through a novel combination of Web technologies,"Mark D., Wilkinson; Ruben, Verborgh; Luiz Olavo, Bonino da Silva Santos; Tim, Clark; Morris A., Swertz; Fleur D.L., Kelpin; Alasdair J.G., Gray; Erik A., Schultes; Erik M., van Mulligen; Paolo, Ciccarese; Arnold, Kuzniar; Anand, Gavai; Mark, Thompson; Rajaram, Kaliyaperumal; Jerven T., Bolleman; Michel, Dumontier",10.7717/peerj-cs.110,"Data in the life sciences are extremely diverse and are stored in a broad spectrum of repositories ranging from those designed for particular data types (such as KEGG for pathway data or UniProt for protein data) to those that are general-purpose (such as FigShare, Zenodo, Dataverse or EUDAT). These data have widely different levels of sensitivity and security considerations. For example, clinical observations about genetic mutations in patients are highly sensitive, while observations of species diversity are generally not. The lack of uniformity in data models from one repository to another, and in the richness and availability of metadata descriptions, makes integration and analysis of these data a manual, time-consuming task with no scalability. Here we explore a set of resource-oriented Web design patterns for data discovery, accessibility, transformation, and integration that can be implemented by any general- or special-purpose repository as a means to assist users in finding and reusing their data holdings. We show that by using off-the-shelf technologies, interoperability can be achieved atthe level of an individual spreadsheet cell. We note that the behaviours of this architecture compare favourably to the desiderata defined by the FAIR Data Principles, and can therefore represent an exemplar implementation of those principles. The proposed interoperability design patterns may be used to improve discovery and integration of both new and legacy data, maximizing the utility of all scholarly outputs.",Bioinformatics; Data Science; Databases; Emerging Technologies; World Wide Web and Web Science,FAIR data; Interoperability; Data integration; Semantic web; Linked data; REST,PeerJ Computer Science,3,43,9,1,2017
MLitB: machine learning in the browser,"Edward, Meeds; Remco, Hendriks; Said, Al Faraby; Magiel, Bruntink; Max, Welling",10.7717/peerj-cs.11,"With few exceptions, the field of Machine Learning (ML) research has largely ignored the browser as a computational engine. Beyond an educational resource for ML, the browser has vast potential to not only improve the state-of-the-art in ML research, but also, inexpensively and on a massive scale, to bring sophisticated ML learning and prediction to the public at large. This paper introduces MLitB, a prototype ML framework written entirely in Javascript, capable of performing large-scale distributed computing with heterogeneous classes of devices. The development of MLitB has been driven by several underlying objectives whose aim is to make ML learning and usage ubiquitous (by using ubiquitous compute devices), cheap and effortlessly distributed, and collaborative. This is achieved by allowing every internet capable device to run training algorithms and predictive models with no software installation and by saving models in universally readable formats. Our prototype library is capable of training deep neural networks with synchronized, distributed stochastic gradient descent. MLitB offers several important opportunities for novel ML research, including: development of distributed learning algorithms, advancement of web GPU algorithms, novel field and mobile applications, privacy preserving computing, and green grid-computing. MLitB is available as open source software.",Data Mining and Machine Learning; Emerging Technologies; Mobile and Ubiquitous Computing; World Wide Web and Web Science; Software Engineering,Machine learning; Pervasive computing; Ubiquitous computing; Social computing; Mobile computing; Client–server systems; Distributed computing; Crowdsourcing,PeerJ Computer Science,1,45,8,0,2015
Evolution maps and applications,"Ofer, Biller; Irina, Rabaev; Klara, Kedem; Its’hak, Dinstein; Jihad J., El-Sana",10.7717/peerj-cs.39,"Common tasks in document analysis, such as binarization, line extraction etc., are still considered difficult for highly degraded text documents. Having reliable fundamental information regarding the characters of the document, such as the distribution of character dimensions and stroke width, can significantly improve the performance of these tasks. We introduce a novel perspective of the image data which maps the evolution of connected components along the change in gray scale threshold. The maps reveal significant information about the sets of elements in the document, such as characters, noise, stains, and words. The information is further employed to improve state of the art binarization algorithm, and achieve automatically character size estimation, line extraction, stroke width estimation, and feature distribution analysis, all of which are hard tasks for highly degraded documents.",Computer Vision; Digital Libraries,Text document analysis; Historical documents; Degraded documents; Connected component analysis,PeerJ Computer Science,2,27,13,0,2016
Measuring online social bubbles,"Dimitar, Nikolov; Diego F.M., Oliveira; Alessandro, Flammini; Filippo, Menczer",10.7717/peerj-cs.38,"Social media have become a prevalent channel to access information, spread ideas, and influence opinions. However, it has been suggested that social and algorithmic filtering may cause exposure to less diverse points of view. Here we quantitatively measure this kind of social bias at the collective level by mining a massive datasets of web clicks. Our analysis shows that collectively, people access information from a significantly narrower spectrum of sources through social media and email, compared to a search baseline. The significance of this finding for individual exposure is revealed by investigating the relationship between the diversity of information sources experienced by users at both the collective and individual levels in two datasets where individual users can be analyzed—Twitter posts and search logs. There is a strong correlation between collective and individual diversity, supporting the notion that when we use social media we find ourselves inside “social bubbles.” Our results could lead to a deeper understanding of how technology biases our exposure to new information.",Network Science and Online Social Networks; Social Computing; World Wide Web and Web Science,Bias; Diversity; Polarization; Filter bubble; Echo chamber; Web traffic,PeerJ Computer Science,1,35,5,3,2015
Automatic detection of potentially illegal online sales of elephant ivory via data mining,"Julio, Hernandez-Castro; David L., Roberts",10.7717/peerj-cs.10,"In this work, we developed an automated system to detect potentially illegal elephant ivory items for sale on eBay. Two law enforcement experts, with specific knowledge of elephant ivory identification, manually classified items on sale in the Antiques section of eBay UK over an 8 week period. This set the “Gold Standard” that we aim to emulate using data-mining. We achieved close to 93% accuracy with less data than the experts, as we relied entirely on metadata, but did not employ item descriptions or associated images, thus proving the potential and generality of our approach. The reported accuracy may be improved with the addition of text mining techniques for the analysis of the item description, and by applying image classification for the detection of Schreger lines, indicative of elephant ivory. However, any solution relying on images or text description could not be employed on other wildlife illegal markets where pictures can be missing or misleading and text absent (e.g., Instagram). In our setting, we gave human experts all available information while only using minimal information for our analysis. Despite this, we succeeded at achieving a very high accuracy. This work is an important first step in speeding up the laborious, tedious and expensive task of expert discovery of illegal trade over the internet. It will also allow for faster reporting to law enforcement and better accountability. We hope this will also contribute to reducing poaching, by making this illegal trade harder and riskier for those involved.",Data Mining and Machine Learning,eBay; Elephas; Loxodonta; Wildlife trade; Internet; Machine learning,PeerJ Computer Science,1,11,3,2,2015
Gender differences and bias in open source: pull request acceptance of women versus men,"Josh, Terrell; Andrew, Kofink; Justin, Middleton; Clarissa, Rainear; Emerson, Murphy-Hill; Chris, Parnin; Jon, Stallings",10.7717/peerj-cs.111,"Biases against women in the workplace have been documented in a variety of studies. This paper presents a large scale study on gender bias, where we compare acceptance rates of contributions from men versus women in an open source software community. Surprisingly, our results show that women’s contributions tend to be accepted more often than men’s. However, for contributors who are outsiders to a project and their gender is identifiable, men’s acceptance rates are higher. Our results suggest that although women on GitHub may be more competent overall, bias against them exists nonetheless.",Human-Computer Interaction; Social Computing; Programming Languages; Software Engineering,Gender; Bias; Open source; Software development; Software engineering,PeerJ Computer Science,3,36,11,5,2017
Challenges as enablers for high quality Linked Data: insights from the Semantic Publishing Challenge,"Anastasia, Dimou; Sahar, Vahdati; Angelo, Di Iorio; Christoph, Lange; Ruben, Verborgh; Erik, Mannens",10.7717/peerj-cs.105,"While most challenges organized so far in the Semantic Web domain are focused on comparing tools with respect to different criteria such as their features and competencies, or exploiting semantically enriched data, the Semantic Web Evaluation Challenges series, co-located with the ESWC Semantic Web Conference, aims to compare them based on their output, namely the produced dataset. The Semantic Publishing Challenge is one of these challenges. Its goal is to involve participants in extracting data from heterogeneous sources on scholarly publications, and producing Linked Data that can be exploited by the community itself. This paper reviews lessons learned from both (i) the overall organization of the Semantic Publishing Challenge, regarding the definition of the tasks, building the input dataset and forming the evaluation, and (ii) the results produced by the participants, regarding the proposed approaches, the used tools, the preferred vocabularies and the results produced in the three editions of 2014, 2015 and 2016. We compared these lessons to other Semantic Web Evaluation Challenges. In this paper, we (i) distill best practices for organizing such challenges that could be applied to similar events, and (ii) report observations on Linked Data publishing derived from the submitted solutions. We conclude that higher quality may be achieved when Linked Data is produced as a result of a challenge, because the competition becomes an incentive, while solutions become better with respect to Linked Data publishing best practices when they are evaluated against the rules of the challenge.",Data Science; Digital Libraries; Emerging Technologies; World Wide Web and Web Science,Linked Data; Semantic Web; Linked Data publishing; Semantic Publishing; Challenge; Survey,PeerJ Computer Science,3,37,0,11,2017
Utilizing social media and video games to control #DIY microscopes,"Maxime, Leblanc-Latour; Craig, Bryan; Andrew E., Pelling",10.7717/peerj-cs.139,"Open-source lab equipment is becoming more widespread with the popularization of fabrication tools such as 3D printers, laser cutters, CNC machines, open source microcontrollers and open source software. Although many pieces of common laboratory equipment have been developed, software control of these items is sometimes lacking. Specifically, control software that can be easily implemented and enable user-input and control over multiple platforms (PC, smartphone, web, etc.). The aim of this proof-of principle study was to develop and implement software for the control of a low-cost, 3D printed microscope. Here, we present two approaches which enable microscope control by exploiting the functionality of the social media platform Twitter or player actions inside of the videogame Minecraft. The microscope was constructed from a modified web-camera and implemented on a Raspberry Pi computer. Three aspects of microscope control were tested, including single image capture, focus control and time-lapse imaging. The Twitter embodiment enabled users to send ‘tweets’ directly to the microscope. Image data acquired by the microscope was then returned to the user through a Twitter reply and stored permanently on the photo-sharing platform Flickr, along with any relevant metadata. Local control of the microscope was also implemented by utilizing the video game Minecraft, in situations where Internet connectivity is not present or stable. A virtual laboratory was constructed inside the Minecraft world and player actions inside the laboratory were linked to specific microscope functions. Here, we present the methodology and results of these experiments and discuss possible limitations and future extensions of this work.",Network Science and Online Social Networks; Social Computing,Microscope; Do-It-Yourself; Open source; Raspberry Pi; Twitter; Flickr,PeerJ Computer Science,3,29,6,0,2017
CodonGenie: optimised ambiguous codon design tools,"Neil, Swainston; Andrew, Currin; Lucy, Green; Rainer, Breitling; Philip J., Day; Douglas B., Kell",10.7717/peerj-cs.120,"CodonGenie, freely available from http://codon.synbiochem.co.uk, is a simple web application for designing ambiguous codons to support protein mutagenesis applications. Ambiguous codons are derived from specific heterogeneous nucleotide mixtures, which create sequence degeneracy when synthesised in a DNA library. In directed evolution studies, such codons are carefully selected to encode multiple amino acids. For example, the codon NTN, where the code N denotes a mixture of all four nucleotides, will encode a mixture of phenylalanine, leucine, isoleucine, methionine and valine. Given a user-defined target collection of amino acids matched to an intended host organism, CodonGenie designs and analyses all ambiguous codons that encode the required amino acids. The codons are ranked according to their efficiency in encoding the required amino acids while minimising the inclusion of additional amino acids and stop codons. Organism-specific codon usage is also considered.",Bioengineering; Bioinformatics; Biotechnology; Computational Biology; Synthetic Biology,Directed evolution; Codon; Protein engineering; Industrial biotechnology; Mutagenesis; Enzyme engineering,PeerJ Computer Science,3,19,1,1,2017
The appropriation of GitHub for curation,"Yu, Wu; Na, Wang; Jessica, Kropczynski; John M., Carroll",10.7717/peerj-cs.134,"GitHub is a widely used online collaborative software development environment. In this paper, we describe curation projects as a new category of GitHub project that collects, evaluates, and preserves resources for software developers. We investigate: (1) what motivates software developers to curate resources; (2) why curation has occurred on GitHub; (3) how curated resources are used by software developers; and (4) how the GitHub platform could better support these practices. We conduct in-depth interviews with 16 software developers, each of whom hosts curation projects on GitHub. Our results suggest that the motivators that inspire software developers to curate resources on GitHub are similar to those that motivate them to participate in the development of open source projects. Convenient tools (e.g., Markdown syntax and Git version control system) and the opportunity to address professional needs of a large number of peers attract developers to engage in curation projects on GitHub. Benefits of curating on GitHub include learning opportunities, support for development work, and professional interaction. However, curation is limited by GitHub’s document structure, format, and a lack of key features, such as search. In light of this, we propose design possibilities to encourage and improve appropriations of GitHub for curation.",Human-Computer Interaction; Software Engineering,Curation; GitHub; Appropriation,PeerJ Computer Science,3,36,1,1,2017
Computational testing for automated preprocessing: a Matlab toolbox to enable large scale electroencephalography data processing,"Benjamin U., Cowley; Jussi, Korpela; Jari, Torniainen",10.7717/peerj-cs.108,"Electroencephalography (EEG) is a rich source of information regarding brain function. However, the preprocessing of EEG data can be quite complicated, due to several factors. For example, the distinction between true neural sources and noise is indeterminate; EEG data can also be very large. The various factors create a large number of subjective decisions with consequent risk of compound error. Existing tools present the experimenter with a large choice of analysis methods. Yet it remains a challenge for the researcher to integrate methods for batch-processing of the average large datasets, and compare methods to choose an optimal approach across the many possible parameter configurations. Additionally, many tools still require a high degree of manual decision making for, e.g. the classification of artefacts in channels, epochs or segments. This introduces extra subjectivity, is slow and is not reproducible. Batching and well-designed automation can help to regularise EEG preprocessing, and thus reduce human effort, subjectivity and consequent error. We present the computational testing for automated preprocessing (CTAP) toolbox, to facilitate: (i) batch-processing that is easy for experts and novices alike; (ii) testing and manual comparison of preprocessing methods. CTAP extends the existing data structure and functions from the well-known EEGLAB toolbox, based on Matlab and produces extensive quality control outputs. CTAP is available under MIT licence from https://github.com/bwrc/ctap.",Bioinformatics; Brain-Computer Interface,Computation; Testing; Automation; Preprocessing; EEGLAB; Electroencephalography; Signal processing,PeerJ Computer Science,3,29,11,1,2017
Triple Modular Redundancy verification via heuristic netlist analysis,"Giovanni, Beltrame",10.7717/peerj-cs.21,"Triple Modular Redundancy (TMR) is a common technique to protect memory elements for digital processing systems subject to radiation effects (such as in space, high-altitude, or near nuclear sources). This paper presents an approach to verify the correct implementation of TMR for the memory elements of a given netlist (i.e., a digital circuit specification) using heuristic analysis. The purpose is detecting any issues that might incur during the use of automatic tools for TMR insertion, optimization, place and route, etc. Our analysis does not require a testbench and can perform full, exhaustive coverage within less than an hour even for large designs. This is achieved by applying a divide et impera approach, splitting the circuit into smaller submodules without loss of generality, instead of applying formal verification to the whole netlist at once. The methodology has been applied to a production netlist of the LEON2-FT processor that had reported errors during radiation testing, successfully showing a number of unprotected memory elements, namely 351 flip-flops.",Computer Aided Design; Computer Architecture; Embedded Computing,Single event effects; Triple Modular Redundancy; Verification,PeerJ Computer Science,1,11,7,1,2015
Generation of high order geometry representations in Octree meshes,"Harald G., Klimach; Jens, Zudrop; Sabine P., Roller",10.7717/peerj-cs.35,"We propose a robust method to convert triangulated surface data into polynomial volume data. Such polynomial representations are required for high-order partial differential solvers, as low-order surface representations would diminish the accuracy of their solution. Our proposed method deploys a first order spatial bisection algorithm to find robustly an approximation of given geometries. The resulting voxelization is then used to generate Legendre polynomials of arbitrary degree. By embedding the locally defined polynomials in cubical elements of a coarser mesh, this method can reliably approximate even complex structures, like porous media. It thereby is possible to provide appropriate material definitions for high order discontinuous Galerkin schemes. We describe the method to construct the polynomial and how it fits into the overall mesh generation. Our discussion includes numerical properties of the method and we show some results from applying it to various geometries. We have implemented the described method in our mesh generator Seeder, which is publically available under a permissive open-source license.",Computer Aided Design; Scientific Computing and Simulation,Polynomial approximation; Discontinuous Galerkin; Mesh generation; High-order,PeerJ Computer Science,1,16,11,2,2015
Managing server clusters on intermittent power,"Navin, Sharma; Dilip, Krishnappa; Sean, Barker; David, Irwin; Prashant, Shenoy",10.7717/peerj-cs.34,"Reducing the energy footprint of data centers continues to receive significant attention due to both its financial and environmental impact. There are numerous methods that limit the impact of both factors, such as expanding the use of renewable energy or participating in automated demand-response programs. To take advantage of these methods, servers and applications must gracefully handle intermittent constraints in their power supply. In this paper, we propose blinking—metered transitions between a high-power active state and a low-power inactive state—as the primary abstraction for conforming to intermittent power constraints. We design Blink, an application-independent hardware–software platform for developing and evaluating blinking applications, and define multiple types of blinking policies. We then use Blink to design both a blinking version of memcached (BlinkCache) and a multimedia cache (GreenCache) to demonstrate how application characteristics affect the design of blink-aware distributed applications. Our results show that for BlinkCache, a load-proportional blinking policy combines the advantages of both activation and synchronous blinking for realistic Zipf-like popularity distributions and wind/solar power signals by achieving near optimal hit rates (within 15% of an activation policy), while also providing fairer access to the cache (within 2% of a synchronous policy) for equally popular objects. In contrast, for GreenCache, due to multimedia workload patterns, we find that a staggered load proportional blinking policy with replication of the first chunk of each video reduces the buffering time at all power levels, as compared to activation or load-proportional blinking policies.",Distributed and Parallel Computing; Multimedia; Operating Systems,Green data center; Intermittent power; Blink; Green cache; Memcached; Multimedia cache,PeerJ Computer Science,1,56,28,5,2015
CFSAN SNP Pipeline: an automated method for constructing SNP matrices from next-generation sequence data,"Steve, Davis; James B., Pettengill; Yan, Luo; Justin, Payne; Al, Shpuntoff; Hugh, Rand; Errol, Strain",10.7717/peerj-cs.20,"The analysis of next-generation sequence (NGS) data is often a fragmented step-wise process. For example, multiple pieces of software are typically needed to map NGS reads, extract variant sites, and construct a DNA sequence matrix containing only single nucleotide polymorphisms (i.e., a SNP matrix) for a set of individuals. The management and chaining of these software pieces and their outputs can often be a cumbersome and difficult task. Here, we present CFSAN SNP Pipeline, which combines into a single package the mapping of NGS reads to a reference genome with Bowtie2, processing of those mapping (BAM) files using SAMtools, identification of variant sites using VarScan, and production of a SNP matrix using custom Python scripts. We also introduce a Python package (CFSAN SNP Mutator) that when given a reference genome will generate variants of known position against which we validate our pipeline. We created 1,000 simulated Salmonella enterica sp. enterica Serovar Agona genomes at 100× and 20× coverage, each containing 500 SNPs, 20 single-base insertions and 20 single-base deletions. For the 100× dataset, the CFSAN SNP Pipeline recovered 98.9% of the introduced SNPs and had a false positive rate of 1.04 × 10−6; for the 20× dataset 98.8% of SNPs were recovered and the false positive rate was 8.34 × 10−7. Based on these results, CFSAN SNP Pipeline is a robust and accurate tool that it is among the first to combine into a single executable the myriad steps required to produce a SNP matrix from NGS data. Such a tool is useful to those working in an applied setting (e.g., food safety traceback investigations) as well as for those interested in evolutionary questions.",Bioinformatics,Phylogenetics; Single nucleotide polymorphism; Mapping; Whole genome sequencing; Python; Validation,PeerJ Computer Science,1,14,2,3,2015
Isolated guitar transcription using a deep belief network,"Gregory, Burlet; Abram, Hindle",10.7717/peerj-cs.109,"Music transcription involves the transformation of an audio recording to common music notation, colloquially referred to as sheet music. Manually transcribing audio recordings is a difficult and time-consuming process, even for experienced musicians. In response, several algorithms have been proposed to automatically analyze and transcribe the notes sounding in an audio recording; however, these algorithms are often general-purpose, attempting to process any number of instruments producing any number of notes sounding simultaneously. This paper presents a polyphonic transcription algorithm that is constrained to processing the audio output of a single instrument, specifically an acoustic guitar. The transcription system consists of a novel note pitch estimation algorithm that uses a deep belief network and multi-label learning techniques to generate multiple pitch estimates for each analysis frame of the input audio signal. Using a compiled dataset of synthesized guitar recordings for evaluation, the algorithm described in this work results in an 11% increase in the f-measure of note transcriptions relative to Zhou et al.’s (2009) transcription algorithm in the literature. This paper demonstrates the effectiveness of deep, multi-label learning for the task of polyphonic transcription.",Data Mining and Machine Learning; Data Science,Deep learning; Music information retrieval; Instrument transcription,PeerJ Computer Science,3,51,7,5,2017
Are suggestions from coupled file changes useful for perfective maintenance tasks?,"Jasmin, Ramadani; Stefan, Wagner",10.7717/peerj-cs.135,"Software maintenance is an important activity in the development process where maintenance team members leave and new members join over time. The identification of files which are changed together frequently has been proposed several times. Yet, existing studies about coupled file changes ignore the feedback from developers as well as the impact of these changes on the performance of maintenance and rather these studies rely on the analysis findings and expert evaluation.We investigate the usefulness of coupled file changes during perfective maintenance tasks when developers are inexperienced in programming or when they were new on the project. Using data mining on software repositories we identify files that are changed most frequently together in the past. We extract coupled file changes from the Git repository of a Java software system and join them with corresponding attributes from the versioning and issue tracking system and the project documentation. We present a controlled experiment involving 36 student participants in which we investigate if coupled file change suggestions influence the correctness of the task solutions and the required time to complete them.The results show that the use of coupled file change suggestions significantly increases the correctness of the solutions. However, there is only a minor effect on the time required to complete the perfective maintenance tasks. We also derived a set of the most useful attributes based on the developers’ feedback.Coupled file changes and a limited number of the proposed attributes are useful for inexperienced developers working on perfective maintenance tasks where although the developers using these suggestions solved more tasks, they still need time to understand and organize this information.",Data Science; Software Engineering,Data mining; Software repositories; Coupled changes; Git,PeerJ Computer Science,3,58,5,12,2017
ScholarLens: extracting competences from research publications for the automatic generation of semantic user profiles,"Bahar, Sateli; Felicitas, Löffler; Birgitta, König-Ries; René, Witte",10.7717/peerj-cs.121,"Scientists increasingly rely on intelligent information systems to help them in their daily tasks, in particular for managing research objects, like publications or datasets. The relatively young research field of Semantic Publishing has been addressing the question how scientific applications can be improved through semantically rich representations of research objects, in order to facilitate their discovery and re-use. To complement the efforts in this area, we propose an automatic workflow to construct semantic user profiles of scholars, so that scholarly applications, like digital libraries or data repositories, can better understand their users’ interests, tasks, and competences, by incorporating these user profiles in their design. To make the user profiles sharable across applications, we propose to build them based on standard semantic web technologies, in particular the Resource Description Framework (RDF) for representing user profiles and Linked Open Data (LOD) sources for representing competence topics. To avoid the cold start problem, we suggest to automatically populate these profiles by analyzing the publications (co-)authored by users, which we hypothesize reflect their research competences.We developed a novel approach, ScholarLens, which can automatically generate semantic user profiles for authors of scholarly literature. For modeling the competences of scholarly users and groups, we surveyed a number of existing linked open data vocabularies. In accordance with the LOD best practices, we propose an RDF Schema (RDFS) based model for competence records that reuses existing vocabularies where appropriate. To automate the creation of semantic user profiles, we developed a complete, automated workflow that can generate semantic user profiles by analyzing full-text research articles through various natural language processing (NLP) techniques. In our method, we start by processing a set of research articles for a given user. Competences are derived by text mining the articles, including syntactic, semantic, and LOD entity linking steps. We then populate a knowledge base in RDF format with user profiles containing the extracted competences.We implemented our approach as an open source library and evaluated our system through two user studies, resulting in mean average precision (MAP) of up to 95%. As part of the evaluation, we also analyze the impact of semantic zoning of research articles on the accuracy of the resulting profiles. Finally, we demonstrate how these semantic user profiles can be applied in a number of use cases, including article ranking for personalized search and finding scientists competent in a topic —e.g., to find reviewers for a paper.All software and datasets presented in this paper are available under open source licenses in the supplements and documented at http://www.semanticsoftware.info/semantic-user-profiling-peerj-2016-supplements. Additionally, development releases of ScholarLens are available on our GitHub page: https://github.com/SemanticSoftwareLab/ScholarLens.",Human-Computer Interaction; Artificial Intelligence; Data Mining and Machine Learning; Digital Libraries,Natural language processing; Semantic user profile; Semantic publishing; Scholarly user modeling; Linked open data,PeerJ Computer Science,3,57,7,10,2017
DANNP: an efficient artificial neural network pruning tool,"Mona, Alshahrani; Othman, Soufan; Arturo, Magana-Mora; Vladimir B., Bajic",10.7717/peerj-cs.137,"Artificial neural networks (ANNs) are a robust class of machine learning models and are a frequent choice for solving classification problems. However, determining the structure of the ANNs is not trivial as a large number of weights (connection links) may lead to overfitting the training data. Although several ANN pruning algorithms have been proposed for the simplification of ANNs, these algorithms are not able to efficiently cope with intricate ANN structures required for complex classification problems.We developed DANNP, a web-based tool, that implements parallelized versions of several ANN pruning algorithms. The DANNP tool uses a modified version of the Fast Compressed Neural Network software implemented in C++ to considerably enhance the running time of the ANN pruning algorithms we implemented. In addition to the performance evaluation of the pruned ANNs, we systematically compared the set of features that remained in the pruned ANN with those obtained by different state-of-the-art feature selection (FS) methods.Although the ANN pruning algorithms are not entirely parallelizable, DANNP was able to speed up the ANN pruning up to eight times on a 32-core machine, compared to the serial implementations. To assess the impact of the ANN pruning by DANNP tool, we used 16 datasets from different domains. In eight out of the 16 datasets, DANNP significantly reduced the number of weights by 70%–99%, while maintaining a competitive or better model performance compared to the unpruned ANN. Finally, we used a naïve Bayes classifier derived with the features selected as a byproduct of the ANN pruning and demonstrated that its accuracy is comparable to those obtained by the classifiers trained with the features selected by several state-of-the-art FS methods. The FS ranking methodology proposed in this study allows the users to identify the most discriminant features of the problem at hand. To the best of our knowledge, DANNP (publicly available at www.cbrc.kaust.edu.sa/dannp) is the only available and on-line accessible tool that provides multiple parallelized ANN pruning options. Datasets and DANNP code can be obtained at www.cbrc.kaust.edu.sa/dannp/data.php and https://doi.org/10.5281/zenodo.1001086.",Algorithms and Analysis of Algorithms; Artificial Intelligence; Data Mining and Machine Learning,Artificial neural networks; Pruning; Parallelization; Feature selection; Classification problems; Machine learning; Artificial inteligence,PeerJ Computer Science,3,61,5,4,2017
Visualising higher-dimensional space-time and space-scale objects as projections to ℝ3,"Ken, Arroyo Ohori; Hugo, Ledoux; Jantien, Stoter",10.7717/peerj-cs.123,"Objects of more than three dimensions can be used to model geographic phenomena that occur in space, time and scale. For instance, a single 4D object can be used to represent the changes in a 3D object’s shape across time or all its optimal representations at various levels of detail. In this paper, we look at how such higher-dimensional space-time and space-scale objects can be visualised as projections from ℝ4 to ℝ3. We present three projections that we believe are particularly intuitive for this purpose: (i) a simple ‘long axis’ projection that puts 3D objects side by side; (ii) the well-known orthographic and perspective projections; and (iii) a projection to a 3-sphere (S3) followed by a stereographic projection to ℝ3, which results in an inwards-outwards fourth axis. Our focus is in using these projections from ℝ4 to ℝ3, but they are formulated from ℝn to ℝn−1 so as to be easily extensible and to incorporate other non-spatial characteristics. We present a prototype interactive visualiser that applies these projections from 4D to 3D in real-time using the programmable pipeline and compute shaders of the Metal graphics API.",Graphics; Scientific Computing and Simulation; Spatial and Geographic Information Systems,Projections; Space-time; Space-scale; 4D visualisation; Nd gis,PeerJ Computer Science,3,86,10,0,2017
Towards a standard model for research in agent-based modeling and simulation,"Nuno, Fachada; Vitor V., Lopes; Rui C., Martins; Agostinho C., Rosa",10.7717/peerj-cs.36,"Agent-based modeling (ABM) is a bottom-up modeling approach, where each entity of the system being modeled is uniquely represented as an independent decision-making agent. ABMs are very sensitive to implementation details. Thus, it is very easy to inadvertently introduce changes which modify model dynamics. Such problems usually arise due to the lack of transparency in model descriptions, which constrains how models are assessed, implemented and replicated. In this paper, we present PPHPC, a model which aims to serve as a standard in agent based modeling research, namely, but not limited to, conceptual model specification, statistical analysis of simulation output, model comparison and parallelization studies. This paper focuses on the first two aspects (conceptual model specification and statistical analysis of simulation output), also providing a canonical implementation of PPHPC. The paper serves as a complete reference to the presented model, and can be used as a tutorial for simulation practitioners who wish to improve the way they communicate their ABMs.",Agents and Multi-Agent Systems; Scientific Computing and Simulation; Theory and Formal Methods,Agent-based modeling; Standard model; Statistical analysis of simulation output; ODD,PeerJ Computer Science,1,57,3,12,2015
Sharing analysis in the Pawns compiler,"Lee, Naish",10.7717/peerj-cs.22,"Pawns is a programming language under development that supports algebraic data types, polymorphism, higher order functions and “pure” declarative programming. It also supports impure imperative features including destructive update of shared data structures via pointers, allowing significantly increased efficiency for some operations. A novelty of Pawns is that all impure “effects” must be made obvious in the source code and they can be safely encapsulated in pure functions in a way that is checked by the compiler. Execution of a pure function can perform destructive updates on data structures that are local to or eventually returned from the function without risking modification of the data structures passed to the function. This paper describes the sharing analysis which allows impurity to be encapsulated. Aspects of the analysis are similar to other published work, but in addition it handles explicit pointers and destructive update, higher order functions including closures and pre- and post-conditions concerning sharing for functions.",Programming Languages,Functional programming language; Algebraic data type; Destructive update; Mutability; Effects; Aliasing analysis; Sharing analysis,PeerJ Computer Science,1,16,0,0,2015
Two-dimensional Kolmogorov complexity and an empirical validation of the Coding theorem method by compressibility,"Hector, Zenil; Fernando, Soler-Toscano; Jean-Paul, Delahaye; Nicolas, Gauvrit",10.7717/peerj-cs.23,"We propose a measure based upon the fundamental theoretical concept in algorithmic information theory that provides a natural approach to the problem of evaluating n-dimensional complexity by using an n-dimensional deterministic Turing machine. The technique is interesting because it provides a natural algorithmic process for symmetry breaking generating complex n-dimensional structures from perfectly symmetric and fully deterministic computational rules producing a distribution of patterns as described by algorithmic probability. Algorithmic probability also elegantly connects the frequency of occurrence of a pattern with its algorithmic complexity, hence effectively providing estimations to the complexity of the generated patterns. Experiments to validate estimations of algorithmic complexity based on these concepts are presented, showing that the measure is stable in the face of some changes in computational formalism and that results are in agreement with the results obtained using lossless compression algorithms when both methods overlap in their range of applicability. We then use the output frequency of the set of 2-dimensional Turing machines to classify the algorithmic complexity of the space-time evolutions of Elementary Cellular Automata.",Computational Biology; Artificial Intelligence; Theory and Formal Methods,Algorithmic complexity; Algorithmic probability; Kolmogorov–Chaitin complexity; Algorithmic information theory; Cellular automata; Solomonoff–Levin universal distribution; Information theory; Dimensional complexity; Image complexity; Small Turing machines,PeerJ Computer Science,1,40,13,2,2015
"Semantic representation of scientific literature: bringing claims, contributions and named entities onto the Linked Open Data cloud","Bahar, Sateli; René, Witte",10.7717/peerj-cs.37,"Motivation. Finding relevant scientific literature is one of the essential tasks researchers are facing on a daily basis. Digital libraries and web information retrieval techniques provide rapid access to a vast amount of scientific literature. However, no further automated support is available that would enable fine-grained access to the knowledge ‘stored’ in these documents. The emerging domain of Semantic Publishing aims at making scientific knowledge accessible to both humans and machines, by adding semantic annotations to content, such as a publication’s contributions, methods, or application domains. However, despite the promises of better knowledge access, the manual annotation of existing research literature is prohibitively expensive for wide-spread adoption. We argue that a novel combination of three distinct methods can significantly advance this vision in a fully-automated way: (i) Natural Language Processing (NLP) for Rhetorical Entity (RE) detection; (ii) Named Entity (NE) recognition based on the Linked Open Data (LOD) cloud; and (iii) automatic knowledge base construction for both NEs and REs using semantic web ontologies that interconnect entities in documents with the machine-readable LOD cloud.Results. We present a complete workflow to transform scientific literature into a semantic knowledge base, based on the W3C standards RDF and RDFS. A text mining pipeline, implemented based on the GATE framework, automatically extracts rhetorical entities of type Claims and Contributions from full-text scientific literature. These REs are further enriched with named entities, represented as URIs to the linked open data cloud, by integrating the DBpedia Spotlight tool into our workflow. Text mining results are stored in a knowledge base through a flexible export process that provides for a dynamic mapping of semantic annotations to LOD vocabularies through rules stored in the knowledge base. We created a gold standard corpus from computer science conference proceedings and journal articles, where Claim and Contribution sentences are manually annotated with their respective types using LOD URIs. The performance of the RE detection phase is evaluated against this corpus, where it achieves an average F-measure of 0.73. We further demonstrate a number of semantic queries that show how the generated knowledge base can provide support for numerous use cases in managing scientific literature.Availability. All software presented in this paper is available under open source licenses at http://www.semanticsoftware.info/semantic-scientific-literature-peerj-2015-supplements. Development releases of individual components are additionally available on our GitHub page at https://github.com/SemanticSoftwareLab.",Artificial Intelligence; Digital Libraries; Natural Language and Speech,Natural language processing; Semantic web; Semantic publishing,PeerJ Computer Science,1,33,8,7,2015
Finding the optimal Bayesian network given a constraint graph,"Jacob M., Schreiber; William S., Noble",10.7717/peerj-cs.122,"Despite recent algorithmic improvements, learning the optimal structure of a Bayesian network from data is typically infeasible past a few dozen variables. Fortunately, domain knowledge can frequently be exploited to achieve dramatic computational savings, and in many cases domain knowledge can even make structure learning tractable. Several methods have previously been described for representing this type of structural prior knowledge, including global orderings, super-structures, and constraint rules. While super-structures and constraint rules are flexible in terms of what prior knowledge they can encode, they achieve savings in memory and computational time simply by avoiding considering invalid graphs. We introduce the concept of a “constraint graph” as an intuitive method for incorporating rich prior knowledge into the structure learning task. We describe how this graph can be used to reduce the memory cost and computational time required to find the optimal graph subject to the encoded constraints, beyond merely eliminating invalid graphs. In particular, we show that a constraint graph can break the structure learning task into independent subproblems even in the presence of cyclic prior knowledge. These subproblems are well suited to being solved in parallel on a single machine or distributed across many machines without excessive communication cost.",Artificial Intelligence; Data Mining and Machine Learning; Data Science; Distributed and Parallel Computing,Bayesian network; Structure learning; Discrete optimization; Parallel processing; Big data,PeerJ Computer Science,3,21,4,3,2017
Fatal injection: a survey of modern code injection attack countermeasures,"Dimitris, Mitropoulos; Diomidis, Spinellis",10.7717/peerj-cs.136,"With a code injection attack (CIA) an attacker can introduce malicious code into a computer program or system that fails to properly encode data that comes from an untrusted source. A CIA can have different forms depending on the execution context of the application and the location of the programming flaw that leads to the attack. Currently, CIAs are considered one of the most damaging classes of application attacks since they can severely affect an organisation’s infrastructure and cause financial and reputational damage to it. In this paper we examine and categorize the countermeasures developed to detect the various attack forms. In particular, we identify two distinct categories. The first incorporates static program analysis tools used to eliminate flaws that can lead to such attacks during the development of the system. The second involves the use of dynamic detection safeguards that prevent code injection attacks while the system is in production mode. Our analysis is based on nonfunctional characteristics that are considered critical when creating security mechanisms. Such characteristics involve usability, overhead, implementation dependencies, false positives and false negatives. Our categorization and analysis can help both researchers and practitioners either to develop novel approaches, or use the appropriate mechanisms according to their needs.",Security and Privacy,Application security; Code injection attacks; Countermeasures; Static analysis; Dynamic prevention; Software vulnerabilities; Cross-site scripting,PeerJ Computer Science,3,178,2,2,2017
Research Articles in Simplified HTML: a Web-first format for HTML-based scholarly articles,"Silvio, Peroni; Francesco, Osborne; Angelo, Di Iorio; Andrea Giovanni, Nuzzolese; Francesco, Poggi; Fabio, Vitali; Enrico, Motta",10.7717/peerj-cs.132,"This paper introduces the Research Articles in Simplified HTML (or RASH), which is a Web-first format for writing HTML-based scholarly papers; it is accompanied by the RASH Framework, a set of tools for interacting with RASH-based articles. The paper also presents an evaluation that involved authors and reviewers of RASH articles submitted to the SAVE-SD 2015 and SAVE-SD 2016 workshops.RASH has been developed aiming to: be easy to learn and use; share scholarly documents (and embedded semantic annotations) through the Web; support its adoption within the existing publishing workflow.The evaluation study confirmed that RASH is ready to be adopted in workshops, conferences, and journals and can be quickly learnt by researchers who are familiar with HTML.The evaluation study also highlighted some issues in the adoption of RASH, and in general of HTML formats, especially by less technically savvy users. Moreover, additional tools are needed, e.g., for enabling additional conversions from/to existing formats such as OpenXML.RASH (and its Framework) is another step towards enabling the definition of formal representations of the meaning of the content of an article, facilitating its automatic discovery, enabling its linking to semantically related articles, providing access to data within the article in actionable form, and allowing integration of data between papers.RASH addresses the intrinsic needs related to the various users of a scholarly article: researchers (focussing on its content), readers (experiencing new ways for browsing it), citizen scientists (reusing available data formally defined within it through semantic annotations), publishers (using the advantages of new technologies as envisioned by the Semantic Publishing movement).RASH helps authors to focus on the organisation of their texts, supports them in the task of semantically enriching the content of articles, and leaves all the issues about validation, visualisation, conversion, and semantic data extraction to the various tools developed within its Framework.",Digital Libraries; World Wide Web and Web Science,Document conversion; XSLT; RASH; Semantic Publishing; Digital Publishing; Semantic Web,PeerJ Computer Science,3,40,5,3,2017
Nowcasting commodity prices using social media,"Jaewoo, Kim; Meeyoung, Cha; Jong Gun, Lee",10.7717/peerj-cs.126,"Gathering up-to-date information on food prices is critical in developing regions, as it allows policymakers and development practitioners to rely on accurate data on food security. This study explores the feasibility of utilizing social media as a new data source for predicting food security landscape in developing countries. Through a case study of Indonesia, we developed a nowcast model that monitors mentions of food prices on Twitter and forecasts daily price fluctuations of four major food commodities: beef, chicken, onion, and chilli. A longitudinal test over 15 months of data demonstrates that not only that the proposed model accurately predicts food prices, but it is also resilient to data scarcity. The high accuracy of the nowcast model is attributed to the observed trend that the volume of tweets mentioning food prices tends to increase on days when food prices change sharply. We discuss factors that affect the veracity of price quotations such as social network-wide sensitivity and user influence.",Data Science; Network Science and Online Social Networks; Social Computing,Nowcast; Price prediction; Food security; Social media; Price monitoring; Real-time; Twitter; Developing countries,PeerJ Computer Science,3,39,8,4,2017
Identification of high-efficiency 3′GG gRNA motifs in indexed FASTA files with ngg2,"Elisha D., Roberson",10.7717/peerj-cs.33,"CRISPR/Cas9 is emerging as one of the most-used methods of genome modification in organisms ranging from bacteria to human cells. However, the efficiency of editing varies tremendously site-to-site. A recent report identified a novel motif, called the 3′GG motif, which substantially increases the efficiency of editing at all sites tested in C. elegans. Furthermore, they highlighted that previously published gRNAs with high editing efficiency also had this motif. I designed a Python command-line tool, ngg2, to identify 3′GG gRNA sites from indexed FASTA files. As a proof-of-concept, I screened for these motifs in six model genomes: Saccharomyces cerevisiae, Caenorhabditis elegans, Drosophila melanogaster, Danio rerio, Mus musculus, and Homo sapiens. I also scanned the genomes of pig (Sus scrofa) and African elephant (Loxodonta africana) to demonstrate the utility in non-model organisms. I identified more than 60 million single match 3′GG motifs in these genomes. Greater than 61% of all protein coding genes in the reference genomes had at least one unique 3′GG gRNA site overlapping an exon. In particular, more than 96% of mouse and 93% of human protein coding genes have at least one unique, overlapping 3′GG gRNA. These identified sites can be used as a starting point in gRNA selection, and the ngg2 tool provides an important ability to identify 3′GG editing sites in any species with an available genome sequence.",Bioinformatics; Computational Biology; Data Science; Databases,gRNA; Motif discovery; Python; Open-source; CRISPR/Cas9; 3,PeerJ Computer Science,1,26,1,6,2015
Application of Graph Theory to the elaboration of personal genomic data for genealogical research,"Vincenzo, Palleschi; Luca, Pagani; Stefano, Pagnotta; Giuseppe, Amato; Sergio, Tofanelli",10.7717/peerj-cs.27,"In this communication a representation of the links between DNA-relatives based on Graph Theory is applied to the analysis of personal genomic data to obtain genealogical information. The method is tested on both simulated and real data and its applicability to the field of genealogical research is discussed. We envisage the proposed approach as a valid tool for a streamlined application to the publicly available data generated by many online personal genomic companies. In this way, anonymized matrices of pairwise genome sharing counts can help to improve the retrieval of genetic relationships between customers who provide explicit consent to the treatment of their data.",Computational Biology; Artificial Intelligence; Visual Analytics,DNA analysis; Personal genomics; Genealogy; Ancestry reconstruction; Statistical methods; Graph Theory; Genetic genealogy,PeerJ Computer Science,1,7,4,2,2015
Quantifying the effect of sentiment on information diffusion in social media,"Emilio, Ferrara; Zeyao, Yang",10.7717/peerj-cs.26,"Social media has become the main vehicle of information production and consumption online. Millions of users every day log on their Facebook or Twitter accounts to get updates and news, read about their topics of interest, and become exposed to new opportunities and interactions. Although recent studies suggest that the contents users produce will affect the emotions of their readers, we still lack a rigorous understanding of the role and effects of contents sentiment on the dynamics of information diffusion. This work aims at quantifying the effect of sentiment on information diffusion, to understand: (i) whether positive conversations spread faster and/or broader than negative ones (or vice-versa); (ii) what kind of emotions are more typical of popular conversations on social media; and, (iii) what type of sentiment is expressed in conversations characterized by different temporal dynamics. Our findings show that, at the level of contents, negative messages spread faster than positive ones, but positive ones reach larger audiences, suggesting that people are more inclined to share and favorite positive contents, the so-called positive bias. As for the entire conversations, we highlight how different temporal dynamics exhibit different sentiment patterns: for example, positive sentiment builds up for highly-anticipated events, while unexpected events are mainly characterized by negative sentiment. Our contribution represents a step forward to understand how the emotions expressed in short texts correlate with their spreading in online social ecosystems, and may help to craft effective policies and strategies for content generation and diffusion.",Data Mining and Machine Learning; Data Science; Network Science and Online Social Networks,Computational social science; Social networks; Social media; Sentiment analysis; Information diffusion,PeerJ Computer Science,1,64,5,0,2015
Networks of reader and country status: an analysis of Mendeley reader statistics,"Robin, Haunschild; Lutz, Bornmann; Loet, Leydesdorff",10.7717/peerj-cs.32,"The number of papers published in journals indexed by the Web of Science core collection is steadily increasing. In recent years, nearly two million new papers were published each year; somewhat more than one million papers when primary research papers are considered only (articles and reviews are the document types where primary research is usually reported or reviewed). However, who reads these papers? More precisely, which groups of researchers from which (self-assigned) scientific disciplines and countries are reading these papers? Is it possible to visualize readership patterns for certain countries, scientific disciplines, or academic status groups? One popular method to answer these questions is a network analysis. In this study, we analyze Mendeley readership data of a set of 1,133,224 articles and 64,960 reviews with publication year 2012 to generate three different networks: (1) The network based on disciplinary affiliations of Mendeley readers contains four groups: (i) biology, (ii) social sciences and humanities (including relevant computer sciences), (iii) bio-medical sciences, and (iv) natural sciences and engineering. In all four groups, the category with the addition “miscellaneous” prevails. (2) The network of co-readers in terms of professional status shows that a common interest in papers is mainly shared among PhD students, Master’s students, and postdocs. (3) The country network focusses on global readership patterns: a group of 53 nations is identified as core to the scientific enterprise, including Russia and China as well as two thirds of the OECD (Organisation for Economic Co-operation and Development) countries.",Data Science; Databases; Network Science and Online Social Networks,Mendeley; Network; Bibliometrics; Pajek; Altmetrics; VOSviewer,PeerJ Computer Science,1,27,6,2,2015
Accelerating the XGBoost algorithm using GPU computing,"Rory, Mitchell; Eibe, Frank",10.7717/peerj-cs.127,"We present a CUDA-based implementation of a decision tree construction algorithm within the gradient boosting library XGBoost. The tree construction algorithm is executed entirely on the graphics processing unit (GPU) and shows high performance with a variety of datasets and settings, including sparse input matrices. Individual boosting iterations are parallelised, combining two approaches. An interleaved approach is used for shallow trees, switching to a more conventional radix sort-based approach for larger depths. We show speedups of between 3× and 6× using a Titan X compared to a 4 core i7 CPU, and 1.2× using a Titan X compared to 2× Xeon CPUs (24 cores). We show that it is possible to process the Higgs dataset (10 million instances, 28 features) entirely within GPU memory. The algorithm is made available as a plug-in within the XGBoost library and fully supports all XGBoost features including classification, regression and ranking tasks.",Artificial Intelligence; Data Mining and Machine Learning; Data Science,Supervised machine learning; Gradient boosting; GPU computing,PeerJ Computer Science,3,25,11,36,2017
DGPathinter: a novel model for identifying driver genes via knowledge-driven matrix factorization with prior knowledge from interactome and pathways,"Jianing, Xi; Minghui, Wang; Ao, Li",10.7717/peerj-cs.133,"Cataloging mutated driver genes that confer a selective growth advantage for tumor cells from sporadic passenger mutations is a critical problem in cancer genomic research. Previous studies have reported that some driver genes are not highly frequently mutated and cannot be tested as statistically significant, which complicates the identification of driver genes. To address this issue, some existing approaches incorporate prior knowledge from an interactome to detect driver genes which may be dysregulated by interaction network context. However, altered operations of many pathways in cancer progression have been frequently observed, and prior knowledge from pathways is not exploited in the driver gene identification task. In this paper, we introduce a driver gene prioritization method called driver gene identification through pathway and interactome information (DGPathinter), which is based on knowledge-based matrix factorization model with prior knowledge from both interactome and pathways incorporated. When DGPathinter is applied on somatic mutation datasets of three types of cancers and evaluated by known driver genes, the prioritizing performances of DGPathinter are better than the existing interactome driven methods. The top ranked genes detected by DGPathinter are also significantly enriched for known driver genes. Moreover, most of the top ranked scored pathways given by DGPathinter are also cancer progression-associated pathways. These results suggest that DGPathinter is a useful tool to identify potential driver genes.",Bioinformatics; Computational Biology,Matrix factorization; Prior knowledge; Bioinformatics; Data mining,PeerJ Computer Science,3,67,4,3,2017
How are topics born? Understanding the research dynamics preceding the emergence of new areas,"Angelo A., Salatino; Francesco, Osborne; Enrico, Motta",10.7717/peerj-cs.119,"The ability to promptly recognise new research trends is strategic for many stakeholders, including universities, institutional funding bodies, academic publishers and companies. While the literature describes several approaches which aim to identify the emergence of new research topics early in their lifecycle, these rely on the assumption that the topic in question is already associated with a number of publications and consistently referred to by a community of researchers. Hence, detecting the emergence of a new research area at an embryonic stage, i.e., before the topic has been consistently labelled by a community of researchers and associated with a number of publications, is still an open challenge. In this paper, we begin to address this challenge by performing a study of the dynamics preceding the creation of new topics. This study indicates that the emergence of a new topic is anticipated by a significant increase in the pace of collaboration between relevant research areas, which can be seen as the ‘parents’ of the new topic. These initial findings (i) confirm our hypothesis that it is possible in principle to detect the emergence of a new topic at the embryonic stage, (ii) provide new empirical evidence supporting relevant theories in Philosophy of Science, and also (iii) suggest that new topics tend to emerge in an environment in which weakly interconnected research areas begin to cross-fertilise.",Artificial Intelligence; Data Science; Digital Libraries,Scholarly data; Topic emergence detection; Empirical study; Research trend detection; Topic discovery; Digital libraries,PeerJ Computer Science,3,50,13,10,2017
Power1D: a Python toolbox for numerical power estimates in experiments involving one-dimensional continua,"Todd C., Pataky",10.7717/peerj-cs.125,"The unit of experimental measurement in a variety of scientific applications is the one-dimensional (1D) continuum: a dependent variable whose value is measured repeatedly, often at regular intervals, in time or space. A variety of software packages exist for computing continuum-level descriptive statistics and also for conducting continuum-level hypothesis testing, but very few offer power computing capabilities, where ‘power’ is the probability that an experiment will detect a true continuum signal given experimental noise. Moreover, no software package yet exists for arbitrary continuum-level signal/noise modeling. This paper describes a package called power1d which implements (a) two analytical 1D power solutions based on random field theory (RFT) and (b) a high-level framework for computational power analysis using arbitrary continuum-level signal/noise modeling. First power1d’s two RFT-based analytical solutions are numerically validated using its random continuum generators. Second arbitrary signal/noise modeling is demonstrated to show how power1d can be used for flexible modeling well beyond the assumptions of RFT-based analytical solutions. Its computational demands are non-excessive, requiring on the order of only 30 s to execute on standard desktop computers, but with approximate solutions available much more rapidly. Its broad signal/noise modeling capabilities along with relatively rapid computations imply that power1d may be a useful tool for guiding experimentation involving multiple measurements of similar 1D continua, and in particular to ensure that an adequate number of measurements is made to detect assumed continuum signals.",Scientific Computing and Simulation; Programming Languages,Gaussian random fields; Time series; Random field theory; Hypothesis testing; Computational statistics; Data modeling,PeerJ Computer Science,3,18,16,0,2017
Guidelines for using empirical studies in software engineering education,"Fabian, Fagerholm; Marco, Kuhrmann; Jürgen, Münch",10.7717/peerj-cs.131,"Software engineering education is under constant pressure to provide students with industry-relevant knowledge and skills. Educators must address issues beyond exercises and theories that can be directly rehearsed in small settings. Industry training has similar requirements of relevance as companies seek to keep their workforce up to date with technological advances. Real-life software development often deals with large, software-intensive systems and is influenced by the complex effects of teamwork and distributed software development, which are hard to demonstrate in an educational environment. A way to experience such effects and to increase the relevance of software engineering education is to apply empirical studies in teaching. In this paper, we show how different types of empirical studies can be used for educational purposes in software engineering. We give examples illustrating how to utilize empirical studies, discuss challenges, and derive an initial guideline that supports teachers to include empirical studies in software engineering courses. Furthermore, we give examples that show how empirical studies contribute to high-quality learning outcomes, to student motivation, and to the awareness of the advantages of applying software engineering principles. Having awareness, experience, and understanding of the actions required, students are more likely to apply such principles under real-life constraints in their working life.",Computer Education; Software Engineering,Software Engineering Education; Computer Science Curricula; Teaching Methods; Empirical Studies; Experimentation; Education; Guideline,PeerJ Computer Science,3,65,0,12,2017
A redundancy-removing feature selection algorithm for nominal data,"Zhihua, Li; Wenqu, Gu",10.7717/peerj-cs.24,"No order correlation or similarity metric exists in nominal data, and there will always be more redundancy in a nominal dataset, which means that an efficient mutual information-based nominal-data feature selection method is relatively difficult to find. In this paper, a nominal-data feature selection method based on mutual information without data transformation, called the redundancy-removing more relevance less redundancy algorithm, is proposed. By forming several new information-related definitions and the corresponding computational methods, the proposed method can compute the information-related amount of nominal data directly. Furthermore, by creating a new evaluation function that considers both the relevance and the redundancy globally, the new feature selection method can evaluate the importance of each nominal-data feature. Although the presented feature selection method takes commonly used MIFS-like forms, it is capable of handling high-dimensional datasets without expensive computations. We perform extensive experimental comparisons of the proposed algorithm and other methods using three benchmarking nominal datasets with two different classifiers. The experimental results demonstrate the average advantage of the presented algorithm over the well-known NMIFS algorithm in terms of the feature selection and classification accuracy, which indicates that the proposed method has a promising performance.",Data Mining and Machine Learning; Data Science,Nominal data; Feature selection; Redundancy-removing; Mutual information,PeerJ Computer Science,1,24,3,5,2015
The interplay of physical and social wellbeing in older adults: investigating the relationship between physical training and social interactions with virtual social environments,"Iman, Khaghani Far; Michela, Ferron; Francisco, Ibarra; Marcos, Baez; Stefano, Tranquillini; Fabio, Casati; Nicola, Doppio",10.7717/peerj-cs.30,"Background. Regular physical activity can substantially improve the physical wellbeing of older adults, preventing several chronic diseases and increasing cognitive performance and mood. However, research has shown that older adults are the most sedentary segment of society, spending much of their time seated or inactive. A variety of barriers make it difficult for older adults to maintain an active lifestyle, including logistical difficulties in going to a gym (for some adults, leaving home can be challenging), reduced functional abilities, and lack of motivation. In this paper, we report on the design and evaluation of Gymcentral. A training application running on tablet was designed to allow older adults to follow a personalized home-based exercise program while being remotely assisted by a coach. The objective of the study was to assess if a virtual gym that enables virtual presence and social interaction is more motivating for training than the same virtual gym without social interaction.Methods. A total of 37 adults aged between 65 and 87 years old (28 females and 9 males, mean age = 71, sd = 5.8) followed a personalized home-based strength and balance training plan for eight weeks. The participants performed the exercises autonomously at home using the Gymcentral application. Participants were assigned to two training groups: the Social group used an application with persuasive and social functionalities, while the Control group used a basic version of the service with no persuasive and social features. We further explored the effects of social facilitation, and in particular of virtual social presence, in user participation to training sessions. Outcome measures were adherence, persistence and co-presence rate.Results. Participants in the Social group attended significantly more exercise sessions than the Control group, providing evidence of a better engagement in the training program. Besides the focus on social persuasion measures, the study also confirms that a virtual gym service is effective for supporting individually tailored home-based physical training for older adults. The study also confirms that social facilitation tools motivate users to train together in a virtual fitness environment.Discussion. The study confirms that Gymcentral increases the participation of older adults in physical training compare to a similar version of the application without social and persuasive features. In addition, a significant increase in the co-presence of the Social group indicates that social presence motivates the participants to join training sessions at the same time with the other participants. These results are encouraging, as they motivate further research into using home-based training programs as an opportunity to stay physically and socially active, especially for those who for various reasons are bound to stay at home.",Emerging Technologies; Human-Computer Interaction; Mobile and Ubiquitous Computing; Network Science and Online Social Networks,Older adults; Home-based physical intervention; Persuasion technology; Physical wellbeing; Social interactions,PeerJ Computer Science,1,60,7,0,2015
"How do you feel, developer? An explanatory theory of the impact of affects on programming performance","Daniel, Graziotin; Xiaofeng, Wang; Pekka, Abrahamsson",10.7717/peerj-cs.18,"Affects—emotions and moods—have an impact on cognitive activities and the working performance of individuals. Development tasks are undertaken through cognitive processes, yet software engineering research lacks theory on affects and their impact on software development activities. In this paper, we report on an interpretive study aimed at broadening our understanding of the psychology of programming in terms of the experience of affects while programming, and the impact of affects on programming performance. We conducted a qualitative interpretive study based on: face-to-face open-ended interviews, in-field observations, and e-mail exchanges. This enabled us to construct a novel explanatory theory of the impact of affects on development performance. The theory is explicated using an established taxonomy framework. The proposed theory builds upon the concepts of events, affects, attractors, focus, goals, and performance. Theoretical and practical implications are given.",Human-Computer Interaction; Social Computing; Software Engineering,Affects; Emotions; Productivity; Moods; Psychology of programming; Human aspects of software engineering; Process theory; Performance; Interpretivism; Theory building,PeerJ Computer Science,1,100,2,0,2015
Expected usability is not a valid indicator of experienced usability,"Meinald T., Thielsch; Ronja, Engel; Gerrit, Hirschfeld",10.7717/peerj-cs.19,"Usability is a core construct of website evaluation and inherently defined as interactive. Yet, when analysing first impressions of websites, expected usability, i.e., before use, is of interest. Here we investigate to what extend ratings of expected usability are related to (a) experienced usability, i.e., ratings after use, and (b) objective usability measures, i.e., task performance. Furthermore, we try to elucidate how ratings of expected usability are correlated to aesthetic judgments. In an experiment, 57 participants submitted expected usability ratings after the presentation of website screenshots in three viewing-time conditions (50, 500, and 10,000 ms) and after an interactive task (experienced usability). Additionally, objective usability measures (task completion and duration) and subjective aesthetics evaluations were recorded for each website. The results at both the group and individual level show that expected usability ratings are not significantly related either to experienced usability or objective usability measures. Instead, they are highly correlated with aesthetics ratings. Taken together, our results highlight the need for interaction in empirical website usability testing, even when exploring very early usability impressions. In our study, user ratings of expected usability were no valid proxy neither for objective usability nor for experienced website usability.",Human-Computer Interaction; World Wide Web and Web Science,Usability; Aesthetics; First impression; Website evaluation; Website,PeerJ Computer Science,1,49,3,1,2015
An algorithm for discovering Lagrangians automatically from data,"Daniel J.A., Hills; Adrian M., Grütter; Jonathan J., Hudson",10.7717/peerj-cs.31,"An activity fundamental to science is building mathematical models. These models are used to both predict the results of future experiments and gain insight into the structure of the system under study. We present an algorithm that automates the model building process in a scientifically principled way. The algorithm can take observed trajectories from a wide variety of mechanical systems and, without any other prior knowledge or tuning of parameters, predict the future evolution of the system. It does this by applying the principle of least action and searching for the simplest Lagrangian that describes the system’s behaviour. By generating this Lagrangian in a human interpretable form, it can also provide insight into the workings of the system.",Artificial Intelligence; Data Mining and Machine Learning; Scientific Computing and Simulation,Lagrangian; Physics; Least-action; Discovery,PeerJ Computer Science,1,22,5,1,2015
Mining known attack patterns from security-related events,"Nicandro, Scarabeo; Benjamin C.M., Fung; Rashid H., Khokhar",10.7717/peerj-cs.25,"Managed Security Services (MSS) have become an essential asset for companies to have in order to protect their infrastructure from hacking attempts such as unauthorized behaviour, denial of service (DoS), malware propagation, and anomalies. A proliferation of attacks has determined the need for installing more network probes and collecting more security-related events in order to assure the best coverage, necessary for generating incident responses. The increase in volume of data to analyse has created a demand for specific tools that automatically correlate events and gather them in pre-defined scenarios of attacks. Motivated by Above Security, a specialized company in the sector, and by National Research Council Canada (NRC), we propose a new data mining system that employs text mining techniques to dynamically relate security-related events in order to reduce analysis time, increase the quality of the reports, and automatically build correlated scenarios.",Computer Networks and Communications; Data Mining and Machine Learning; Security and Privacy,Security; Data mining; Text-mining; Correlation; Semantic; Log events; Security operation center; Managed security services,PeerJ Computer Science,1,28,8,0,2015
Integrating User eXperience practices into software development processes: implications of the UX characteristics,"Pariya, Kashfi; Agneta, Nilsson; Robert, Feldt",10.7717/peerj-cs.130,"User eXperience (UX) is a key factor in the success of software systems. Many software companies face challenges in their work with UX. Existing research does not analyze UX practices and challenges in relation to other software quality characteristics or, in particular, in relation to usability. A better understanding of these challenges can help researchers and practitioners better address them in the future. In this empirical study, we have interviewed 17 practitioners with different backgrounds and occupations from eight software development companies. Their responses are coded, and analyzed with thematic analysis. We report eight themes of challenges that practitioners face in their work with UX. While some of these challenges partly overlap with those reported in existing literature about usability or other software quality characteristics, the participants of our study either view many of the challenges as unique to UX, or more severe in the case of UX. Although at a superficial level challenges of UX and other quality characteristics overlap, we differentiate these challenges at a deeper level through the five main characteristics of UX: subjective, holistic, dynamic, context-dependent and worthwhile. In particular, we identified that these characteristics have at least 20 implications (i.e. additional difficulties) for day-to-day work of practitioners. We found that 11 of these implications have been previously reported in literature. However, to the best of our knowledge, the remaining nine implications are unique to our study. These implications can explain why practitioners perceive the challenges to be more severe than for other quality characteristics. Most importantly, they can explain the industry’s lopsided focus on the pragmatic aspect of UX. Our findings can be useful for researchers in identifying new and industry-relevant research areas and for practitioners to learn from empirically investigated challenges in UX work, and base their improvement efforts on such knowledge. Identifying and investigating the overlaps underlines the importance of these challenges, and can also help finding research areas not only for enhancing UX work but also software quality in general. It also makes it easier for practitioners to spot, better understand as well as find mitigation strategies for UX, through learning from past experiences and developments in the area of software quality.",Human-Computer Interaction; Software Engineering,Usability; Software quality; Quality requirements; User experience; Non-functional requirements,PeerJ Computer Science,3,46,10,4,2017
Prediction of protein function using a deep convolutional neural network ensemble,"Evangelia I., Zacharaki",10.7717/peerj-cs.124,"The availability of large databases containing high resolution three-dimensional (3D) models of proteins in conjunction with functional annotation allows the exploitation of advanced supervised machine learning techniques for automatic protein function prediction.In this work, novel shape features are extracted representing protein structure in the form of local (per amino acid) distribution of angles and amino acid distances, respectively. Each of the multi-channel feature maps is introduced into a deep convolutional neural network (CNN) for function prediction and the outputs are fused through support vector machines or a correlation-based k-nearest neighbor classifier. Two different architectures are investigated employing either one CNN per multi-channel feature set, or one CNN per image channel.Cross validation experiments on single-functional enzymes (n = 44,661) from the PDB database achieved 90.1% correct classification, demonstrating an improvement over previous results on the same dataset when sequence similarity was not considered.The automatic prediction of protein function can provide quick annotations on extensive datasets opening the path for relevant applications, such as pharmacological target identification. The proposed method shows promise for structure-based protein function prediction, but sufficient data may not yet be available to properly assess the method’s performance on non-homologous proteins and thus reduce the confounding factor of evolutionary relationships.",Bioinformatics; Computational Biology; Data Mining and Machine Learning,Enzyme classification; Function predition; Deep learning; Convolutional neural networks; Structure representation,PeerJ Computer Science,3,36,5,3,2017
VESPA: Very large-scale Evolutionary and Selective Pressure Analyses,"Andrew E., Webb; Thomas A., Walsh; Mary J., O’Connell",10.7717/peerj-cs.118,"Large-scale molecular evolutionary analyses of protein coding sequences requires a number of preparatory inter-related steps from finding gene families, to generating alignments and phylogenetic trees and assessing selective pressure variation. Each phase of these analyses can represent significant challenges, particularly when working with entire proteomes (all protein coding sequences in a genome) from a large number of species.We present VESPA, software capable of automating a selective pressure analysis using codeML in addition to the preparatory analyses and summary statistics. VESPA is written in python and Perl and is designed to run within a UNIX environment.We have benchmarked VESPA and our results show that the method is consistent, performs well on both large scale and smaller scale datasets, and produces results in line with previously published datasets.Large-scale gene family identification, sequence alignment, and phylogeny reconstruction are all important aspects of large-scale molecular evolutionary analyses. VESPA provides flexible software for simplifying these processes along with downstream selective pressure variation analyses. The software automatically interprets results from codeML and produces simplified summary files to assist the user in better understanding the results. VESPA may be found at the following website: http://www.mol-evol.org/VESPA.",Bioinformatics; Computational Biology,Selective pressure analysis; Protein molecular evolution; Large-scale comparative genomics; Gene family evolution; Positive selection,PeerJ Computer Science,3,24,2,3,2017
A redesign of OGC Symbology Encoding standard for sharing cartography,"Erwan, Bocher; Olivier, Ertz",10.7717/peerj-cs.143,"Despite most Spatial Data Infrastructures offering service-based visualization of geospatial data, requirements are often at a very basic level leading to poor quality of maps. This is a general observation for any geospatial architecture as soon as open standards as those of the Open Geospatial Consortium (OGC) are applied. To improve the situation, this paper does focus on improvements at the portrayal interoperability side by considering standardization aspects. We propose two major redesign recommendations. First to consolidate the cartographic theory at the core of the OGC Symbology Encoding standard. Secondly to build the standard in a modular way so as to be ready to be extended with upcoming future cartographic requirements. Thus, we start by defining portrayal interoperability by means of typical-use cases that frame the concept of sharing cartography. Then we bring to light the strengths and limits of the relevant open standards to consider in this context. Finally we propose a set of recommendations to overcome the limits so as to make these use cases a true reality. Even if the definition of a cartographic-oriented standard is not able to act as a complete cartographic design framework by itself, we argue that pushing forward the standardization work dedicated to cartography is a way to share and disseminate good practices and finally to improve the quality of the visualizations.",Spatial and Geographic Information Systems; World Wide Web and Web Science,Cartography; Spatial Data Infrastructure; Open standards; Portrayal interoperability; Open Geospatial Consortium,PeerJ Computer Science,4,65,17,0,2018
Combining active learning suggestions,"Alasdair, Tran; Cheng Soon, Ong; Christian, Wolf",10.7717/peerj-cs.157,"We study the problem of combining active learning suggestions to identify informative training examples by empirically comparing methods on benchmark datasets. Many active learning heuristics for classification problems have been proposed to help us pick which instance to annotate next. But what is the optimal heuristic for a particular source of data? Motivated by the success of methods that combine predictors, we combine active learners with bandit algorithms and rank aggregation methods. We demonstrate that a combination of active learners outperforms passive learning in large benchmark datasets and removes the need to pick a particular active learner a priori. We discuss challenges to finding good rewards for bandit approaches and show that rank aggregation performs well.",Data Mining and Machine Learning,Active learning; Bandit; Rank aggregation; Benchmark; Multiclass classification,PeerJ Computer Science,4,36,12,12,2018
Deep learning for constructing microblog behavior representation to identify social media user’s personality,"Xiaoqian, Liu; Tingshao, Zhu",10.7717/peerj-cs.81,"Due to the rapid development of information technology, the Internet has gradually become a part of everyday life. People would like to communicate with friends to share their opinions on social networks. The diverse behavior on socials networks is an ideal reflection of users’ personality traits. Existing behavior analysis methods for personality prediction mostly extract behavior attributes with heuristic analysis. Although they work fairly well, they are hard to extend and maintain. In this paper, we utilize a deep learning algorithm to build a feature learning model for personality prediction, which could perform an unsupervised extraction of the Linguistic Representation Feature Vector (LRFV) activity without supervision from text actively published on the Sina microblog. Compared with other feature extractsion methods, LRFV, as an abstract representation of microblog content, could describe a user’s semantic information more objectively and comprehensively. In the experiments, the personality prediction model is built using a linear regression algorithm, and different attributes obtained through different feature extraction methods are taken as input of the prediction model, respectively. The results show that LRFV performs better in microblog behavior descriptions, and improves the performance of the personality prediction model.",Artificial Intelligence; Natural Language and Speech; Social Computing,Personality prediction; Social media behavior; Deep learning; Feature learning,PeerJ Computer Science,2,26,4,5,2016
Implementing generalized deep-copy in MPI,"Joss, Whittle; Rita, Borgo; Mark W., Jones",10.7717/peerj-cs.95,"In this paper, we introduce a framework for implementing deep copy on top of MPI. The process is initiated by passing just the root object of the dynamic data structure. Our framework takes care of all pointer traversal, communication, copying and reconstruction on receiving nodes. The benefit of our approach is that MPI users can deep copy complex dynamic data structures without the need to write bespoke communication or serialize/deserialize methods for each object. These methods can present a challenging implementation problem that can quickly become unwieldy to maintain when working with complex structured data. This paper demonstrates our generic implementation, which encapsulates both approaches. We analyze the approach with a variety of structures (trees, graphs (including complete graphs) and rings) and demonstrate that it performs comparably to hand written implementations, using a vastly simplified programming interface. We make the source code available completely as a convenient header file.",Computer Networks and Communications; Distributed and Parallel Computing; Programming Languages,MPI extension library; Deep copy; Serialization; Marshalling; Dynamic data structures; Deserialization; Unmarshalling,PeerJ Computer Science,2,24,10,0,2016
Tolerance-based interaction: a new model targeting opinion formation and diffusion in social networks,"Alexandru, Topirceanu; Mihai, Udrescu; Mircea, Vladutiu; Radu, Marculescu",10.7717/peerj-cs.42,"One of the main motivations behind social network analysis is the quest for understanding opinion formation and diffusion. Previous models have limitations, as they typically assume opinion interaction mechanisms based on thresholds which are either fixed or evolve according to a random process that is external to the social agent. Indeed, our empirical analysis on large real-world datasets such as Twitter, Meme Tracker, and Yelp, uncovers previously unaccounted for dynamic phenomena at population-level, namely the existence of distinct opinion formation phases and social balancing. We also reveal that a phase transition from an erratic behavior to social balancing can be triggered by network topology and by the ratio of opinion sources. Consequently, in order to build a model that properly accounts for these phenomena, we propose a new (individual-level) opinion interaction model based on tolerance. As opposed to the existing opinion interaction models, the new tolerance model assumes that individual’s inner willingness to accept new opinions evolves over time according to basic human traits. Finally, by employing discrete event simulation on diverse social network topologies, we validate our opinion interaction model and show that, although the network size and opinion source ratio are important, the phase transition to social balancing is mainly fostered by the democratic structure of the small-world topology.",Network Science and Online Social Networks; Scientific Computing and Simulation; Social Computing,Social networks; Opinion diffusion; Phase transition; Discrete event simulation; Tolerance,PeerJ Computer Science,2,67,12,0,2016
MonoPhy: a simple R package to find and visualize monophyly issues,"Orlando, Schwery; Brian C., O’Meara",10.7717/peerj-cs.56,"Background. The monophyly of taxa is an important attribute of a phylogenetic tree. A lack of it may hint at shortcomings of either the tree or the current taxonomy, or can indicate cases of incomplete lineage sorting or horizontal gene transfer. Whichever is the reason, a lack of monophyly can misguide subsequent analyses. While monophyly is conceptually simple, it is manually tedious and time consuming to assess on modern phylogenies of hundreds to thousands of species.Results. The R package MonoPhy allows assessment and exploration of monophyly of taxa in a phylogeny. It can assess the monophyly of genera using the phylogeny only, and with an additional input file any other desired higher order taxa or unranked groups can be checked as well.Conclusion. Summary tables, easily subsettable results and several visualization options allow quick and convenient exploration of monophyly issues, thus making MonoPhy a valuable tool for any researcher working with phylogenies.",Bioinformatics; Computational Biology,Phylogeny; Evolution; Monophyly; R package; Taxonomy; Rogue taxa; Tree conflict; Horizontal gene transfer; Incomplete lineage sorting,PeerJ Computer Science,2,26,1,1,2016
Navigating the massive world of reddit: using backbone networks to map user interests in social media,"Randal S., Olson; Zachary P., Neal",10.7717/peerj-cs.4,"In the massive online worlds of social media, users frequently rely on organizing themselves around specific topics of interest to find and engage with like-minded people. However, navigating these massive worlds and finding topics of specific interest often proves difficult because the worlds are mostly organized haphazardly, leaving users to find relevant interests by word of mouth or using a basic search feature. Here, we report on a method using the backbone of a network to create a map of the primary topics of interest in any social network. To demonstrate the method, we build an interest map for the social news web site reddit and show how such a map could be used to navigate a social media world. Moreover, we analyze the network properties of the reddit social network and find that it has a scale-free, small-world, and modular community structure, much like other online social networks such as Facebook and Twitter. We suggest that the integration of interest maps into popular social media platforms will assist users in organizing themselves into more specific interest groups, which will help alleviate the overcrowding effect often observed in large online communities.",Network Science and Online Social Networks; Visual Analytics,Social networks; reddit; Mapping; Network backbone extraction; Social network navigation,PeerJ Computer Science,1,39,5,0,2015
Capturing the interplay of dynamics and networks through parameterizations of Laplacian operators,"Xiaoran, Yan; Shang-hua, Teng; Kristina, Lerman; Rumi, Ghosh",10.7717/peerj-cs.57,"We study the interplay between a dynamical process and the structure of the network on which it unfolds using the parameterized Laplacian framework. This framework allows for defining and characterizing an ensemble of dynamical processes on a network beyond what the traditional Laplacian is capable of modeling. This, in turn, allows for studying the impact of the interaction between dynamics and network topology on the quality-measure of network clusters and centrality, in order to effectively identify important vertices and communities in the network. Specifically, for each dynamical process in this framework, we define a centrality measure that captures a vertex’s participation in the dynamical process on a given network and also define a function that measures the quality of every subset of vertices as a potential cluster (or community) with respect to this process. We show that the subset-quality function generalizes the traditional conductance measure for graph partitioning. We partially justify our choice of the quality function by showing that the classic Cheeger’s inequality, which relates the conductance of the best cluster in a network with a spectral quantity of its Laplacian matrix, can be extended to the parameterized Laplacian. The parameterized Laplacian framework brings under the same umbrella a surprising variety of dynamical processes and allows us to systematically compare the different perspectives they create on network structure.",Network Science and Online Social Networks,Network; Community structure; Spectral graph theory; Centrality; Dynamical process,PeerJ Computer Science,2,59,8,4,2016
An interactive audio-visual installation using ubiquitous hardware and web-based software deployment,"Tiago Fernandes, Tavares",10.7717/peerj-cs.5,"This paper describes an interactive audio-visual musical installation, namely MOTUS, that aims at being deployed using low-cost hardware and software. This was achieved by writing the software as a web application and using only hardware pieces that are built-in most modern personal computers. This scenario implies in specific technical restrictions, which leads to solutions combining both technical and artistic aspects of the installation. The resulting system is versatile and can be freely used from any computer with Internet access. Spontaneous feedback from the audience has shown that the provided experience is interesting and engaging, regardless of the use of minimal hardware.",Human-Computer Interaction; Multimedia,Audio-visual interaction; Computer music; Webcam,PeerJ Computer Science,1,20,6,1,2015
A systematic analysis of the science of sandboxing,"Michael, Maass; Adam, Sales; Benjamin, Chung; Joshua, Sunshine",10.7717/peerj-cs.43,"Sandboxes are increasingly important building materials for secure software systems. In recognition of their potential to improve the security posture of many systems at various points in the development lifecycle, researchers have spent the last several decades developing, improving, and evaluating sandboxing techniques. What has been done in this space? Where are the barriers to advancement? What are the gaps in these efforts? We systematically analyze a decade of sandbox research from five top-tier security and systems conferences using qualitative content analysis, statistical clustering, and graph-based metrics to answer these questions and more. We find that the term “sandbox” currently has no widely accepted or acceptable definition. We use our broad scope to propose the first concise and comprehensive definition for “sandbox” that consistently encompasses research sandboxes. We learn that the sandboxing landscape covers a range of deployment options and policy enforcement techniques collectively capable of defending diverse sets of components while mitigating a wide range of vulnerabilities. Researchers consistently make security, performance, and applicability claims about their sandboxes and tend to narrowly define the claims to ensure they can be evaluated. Those claims are validated using multi-faceted strategies spanning proof, analytical analysis, benchmark suites, case studies, and argumentation. However, we find two cases for improvement: (1) the arguments researchers present are often ad hoc and (2) sandbox usability is mostly uncharted territory. We propose ways to structure arguments to ensure they fully support their corresponding claims and suggest lightweight means of evaluating sandbox usability.",Security and Privacy; Operating Systems; Software Engineering,Sandboxing; Qualitative content analysis; Software protection; Access control; Security; Security validation,PeerJ Computer Science,2,114,7,6,2016
Iterative guided image fusion,"Alexander, Toet",10.7717/peerj-cs.80,"We propose a multi-scale image fusion scheme based on guided filtering. Guided filtering can effectively reduce noise while preserving detail boundaries. When applied in an iterative mode, guided filtering selectively eliminates small scale details while restoring larger scale edges. The proposed multi-scale image fusion scheme achieves spatial consistency by using guided filtering both at the decomposition and at the recombination stage of the multi-scale fusion process. First, size-selective iterative guided filtering is applied to decompose the source images into approximation and residual layers at multiple spatial scales. Then, frequency-tuned filtering is used to compute saliency maps at successive spatial scales. Next, at each spatial scale binary weighting maps are obtained as the pixelwise maximum of corresponding source saliency maps. Guided filtering of the binary weighting maps with their corresponding source images as guidance images serves to reduce noise and to restore spatial consistency. The final fused image is obtained as the weighted recombination of the individual residual layers and the mean of the approximation layers at the coarsest spatial scale. Application to multiband visual (intensified) and thermal infrared imagery demonstrates that the proposed method obtains state-of-the-art performance for the fusion of multispectral nightvision images. The method has a simple implementation and is computationally efficient.",Computer Vision,Image fusion; Guided filter; Saliency; Infrared; Nightvision; Thermal imagery; Intensified imagery,PeerJ Computer Science,2,80,9,4,2016
Comparison and benchmark of name-to-gender inference services,"Lucía, Santamaría; Helena, Mihaljević",10.7717/peerj-cs.156,"The increased interest in analyzing and explaining gender inequalities in tech, media, and academia highlights the need for accurate inference methods to predict a person’s gender from their name. Several such services exist that provide access to large databases of names, often enriched with information from social media profiles, culture-specific rules, and insights from sociolinguistics. We compare and benchmark five name-to-gender inference services by applying them to the classification of a test data set consisting of 7,076 manually labeled names. The compiled names are analyzed and characterized according to their geographical and cultural origin. We define a series of performance metrics to quantify various types of classification errors, and define a parameter tuning procedure to search for optimal values of the services’ free parameters. Finally, we perform benchmarks of all services under study regarding several scenarios where a particular metric is to be optimized.",Data Mining and Machine Learning; Data Science; Databases; Digital Libraries,Name-based gender inference; Classification algorithms; Performance evaluation; Gender analysis; Scientometrics; Bibliometrics,PeerJ Computer Science,4,28,4,12,2018
Sustainable computational science: the ReScience initiative,"Nicolas P., Rougier; Konrad, Hinsen; Frédéric, Alexandre; Thomas, Arildsen; Lorena A., Barba; Fabien C.Y., Benureau; C. Titus, Brown; Pierre, de Buyl; Ozan, Caglayan; Andrew P., Davison; Marc-André, Delsuc; Georgios, Detorakis; Alexandra K., Diem; Damien, Drix; Pierre, Enel; Benoît, Girard; Olivia, Guest; Matt G., Hall; Rafael N., Henriques; Xavier, Hinaut; Kamil S., Jaron; Mehdi, Khamassi; Almar, Klein; Tiina, Manninen; Pietro, Marchesi; Daniel, McGlinn; Christoph, Metzner; Owen, Petchey; Hans Ekkehard, Plesser; Timothée, Poisot; Karthik, Ram; Yoav, Ram; Etienne, Roesch; Cyrille, Rossant; Vahid, Rostami; Aaron, Shifman; Joseph, Stachelek; Marcel, Stimberg; Frank, Stollmeier; Federico, Vaggi; Guillaume, Viejo; Julien, Vitay; Anya E., Vostinar; Roman, Yurchak; Tiziano, Zito",10.7717/peerj-cs.142,"Computer science offers a large set of tools for prototyping, writing, running, testing, validating, sharing and reproducing results; however, computational science lags behind. In the best case, authors may provide their source code as a compressed archive and they may feel confident their research is reproducible. But this is not exactly true. James Buckheit and David Donoho proposed more than two decades ago that an article about computational results is advertising, not scholarship. The actual scholarship is the full software environment, code, and data that produced the result. This implies new workflows, in particular in peer-reviews. Existing journals have been slow to adapt: source codes are rarely requested and are hardly ever actually executed to check that they produce the results advertised in the article. ReScience is a peer-reviewed journal that targets computational research and encourages the explicit replication of already published research, promoting new and open-source implementations in order to ensure that the original research can be replicated from its description. To achieve this goal, the whole publishing chain is radically different from other traditional scientific journals. ReScience resides on GitHub where each new implementation of a computational study is made available together with comments, explanations, and software tests.",Data Science; Digital Libraries; Scientific Computing and Simulation; Social Computing,Computational science; Open science; Publication; Reproducible; Replicable; Sustainable; GitHub; Open peer-review,PeerJ Computer Science,3,31,1,0,2017
Supervised deep learning embeddings for the prediction of cervical cancer diagnosis,"Kelwin, Fernandes; Davide, Chicco; Jaime S., Cardoso; Jessica, Fernandes",10.7717/peerj-cs.154,"Cervical cancer remains a significant cause of mortality all around the world, even if it can be prevented and cured by removing affected tissues in early stages. Providing universal and efficient access to cervical screening programs is a challenge that requires identifying vulnerable individuals in the population, among other steps. In this work, we present a computationally automated strategy for predicting the outcome of the patient biopsy, given risk patterns from individual medical records. We propose a machine learning technique that allows a joint and fully supervised optimization of dimensionality reduction and classification models. We also build a model able to highlight relevant properties in the low dimensional space, to ease the classification of patients. We instantiated the proposed approach with deep learning architectures, and achieved accurate prediction results (top area under the curve AUC = 0.6875) which outperform previously developed methods, such as denoising autoencoders. Additionally, we explored some clinical findings from the embedding spaces, and we validated them through the medical literature, making them reliable for physicians and biomedical researchers.",Bioinformatics; Computational Biology; Artificial Intelligence; Data Mining and Machine Learning,Dimensionality reduction; Health-care informatics; Denoising autoencoder; Autoencoder; Biomedical informatics; Binary classification; Deep learning; Cervical cancer; Artificial neural networks; Health informatics,PeerJ Computer Science,4,49,7,6,2018
A GPU-based solution for fast calculation of the betweenness centrality in large weighted networks,"Rui, Fan; Ke, Xu; Jichang, Zhao",10.7717/peerj-cs.140,"Betweenness, a widely employed centrality measure in network science, is a decent proxy for investigating network loads and rankings. However, its extremely high computational cost greatly hinders its applicability in large networks. Although several parallel algorithms have been presented to reduce its calculation cost for unweighted networks, a fast solution for weighted networks, which are commonly encountered in many realistic applications, is still lacking. In this study, we develop an efficient parallel GPU-based approach to boost the calculation of the betweenness centrality (BC) for large weighted networks. We parallelize the traditional Dijkstra algorithm by selecting more than one frontier vertex each time and then inspecting the frontier vertices simultaneously. By combining the parallel SSSP algorithm with the parallel BC framework, our GPU-based betweenness algorithm achieves much better performance than its CPU counterparts. Moreover, to further improve performance, we integrate the work-efficient strategy, and to address the load-imbalance problem, we introduce a warp-centric technique, which assigns many threads rather than one to a single frontier vertex. Experiments on both realistic and synthetic networks demonstrate the efficiency of our solution, which achieves 2.9× to 8.44× speedups over the parallel CPU implementation. Our algorithm is open-source and free to the community; it is publicly available through https://dx.doi.org/10.6084/m9.figshare.4542405. Considering the pervasive deployment and declining price of GPUs in personal computers and servers, our solution will offer unprecedented opportunities for exploring betweenness-related problems and will motivate follow-up efforts in network science.",Distributed and Parallel Computing; Network Science and Online Social Networks,Parallel computing; GPU computing; Betweenness centrality; Weighted networks,PeerJ Computer Science,3,45,3,9,2017
"The doctor’s digital double: how warmth, competence, and animation promote adherence intention","Zhengyan, Dai; Karl F., MacDorman",10.7717/peerj-cs.168,"Each year, patient nonadherence to treatment advice costs the US healthcare system more than $300 billion and results in 250,000 deaths. Developing virtual consultations to promote adherence could improve public health while cutting healthcare costs and usage. However, inconsistencies in the realism of computer-animated humans may cause them to appear eerie, a phenomenon termed the uncanny valley. Eeriness could reduce a virtual doctor’s credibility and patients’ adherence.In a 2 × 2 × 2 between-groups posttest-only experiment, 738 participants played the role of a patient in a hypothetical virtual consultation with a doctor. The consultation varied in the doctor’s Character (good or poor bedside manner), Outcome (received a fellowship or sued for malpractice), and Depiction (a recorded video of a real human actor or of his 3D computer-animated double). Character, Outcome, and Depiction were designed to manipulate the doctor’s level of warmth, competence, and realism, respectively.Warmth and competence increased adherence intention and consultation enjoyment, but realism did not. On the contrary, the computer-animated doctor increased adherence intention and consultation enjoyment significantly more than the doctor portrayed by a human actor. We propose that enjoyment of the animated consultation caused the doctor to appear warmer and more real, compensating for his realism inconsistency. Expressed as a path model, this explanation fit the data.The acceptance and effectiveness of the animation should encourage the development of virtual consultations, which have advantages over creating content with human actors including ease of scenario revision, internationalization, localization, personalization, and web distribution.",Human-Computer Interaction; Agents and Multi-Agent Systems; Graphics; Multimedia; Social Computing,Adherence; Anthropomorphism; Avatars; Computer animation; Doctor–patient simulations; Health literacy; Interactive narratives; Uncanny valley,PeerJ Computer Science,4,108,3,6,2018
Probabilistic biomechanical finite element simulations: whole-model classical hypothesis testing based on upcrossing geometry,"Todd C., Pataky; Michihiko, Koseki; Phillip G., Cox",10.7717/peerj-cs.96,"Statistical analyses of biomechanical finite element (FE) simulations are frequently conducted on scalar metrics extracted from anatomically homologous regions, like maximum von Mises stresses from demarcated bone areas. The advantages of this approach are numerical tabulability and statistical simplicity, but disadvantages include region demarcation subjectivity, spatial resolution reduction, and results interpretation complexity when attempting to mentally map tabulated results to original anatomy. This study proposes a method which abandons the two aforementioned advantages to overcome these three limitations. The method is inspired by parametric random field theory (RFT), but instead uses a non-parametric analogue to RFT which permits flexible model-wide statistical analyses through non-parametrically constructed probability densities regarding volumetric upcrossing geometry. We illustrate method fundamentals using basic 1D and 2D models, then use a public model of hip cartilage compression to highlight how the concepts can extend to practical biomechanical modeling. The ultimate whole-volume results are easy to interpret, and for constant model geometry the method is simple to implement. Moreover, our analyses demonstrate that the method can yield biomechanical insights which are difficult to infer from single simulations or tabulated multi-simulation results. Generalizability to non-constant geometry including subject-specific anatomy is discussed.",Scientific Computing and Simulation,Computational statistics; Finite element analysis; Biomechanics; Probabilistic simulation; Random field theory,PeerJ Computer Science,2,49,14,2,2016
Investigating the effect of the reality gap on the human psychophysiological state in the context of human-swarm interaction,"Gaëtan, Podevijn; Rehan, O’Grady; Carole, Fantini-Hauwel; Marco, Dorigo",10.7717/peerj-cs.82,"The reality gap is the discrepancy between simulation and reality—the same behavioural algorithm results in different robot swarm behaviours in simulation and in reality (with real robots). In this paper, we study the effect of the reality gap on the psychophysiological reactions of humans interacting with a robot swarm. We compare the psychophysiological reactions of 28 participants interacting with a simulated robot swarm and with a real (non-simulated) robot swarm. Our results show that a real robot swarm provokes stronger reactions in our participants than a simulated robot swarm. We also investigate how to mitigate the effect of the reality gap (i.e., how to diminish the difference in the psychophysiological reactions between reality and simulation) by comparing psychophysiological reactions in simulation displayed on a computer screen and psychophysiological reactions in simulation displayed in virtual reality. Our results show that our participants tend to have stronger psychophysiological reactions in simulation displayed in virtual reality (suggesting a potential way of diminishing the effect of the reality gap).",Human-Computer Interaction; Adaptive and Self-Organizing Systems; Agents and Multi-Agent Systems; Artificial Intelligence; Robotics,Swarm robotics; Human-swarm interaction; Psychophysiology; Reality gap,PeerJ Computer Science,2,37,6,1,2016
HBPF: a Home Blood Pressure Framework with SLA guarantees to follow up hypertensive patients,"Josep, Cuadrado; Jordi, Vilaplana; Jordi, Mateo; Francesc, Solsona; Sara, Solsona; Josep, Rius; Rui, Alves; Miguel, Camafort; Gerard, Torres; Angels, Betriu; Josep M., Gutierrez; Elvira, Fernández",10.7717/peerj-cs.69,"Hypertension or high blood pressure is a condition on the rise. Not only does it affect the elderly but is also increasingly spreading to younger sectors of the population. Treating it involves exhaustive monitoring of patients. A tool adapted to the particular requirements of hypertension can greatly facilitate monitoring and diagnosis. This paper presents HBPF, an efficient cloud-based Home Blood Pressure Framework. This allows hypertensive patients to communicate with their health-care centers, thus facilitating monitoring for both patients and clinicians. HBPF provides a complete, efficient, and cross-platform framework to follow up hypertensive patients with an SLA guarantee. Response time below one second for 80,000 requests and 28% increase in peak throughput going from one to three virtual machines were obtained. In addition, a mobile app (BP) for Android and iOS with a user-friendly interface is also provided to facilitate following up hypertensive patients. Among them, between 54% and 87% favorably evaluated the tool. BP can be downloaded for free from the website Hesoft Group repository (http://www.hesoftgroup.eu).",Computer Networks and Communications; Distributed and Parallel Computing,Hypertension; Healtcare; eHealth,PeerJ Computer Science,2,24,7,3,2016
Forensic analysis of video steganography tools,"Thomas, Sloan; Julio, Hernandez-Castro",10.7717/peerj-cs.7,"Steganography is the art and science of concealing information in such a way that only the sender and intended recipient of a message should be aware of its presence. Digital steganography has been used in the past on a variety of media including executable files, audio, text, games and, notably, images. Additionally, there is increasing research interest towards the use of video as a media for steganography, due to its pervasive nature and diverse embedding capabilities. In this work, we examine the embedding algorithms and other security characteristics of several video steganography tools. We show how all feature basic and severe security weaknesses. This is potentially a very serious threat to the security, privacy and anonymity of their users. It is important to highlight that most steganography users have perfectly legal and ethical reasons to employ it. Some common scenarios would include citizens in oppressive regimes whose freedom of speech is compromised, people trying to avoid massive surveillance or censorship, political activists, whistle blowers, journalists, etc. As a result of our findings, we strongly recommend ceasing any use of these tools, and to remove any contents that may have been hidden, and any carriers stored, exchanged and/or uploaded online. For many of these tools, carrier files will be trivial to detect, potentially compromising any hidden data and the parties involved in the communication. We finish this work by presenting our steganalytic results, that highlight a very poor current state of the art in practical video steganography tools. There is unfortunately a complete lack of secure and publicly available tools, and even commercial tools offer very poor security. We therefore encourage the steganography community to work towards the development of more secure and accessible video steganography tools, and make them available for the general public. The results presented in this work can also be seen as a useful resource for forensic examiners to determine the existence of any video steganography materials over the course of a computer forensic investigation.",Security and Privacy,Steganography; Steganalysis; Video; Signature; EOF injection,PeerJ Computer Science,1,24,9,4,2015
Probabilistic programming in Python using PyMC3,"John, Salvatier; Thomas V., Wiecki; Christopher, Fonnesbeck",10.7717/peerj-cs.55,"Probabilistic programming allows for automatic Bayesian inference on user-defined probabilistic models. Recent advances in Markov chain Monte Carlo (MCMC) sampling allow inference on increasingly complex models. This class of MCMC, known as Hamiltonian Monte Carlo, requires gradient information which is often not readily available. PyMC3 is a new open source probabilistic programming framework written in Python that uses Theano to compute gradients via automatic differentiation as well as compile probabilistic programs on-the-fly to C for increased speed. Contrary to other probabilistic programming languages, PyMC3 allows model specification directly in Python code. The lack of a domain specific language allows for great flexibility and direct interaction with the model. This paper is a tutorial-style introduction to this software package.",Data Mining and Machine Learning; Data Science; Scientific Computing and Simulation,Bayesian statistic; Probabilistic Programming; Python; Markov chain Monte Carlo; Statistical modeling,PeerJ Computer Science,2,20,7,0,2016
Research collaboration and topic trends in Computer Science based on top active authors,"Yan, Wu; Srinivasan, Venkatramanan; Dah Ming, Chiu",10.7717/peerj-cs.41,"Academic publication metadata can be used to analyze the collaboration, productivity and hot topic trends of a research community. In this paper, we study a specific group of authors, namely the top active authors. They are defined as the top 1% authors with uninterrupted and continuous presence in scientific publications over a time window. We take the top active authors in the Computer Science (CS) community over different time windows in the past 50 years, and use them to analyze collaboration, productivity and topic trends. We show that (a) the top active authors are representative of the overall population; (b) the community is increasingly moving in the direction of Team Research, with increased level and degree of collaboration; and (c) the research topics are increasingly inter-related. By focusing on the top active authors, it helps visualize these trends better. Besides, the observations from top active authors also shed light on design of better evaluation framework and resource management for policy makers in academia.",Data Mining and Machine Learning; Data Science; Digital Libraries; Network Science and Online Social Networks; Social Computing,Top active author; Research collaboration; Topic trends,PeerJ Computer Science,2,22,13,2,2016
A Socratic epistemology for verbal emotional intelligence,"Abe, Kazemzadeh; James, Gibson; Panayiotis, Georgiou; Sungbok, Lee; Shrikanth, Narayanan",10.7717/peerj-cs.40,"We describe and experimentally validate a question-asking framework for machine-learned linguistic knowledge about human emotions. Using the Socratic method as a theoretical inspiration, we develop an experimental method and computational model for computers to learn subjective information about emotions by playing emotion twenty questions (EMO20Q), a game of twenty questions limited to words denoting emotions. Using human–human EMO20Q data we bootstrap a sequential Bayesian model that drives a generalized pushdown automaton-based dialog agent that further learns from 300 human–computer dialogs collected on Amazon Mechanical Turk. The human–human EMO20Q dialogs show the capability of humans to use a large, rich, subjective vocabulary of emotion words. Training on successive batches of human–computer EMO20Q dialogs shows that the automated agent is able to learn from subsequent human–computer interactions. Our results show that the training procedure enables the agent to learn a large set of emotion words. The fully trained agent successfully completes EMO20Q at 67% of human performance and 30% better than the bootstrapped agent. Even when the agent fails to guess the human opponent’s emotion word in the EMO20Q game, the agent’s behavior of searching for knowledge makes it appear human-like, which enables the agent to maintain user engagement and learn new, out-of-vocabulary words. These results lead us to conclude that the question-asking methodology and its implementation as a sequential Bayes pushdown automaton are a successful model for the cognitive abilities involved in learning, retrieving, and using emotion words by an automated agent in a dialog setting.",Agents and Multi-Agent Systems; Artificial Intelligence; Computational Linguistics; Data Mining and Machine Learning; Natural Language and Speech,Natural language processing; Dialog systems; Artificial intelligence; Affective computing; Cognitive science; Dialog agents; Emotions; Question-asking; Sequential Bayesian; Games,PeerJ Computer Science,2,29,2,7,2016
Reconstructing the history of a WD40 beta-propeller tandem repeat using a phylogenetically informed algorithm,"Philippe, Lavoie-Mongrain; Mahdi, Belcaid; Aïda, Ouangraoua; Anne, Bergeron; Guylaine, Poisson",10.7717/peerj-cs.6,"Tandem repeat sequences have been found in great numbers in proteins that are conserved in a wide range of living species. In order to reconstruct the evolutionary history of such sequences, it is necessary to develop algorithms and methods that can work with highly divergent motifs. Here we propose a reconstruction algorithm that uses, in parallel, ortholog tandem repeat sequences from n species whose phylogeny is known, allowing it to distinguish mutations that occurred before and after the first speciation. At each step of the reconstruction, both the boundaries and the length of the duplicated segment are recalculated, making the approach suitable for sequences for which the fixed boundary hypothesis may not hold. We use this algorithm to reconstruct a 4-bladed ancestor of the 7-bladed WD40 beta-propeller, using orthologs of the GNB1 human protein in plants, yeasts, nematodes, insects and fishes. The results obtained for the WD40 repeats are very encouraging, as the noise in the duplication reconstruction is significantly reduced.",Bioinformatics; Computational Biology,Ancestor reconstruction; Tandem repeats; Phylogeny; Beta propellers,PeerJ Computer Science,1,28,9,3,2015
"DAE Tools: equation-based object-oriented modelling, simulation and optimisation software","Dragan D., Nikolić",10.7717/peerj-cs.54,"In this work, DAE Tools modelling, simulation and optimisation software, its programming paradigms and main features are presented. The current approaches to mathematical modelling such as the use of modelling languages and general-purpose programming languages are analysed. The common set of capabilities required by the typical simulation software are discussed, and the shortcomings of the current approaches recognised. A new hybrid approach is introduced, and the modelling languages and the hybrid approach are compared in terms of the grammar, compiler, parser and interpreter requirements, maintainability and portability. The most important characteristics of the new approach are discussed, such as: (1) support for the runtime model generation; (2) support for the runtime simulation set-up; (3) support for complex runtime operating procedures; (4) interoperability with the third party software packages (i.e. NumPy/SciPy); (5) suitability for embedding and use as a web application or software as a service; and (6) code-generation, model exchange and co-simulation capabilities. The benefits of an equation-based approach to modelling, implemented in a fourth generation object-oriented general purpose programming language such as Python are discussed. The architecture and the software implementation details as well as the type of problems that can be solved using DAE Tools software are described. Finally, some applications of the software at different levels of abstraction are presented, and its embedding capabilities and suitability for use as a software as a service is demonstrated.",Scientific Computing and Simulation,Modelling; Simulation; Optimisation; Modelling languages; Model exchange; Domain specific languages; Equation-based; DAE; Code generation,PeerJ Computer Science,2,26,12,1,2016
Clawpack: building an open source ecosystem for solving hyperbolic PDEs,"Kyle T., Mandli; Aron J., Ahmadia; Marsha, Berger; Donna, Calhoun; David L., George; Yiannis, Hadjimichael; David I., Ketcheson; Grady I., Lemoine; Randall J., LeVeque",10.7717/peerj-cs.68,"Clawpack is a software package designed to solve nonlinear hyperbolic partial differential equations using high-resolution finite volume methods based on Riemann solvers and limiters. The package includes a number of variants aimed at different applications and user communities. Clawpack has been actively developed as an open source project for over 20 years. The latest major release, Clawpack 5, introduces a number of new features and changes to the code base and a new development model based on GitHub and Git submodules. This article provides a summary of the most significant changes, the rationale behind some of these changes, and a description of our current development model.",Distributed and Parallel Computing; Scientific Computing and Simulation,Partial differential equations; Finite volume methods; Parallel computing; Open source software; Conservation laws; Balance laws,PeerJ Computer Science,2,52,8,1,2016
Electronic laboratory notebooks in a public–private partnership,"Lea A.I., Vaas; Gesa, Witt; Björn, Windshügel; Andrea, Bosin; Giovanni, Serra; Adrian, Bruengger; Mathias, Winterhalter; Philip, Gribbon; Cindy J., Levy-Petelinkar; Manfred, Kohler",10.7717/peerj-cs.83,"This report shares the experience during selection, implementation and maintenance phases of an electronic laboratory notebook (ELN) in a public–private partnership project and comments on user’s feedback. In particular, we address which time constraints for roll-out of an ELN exist in granted projects and which benefits and/or restrictions come with out-of-the-box solutions. We discuss several options for the implementation of support functions and potential advantages of open access solutions. Connected to that, we identified willingness and a vivid culture of data sharing as the major item leading to success or failure of collaborative research activities. The feedback from users turned out to be the only angle for driving technical improvements, but also exhibited high efficiency. Based on these experiences, we describe best practices for future projects on implementation and support of an ELN supporting a diverse, multidisciplinary user group based in academia, NGOs, and/or for-profit corporations located in multiple time zones.",Computer Education; Mobile and Ubiquitous Computing; Network Science and Online Social Networks; Social Computing,public–private partnership; Open access; Innovative Medicines Initiative; Electronic laboratory notebook; New Drugs for Bad Bugs; IMI; PPP; ND4BB; Collaboration; Sharing information,PeerJ Computer Science,2,32,4,8,2016
Ship space to database: emerging infrastructures for studies of the deep subseafloor biosphere,"Peter T., Darch; Christine L., Borgman",10.7717/peerj-cs.97,"An increasing array of scientific fields face a “data deluge.” However, in many fields data are scarce, with implications for their epistemic status and ability to command funding. Consequently, they often attempt to develop infrastructure for data production, management, curation, and circulation. A component of a knowledge infrastructure may serve one or more scientific domains. Further, a single domain may rely upon multiple infrastructures simultaneously. Studying how domains negotiate building and accessing scarce infrastructural resources that they share with other domains will shed light on how knowledge infrastructures shape science.We conducted an eighteen-month, qualitative study of scientists studying the deep subseafloor biosphere, focusing on the Center for Dark Energy Biosphere Investigations (C-DEBI) and the Integrated Ocean Drilling Program (IODP) and its successor, the International Ocean Discovery Program (IODP2). Our methods comprised ethnographic observation, including eight months embedded in a laboratory, interviews (n = 49), and document analysis.Deep subseafloor biosphere research is an emergent domain. We identified two reasons for the domain’s concern with data scarcity: limited ability to pursue their research objectives, and the epistemic status of their research. Domain researchers adopted complementary strategies to acquire more data. One was to establish C-DEBI as an infrastructure solely for their domain. The second was to use C-DEBI as a means to gain greater access to, and reconfigure, IODP/IODP2 to their advantage. IODP/IODP2 functions as infrastructure for multiple scientific domains, which creates competition for resources. C-DEBI is building its own data management infrastructure, both to acquire more data from IODP and to make better use of data, once acquired.Two themes emerge. One is data scarcity, which can be understood only in relation to a domain’s objectives. To justify support for public funding, domains must demonstrate their utility to questions of societal concern or existential questions about humanity. The deep subseafloor biosphere domain aspires to address these questions in a more statistically intensive manner than is afforded by the data to which it currently has access. The second theme is the politics of knowledge infrastructures. A single scientific domain may build infrastructure for itself and negotiate access to multi-domain infrastructure simultaneously. C-DEBI infrastructure was designed both as a response to scarce IODP/IODP2 resources, and to configure the data allocation processes of IODP/IODP2 in their favor.",Human-Computer Interaction; Digital Libraries,Knowledge infrastructures; Scientific data; Microbiology; Long tail; Big science; Little science; Big data; Little data; Cyberinfrastructure; Data infrastructure,PeerJ Computer Science,2,84,0,1,2016
Cost-efficient enactment of stream processing topologies,"Christoph, Hochreiner; Michael, Vögler; Stefan, Schulte; Schahram, Dustdar",10.7717/peerj-cs.141,"The continuous increase of unbound streaming data poses several challenges to established data stream processing engines. One of the most important challenges is the cost-efficient enactment of stream processing topologies under changing data volume. These data volume pose different loads to stream processing systems whose resource provisioning needs to be continuously updated at runtime. First approaches already allow for resource provisioning on the level of virtual machines (VMs), but this only allows for coarse resource provisioning strategies. Based on current advances and benefits for containerized software systems, we have designed a cost-efficient resource provisioning approach and integrated it into the runtime of the Vienna ecosystem for elastic stream processing. Our resource provisioning approach aims to maximize the resource usage for VMs obtained from cloud providers. This strategy only releases processing capabilities at the end of the VMs minimal leasing duration instead of releasing them eagerly as soon as possible as it is the case for threshold-based approaches. This strategy allows us to improve the service level agreement compliance by up to 25% and a reduction for the operational cost of up to 36%.",Adaptive and Self-Organizing Systems; Distributed and Parallel Computing,Data stream processing; Cloud computing; Resource elasticity; Resource optimization,PeerJ Computer Science,3,39,11,9,2017
ClusterEnG: an interactive educational web resource for clustering and visualizing high-dimensional data,"Mohith, Manjunath; Yi, Zhang; Yeonsung, Kim; Steve H., Yeo; Omar, Sobh; Nathan, Russell; Christian, Followell; Colleen, Bushell; Umberto, Ravaioli; Jun S., Song",10.7717/peerj-cs.155,"Clustering is one of the most common techniques in data analysis and seeks to group together data points that are similar in some measure. Although there are many computer programs available for performing clustering, a single web resource that provides several state-of-the-art clustering methods, interactive visualizations and evaluation of clustering results is lacking.ClusterEnG (acronym for Clustering Engine for Genomics) provides a web interface for clustering data and interactive visualizations including 3D views, data selection and zoom features. Eighteen clustering validation measures are also presented to aid the user in selecting a suitable algorithm for their dataset. ClusterEnG also aims at educating the user about the similarities and differences between various clustering algorithms and provides tutorials that demonstrate potential pitfalls of each algorithm.The web resource will be particularly useful to scientists who are not conversant with computing but want to understand the structure of their data in an intuitive manner. The validation measures facilitate the process of choosing a suitable clustering algorithm among the available options. ClusterEnG is part of a bigger project called KnowEnG (Knowledge Engine for Genomics) and is available at http://education.knoweng.org/clustereng.",Bioinformatics; Computational Biology,Validation measures; Genomics; Web interface; Education; Clustering,PeerJ Computer Science,4,25,5,0,2018
Predicting judicial decisions of the European Court of Human Rights: a Natural Language Processing perspective,"Nikolaos, Aletras; Dimitrios, Tsarapatsanis; Daniel, Preoţiuc-Pietro; Vasileios, Lampos",10.7717/peerj-cs.93,"Recent advances in Natural Language Processing and Machine Learning provide us with the tools to build predictive models that can be used to unveil patterns driving judicial decisions. This can be useful, for both lawyers and judges, as an assisting tool to rapidly identify cases and extract patterns which lead to certain decisions. This paper presents the first systematic study on predicting the outcome of cases tried by the European Court of Human Rights based solely on textual content. We formulate a binary classification task where the input of our classifiers is the textual content extracted from a case and the target output is the actual judgment as to whether there has been a violation of an article of the convention of human rights. Textual information is represented using contiguous word sequences, i.e., N-grams, and topics. Our models can predict the court’s decisions with a strong accuracy (79% on average). Our empirical analysis indicates that the formal facts of a case are the most important predictive factor. This is consistent with the theory of legal realism suggesting that judicial decision-making is significantly affected by the stimulus of the facts. We also observe that the topical content of a case is another important feature in this classification task and explore this relationship further by conducting a qualitative analysis.",Artificial Intelligence; Computational Linguistics; Data Mining and Machine Learning; Data Science; Natural Language and Speech,Natural Language Processing; Text Mining; Legal Science; Machine Learning; Artificial Intelligence; Judicial decisions,PeerJ Computer Science,2,43,4,5,2016
OSoMe: the IUNI observatory on social media,"Clayton A., Davis; Giovanni Luca, Ciampaglia; Luca Maria, Aiello; Keychul, Chung; Michael D., Conover; Emilio, Ferrara; Alessandro, Flammini; Geoffrey C., Fox; Xiaoming, Gao; Bruno, Gonçalves; Przemyslaw A., Grabowicz; Kibeom, Hong; Pik-Mai, Hui; Scott, McCaulay; Karissa, McKelvey; Mark R., Meiss; Snehal, Patil; Chathuri, Peli Kankanamalage; Valentin, Pentchev; Judy, Qiu; Jacob, Ratkiewicz; Alex, Rudnick; Benjamin, Serrette; Prashant, Shiralkar; Onur, Varol; Lilian, Weng; Tak-Lon, Wu; Andrew J., Younge; Filippo, Menczer",10.7717/peerj-cs.87,"The study of social phenomena is becoming increasingly reliant on big data from online social networks. Broad access to social media data, however, requires software development skills that not all researchers possess. Here we present the IUNI Observatory on Social Media, an open analytics platform designed to facilitate computational social science. The system leverages a historical, ongoing collection of over 70 billion public messages from Twitter. We illustrate a number of interactive open-source tools to retrieve, visualize, and analyze derived data from this collection. The Observatory, now available at osome.iuni.iu.edu, is the result of a large, six-year collaborative effort coordinated by the Indiana University Network Science Institute.",Data Science; Network Science and Online Social Networks; Social Computing; World Wide Web and Web Science,Social media; Observatory; Twitter; Web science; Network science; Meme diffusion; Computational social science; Big data; API; OSoMe,PeerJ Computer Science,2,79,10,0,2016
Computing the sparse matrix vector product using block-based kernels without zero padding on processors with AVX-512 instructions,"Bérenger, Bramas; Pavel, Kus",10.7717/peerj-cs.151,"The sparse matrix-vector product (SpMV) is a fundamental operation in many scientific applications from various fields. The High Performance Computing (HPC) community has therefore continuously invested a lot of effort to provide an efficient SpMV kernel on modern CPU architectures. Although it has been shown that block-based kernels help to achieve high performance, they are difficult to use in practice because of the zero padding they require. In the current paper, we propose new kernels using the AVX-512 instruction set, which makes it possible to use a blocking scheme without any zero padding in the matrix memory storage. We describe mask-based sparse matrix formats and their corresponding SpMV kernels highly optimized in assembly language. Considering that the optimal blocking size depends on the matrix, we also provide a method to predict the best kernel to be used utilizing a simple interpolation of results from previous executions. We compare the performance of our approach to that of the Intel MKL CSR kernel and the CSR5 open-source package on a set of standard benchmark matrices. We show that we can achieve significant improvements in many cases, both for sequential and for parallel executions. Finally, we provide the corresponding code in an open source library, called SPC5.",Distributed and Parallel Computing; Scientific Computing and Simulation,SpMV; Code Optimization; SIMD; Vectorization; HPC,PeerJ Computer Science,4,25,6,3,2018
MCLEAN: Multilevel Clustering Exploration As Network,"Daniel, Alcaide; Jan, Aerts",10.7717/peerj-cs.145,"Finding useful patterns in datasets has attracted considerable interest in the field of visual analytics. One of the most common tasks is the identification and representation of clusters. However, this is non-trivial in heterogeneous datasets since the data needs to be analyzed from different perspectives. Indeed, highly variable patterns may mask underlying trends in the dataset. Dendrograms are graphical representations resulting from agglomerative hierarchical clustering and provide a framework for viewing the clustering at different levels of detail. However, dendrograms become cluttered when the dataset gets large, and the single cut of the dendrogram to demarcate different clusters can be insufficient in heterogeneous datasets. In this work, we propose a visual analytics methodology called MCLEAN that offers a general approach for guiding the user through the exploration and detection of clusters. Powered by a graph-based transformation of the relational data, it supports a scalable environment for representation of heterogeneous datasets by changing the spatialization. We thereby combine multilevel representations of the clustered dataset with community finding algorithms. Our approach entails displaying the results of the heuristics to users, providing a setting from which to start the exploration and data analysis. To evaluate our proposed approach, we conduct a qualitative user study, where participants are asked to explore a heterogeneous dataset, comparing the results obtained by MCLEAN with the dendrogram. These qualitative results reveal that MCLEAN is an effective way of aiding users in the detection of clusters in heterogeneous datasets. The proposed methodology is implemented in an R package available at https://bitbucket.org/vda-lab/mclean.",Data Science; Visual Analytics,Exploratory data analysis; Graph and network visualization; Hierarchical clustering; Visual analytics,PeerJ Computer Science,4,30,10,0,2018
Detecting periodicities with Gaussian processes,"Nicolas, Durrande; James, Hensman; Magnus, Rattray; Neil D., Lawrence",10.7717/peerj-cs.50,"We consider the problem of detecting and quantifying the periodic component of a function given noise-corrupted observations of a limited number of input/output tuples. Our approach is based on Gaussian process regression, which provides a flexible non-parametric framework for modelling periodic data. We introduce a novel decomposition of the covariance function as the sum of periodic and aperiodic kernels. This decomposition allows for the creation of sub-models which capture the periodic nature of the signal and its complement. To quantify the periodicity of the signal, we derive a periodicity ratio which reflects the uncertainty in the fitted sub-models. Although the method can be applied to many kernels, we give a special emphasis to the Matérn family, from the expression of the reproducing kernel Hilbert space inner product to the implementation of the associated periodic kernels in a Gaussian process toolkit. The proposed method is illustrated by considering the detection of periodically expressed genes in the arabidopsis genome.",Data Mining and Machine Learning; Optimization Theory and Computation,RKHS; Harmonic analysis; Circadian rhythm; Gene expression; Matérn kernels,PeerJ Computer Science,2,30,6,1,2016
More ties than we thought,"Dan, Hirsch; Ingemar, Markström; Meredith L., Patterson; Anders, Sandberg; Mikael, Vejdemo-Johansson",10.7717/peerj-cs.2,"We extend the existing enumeration of neck tie-knots to include tie-knots with a textured front, tied with the narrow end of a tie. These tie-knots have gained popularity in recent years, based on reconstructions of a costume detail from The Matrix Reloaded, and are explicitly ruled out in the enumeration by Fink & Mao (2000). We show that the relaxed tie-knot description language that comprehensively describes these extended tie-knot classes is context free. It has a regular sub-language that covers all the knots that originally inspired the work. From the full language, we enumerate 266,682 distinct tie-knots that seem tie-able with a normal neck-tie. Out of these 266,682, we also enumerate 24,882 tie-knots that belong to the regular sub-language.",Algorithms and Analysis of Algorithms; Computational Linguistics; Theory and Formal Methods,Necktie knots; Formal language; Automata; Chomsky hierarchy; Generating functions,PeerJ Computer Science,1,13,3,4,2015
Nanopublication beyond the sciences: the PeriodO period gazetteer,"Patrick, Golden; Ryan, Shaw",10.7717/peerj-cs.44,"The information expressed in humanities datasets is inextricably tied to a wider discursive environment that is irreducible to complete formal representation. Humanities scholars must wrestle with this fact when they attempt to publish or consume structured data. The practice of “nanopublication,” which originated in the e-science domain, offers a way to maintain the connection between formal representations of humanities data and its discursive basis. In this paper we describe nanopublication, its potential applicability to the humanities, and our experience curating humanities nanopublications in the PeriodO period gazetteer.",Digital Libraries; World Wide Web and Web Science,Nanopublication; Periodization; Scholarly communication; Time; Linked data; JSON-LD,PeerJ Computer Science,2,26,4,1,2016
Decentralized provenance-aware publishing with nanopublications,"Tobias, Kuhn; Christine, Chichester; Michael, Krauthammer; Núria, Queralt-Rosinach; Ruben, Verborgh; George, Giannakopoulos; Axel-Cyrille, Ngonga Ngomo; Raffaele, Viglianti; Michel, Dumontier",10.7717/peerj-cs.78,"Publication and archival of scientific results is still commonly considered the responsability of classical publishing companies. Classical forms of publishing, however, which center around printed narrative articles, no longer seem well-suited in the digital age. In particular, there exist currently no efficient, reliable, and agreed-upon methods for publishing scientific datasets, which have become increasingly important for science. In this article, we propose to design scientific data publishing as a web-based bottom-up process, without top-down control of central authorities such as publishing companies. Based on a novel combination of existing concepts and technologies, we present a server network to decentrally store and archive data in the form of nanopublications, an RDF-based format to represent scientific data. We show how this approach allows researchers to publish, retrieve, verify, and recombine datasets of nanopublications in a reliable and trustworthy manner, and we argue that this architecture could be used as a low-level data publication layer to serve the Semantic Web in general. Our evaluation of the current network shows that this system is efficient and reliable.",Bioinformatics; Computer Networks and Communications; Digital Libraries; World Wide Web and Web Science,Data publishing; Nanopublications; Provenance; Linked Data; Semantic Web,PeerJ Computer Science,2,50,9,1,2016
Managing contamination delay to improve Timing Speculation architectures,"Naga Durga Prasad, Avirneni; Prem Kumar, Ramesh; Arun K., Somani",10.7717/peerj-cs.79,"Timing Speculation (TS) is a widely known method for realizing better-than-worst-case systems. Aggressive clocking, realizable by TS, enable systems to operate beyond specified safe frequency limits to effectively exploit the data dependent circuit delay. However, the range of aggressive clocking for performance enhancement under TS is restricted by short paths. In this paper, we show that increasing the lengths of short paths of the circuit increases the effectiveness of TS, leading to performance improvement. Also, we propose an algorithm to efficiently add delay buffers to selected short paths while keeping down the area penalty. We present our algorithm results for ISCAS-85 suite and show that it is possible to increase the circuit contamination delay by up to 30% without affecting the propagation delay. We also explore the possibility of increasing short path delays further by relaxing the constraint on propagation delay and analyze the performance impact.",Algorithms and Analysis of Algorithms; Computer Architecture; Embedded Computing; Emerging Technologies,Timing speculation; Timing errors; PVT variation; Overclocking; Delay insertion; Timing constraints; Reliable and aggressive systems; Contamination delay,PeerJ Computer Science,2,36,9,5,2016
PATACSDB—the database of polyA translational attenuators in coding sequences,"Malgorzata, Habich; Sergej, Djuranovic; Pawel, Szczesny",10.7717/peerj-cs.45,"Recent additions to the repertoire of gene expression regulatory mechanisms are polyadenylate (polyA) tracks encoding for poly-lysine runs in protein sequences. Such tracks stall the translation apparatus and induce frameshifting independently of the effects of charged nascent poly-lysine sequence on the ribosome exit channel. As such, they substantially influence the stability of mRNA and the amount of protein produced from a given transcript. Single base changes in these regions are enough to exert a measurable response on both protein and mRNA abundance; this makes each of these sequences a potentially interesting case study for the effects of synonymous mutation, gene dosage balance and natural frameshifting. Here we present PATACSDB, a resource that contain a comprehensive list of polyA tracks from over 250 eukaryotic genomes. Our data is based on the Ensembl genomic database of coding sequences and filtered with algorithm of 12A-1 which selects sequences of polyA tracks with a minimal length of 12 A’s allowing for one mismatched base. The PATACSDB database is accessible at: http://sysbio.ibb.waw.pl/patacsdb. The source code is available at http://github.com/habich/PATACSDB, and it includes the scripts with which the database can be recreated.",Bioinformatics; Databases,Ribosome stalling; Gene regulation; Eukaryotic genomes; mRNA stability; Translation,PeerJ Computer Science,2,14,1,1,2016
Sonification of reference markers for auditory graphs: effects on non-visual point estimation tasks,"Oussama, Metatla; Nick, Bryan-Kinns; Tony, Stockman; Fiore, Martin",10.7717/peerj-cs.51,"Research has suggested that adding contextual information such as reference markers to data sonification can improve interaction with auditory graphs. This paper presents results of an experiment that contributes to quantifying and analysing the extent of such benefits for an integral part of interacting with graphed data: point estimation tasks. We examine three pitch-based sonification mappings; pitch-only, one-reference, and multiple-references that we designed to provide information about distance from an origin. We assess the effects of these sonifications on users’ performances when completing point estimation tasks in a between-subject experimental design against visual and speech control conditions. Results showed that the addition of reference tones increases users accuracy with a trade-off for task completion times, and that the multiple-references mapping is particularly effective when dealing with points that are positioned at the midrange of a given axis.",Human-Computer Interaction,Sonification; Point estimation; Auditory graphs; Non-visual interaction; Reference markers,PeerJ Computer Science,2,31,5,2,2016
Zbrowse: an interactive GWAS results browser,"Greg R., Ziegler; Ryan H., Hartsock; Ivan, Baxter",10.7717/peerj-cs.3,"The growing number of genotyped populations, the advent of high-throughput phenotyping techniques and the development of GWAS analysis software has rapidly accelerated the number of GWAS experimental results. Candidate gene discovery from these results files is often tedious, involving many manual steps searching for genes in windows around a significant SNP. This problem rapidly becomes more complex when an analyst wishes to compare multiple GWAS studies for pleiotropic or environment specific effects. To this end, we have developed a fast and intuitive interactive browser for the viewing of GWAS results with a focus on an ability to compare results across multiple traits or experiments. The software can easily be run on a desktop computer with software that bioinformaticians are likely already familiar with. Additionally, the software can be hosted or embedded on a server for easy access by anyone with a modern web browser.",Bioinformatics; Computational Biology; Visual Analytics,GWAS; Association mapping; Data visualization,PeerJ Computer Science,1,20,4,1,2015
"The Modern Research Data Portal: a design pattern for networked, data-intensive science","Kyle, Chard; Eli, Dart; Ian, Foster; David, Shifflett; Steven, Tuecke; Jason, Williams",10.7717/peerj-cs.144,"We describe best practices for providing convenient, high-speed, secure access to large data via research data portals. We capture these best practices in a new design pattern, the Modern Research Data Portal, that disaggregates the traditional monolithic web-based data portal to achieve orders-of-magnitude increases in data transfer performance, support new deployment architectures that decouple control logic from data storage, and reduce development and operations costs. We introduce the design pattern; explain how it leverages high-performance data enclaves and cloud-based data management services; review representative examples at research laboratories and universities, including both experimental facilities and supercomputer sites; describe how to leverage Python APIs for authentication, authorization, data transfer, and data sharing; and use coding examples to demonstrate how these APIs can be used to implement a range of research data portal capabilities. Sample code at a companion web site, https://docs.globus.org/mrdp, provides application skeletons that readers can adapt to realize their own research data portals.",Computer Networks and Communications; Data Science; Distributed and Parallel Computing; Security and Privacy; World Wide Web and Web Science,Portal; High-speed network; Globus; Science DMZ; Data transfer node,PeerJ Computer Science,4,45,8,1,2018
Interval Coded Scoring: a toolbox for interpretable scoring systems,"Lieven, Billiet; Sabine, Van Huffel; Vanya, Van Belle",10.7717/peerj-cs.150,"Over the last decades, clinical decision support systems have been gaining importance. They help clinicians to make effective use of the overload of available information to obtain correct diagnoses and appropriate treatments. However, their power often comes at the cost of a black box model which cannot be interpreted easily. This interpretability is of paramount importance in a medical setting with regard to trust and (legal) responsibility. In contrast, existing medical scoring systems are easy to understand and use, but they are often a simplified rule-of-thumb summary of previous medical experience rather than a well-founded system based on available data. Interval Coded Scoring (ICS) connects these two approaches, exploiting the power of sparse optimization to derive scoring systems from training data. The presented toolbox interface makes this theory easily applicable to both small and large datasets. It contains two possible problem formulations based on linear programming or elastic net. Both allow to construct a model for a binary classification problem and establish risk profiles that can be used for future diagnosis. All of this requires only a few lines of code. ICS differs from standard machine learning through its model consisting of interpretable main effects and interactions. Furthermore, insertion of expert knowledge is possible because the training can be semi-automatic. This allows end users to make a trade-off between complexity and performance based on cross-validation results and expert knowledge. Additionally, the toolbox offers an accessible way to assess classification performance via accuracy and the ROC curve, whereas the calibration of the risk profile can be evaluated via a calibration curve. Finally, the colour-coded model visualization has particular appeal if one wants to apply ICS manually on new observations, as well as for validation by experts in the specific application domains. The validity and applicability of the toolbox is demonstrated by comparing it to standard Machine Learning approaches such as Naive Bayes and Support Vector Machines for several real-life datasets. These case studies on medical problems show its applicability as a decision support system. ICS performs similarly in terms of classification and calibration. Its slightly lower performance is countered by its model simplicity which makes it the method of choice if interpretability is a key issue.",Data Mining and Machine Learning; Data Science; Optimization Theory and Computation,Decision support; Interpretability; Scoring systems; Sparse Optimization; Classification; Risk assessment; Toolbox,PeerJ Computer Science,4,51,8,5,2018
Software citation principles,"Arfon M., Smith; Daniel S., Katz; Kyle E., Niemeyer",10.7717/peerj-cs.86,"Software is a critical part of modern research and yet there is little support across the scholarly ecosystem for its acknowledgement and citation. Inspired by the activities of the FORCE11 working group focused on data citation, this document summarizes the recommendations of the FORCE11 Software Citation Working Group and its activities between June 2015 and April 2016. Based on a review of existing community practices, the goal of the working group was to produce a consolidated set of citation principles that may encourage broad adoption of a consistent policy for software citation across disciplines and venues. Our work is presented here as a set of software citation principles, a discussion of the motivations for developing the principles, reviews of existing community practice, and a discussion of the requirements these principles would place upon different stakeholders. Working examples and possible technical solutions for how these principles can be implemented will be discussed in a separate paper.",Digital Libraries; Software Engineering,Software citation; Software credit; Attribution,PeerJ Computer Science,2,47,0,2,2016
Competition and cooperation with virtual players in an exergame,"Lindsay A., Shaw; Jude, Buckley; Paul M., Corballis; Christof, Lutteroth; Burkhard C., Wuensche",10.7717/peerj-cs.92,"Two cross-sectional studies investigated the effects of competition and cooperation with virtual players on exercise performance in an immersive virtual reality (VR) cycle exergame. Study 1 examined the effects of: (1) self-competition whereby participants played the exergame while competing against a replay of their previous exergame session (Ghost condition), and (2) playing the exergame with a virtual trainer present (Trainer condition) on distance travelled and calories expended while cycling. Study 2 examined the effects of (1) competition with a virtual trainer system (Competitive condition) and (2) cooperation with a virtual trainer system (Cooperative condition). Post exergame enjoyment and motivation were also assessed.The results of Study 1 showed that the trainer system elicited a lesser distance travelled than when playing with a ghost or on one’s own. These results also showed that competing against a ghost was more enjoyable than playing on one’s own or with the virtual trainer. There was no significant difference between the participants’ rated enjoyment and motivation and their distance travelled or calories burned. The findings of Study 2 showed that the competitive trainer elicited a greater distance travelled and caloric expenditure, and was rated as more motivating. As in Study 1, enjoyment and motivation were not correlated with distance travelled and calories burned.Taken together, these results demonstrate that a competitive experience in exergaming is an effective tool to elicit higher levels of exercise from the user, and can be achieved through virtual substitutes for another human player.",Human-Computer Interaction; Emerging Technologies,Exergame; Virtual reality; Competition; Cooperation; Motivation,PeerJ Computer Science,2,28,4,2,2016
Collaboro: a collaborative (meta) modeling tool,"Javier Luis, Cánovas Izquierdo; Jordi, Cabot",10.7717/peerj-cs.84,"Software development is becoming more and more collaborative, emphasizing the role of end-users in the development process to make sure the final product will satisfy customer needs. This is especially relevant when developing Domain-Specific Modeling Languages (DSMLs), which are modeling languages specifically designed to carry out the tasks of a particular domain. While end-users are actually the experts of the domain for which a DSML is developed, their participation in the DSML specification process is still rather limited nowadays. In this paper, we propose a more community-aware language development process by enabling the active participation of all community members (both developers and end-users) from the very beginning. Our proposal, called Collaboro, is based on a DSML itself enabling the representation of change proposals during the language design and the discussion (and trace back) of possible solutions, comments and decisions arisen during the collaboration. Collaboro also incorporates a metric-based recommender system to help community members to define high-quality notations for the DSMLs. We also show how Collaboro can be used at the model-level to facilitate the collaborative specification of software models. Tool support is available both as an Eclipse plug-in a web-based solution.",Programming Languages; Software Engineering,Collaborative development; Domain-specific languages; Model-driven development,PeerJ Computer Science,2,63,14,1,2016
Machine learning can differentiate venom toxins from other proteins having non-toxic physiological functions,"Ranko, Gacesa; David J., Barlow; Paul F., Long",10.7717/peerj-cs.90,"Ascribing function to sequence in the absence of biological data is an ongoing challenge in bioinformatics. Differentiating the toxins of venomous animals from homologues having other physiological functions is particularly problematic as there are no universally accepted methods by which to attribute toxin function using sequence data alone. Bioinformatics tools that do exist are difficult to implement for researchers with little bioinformatics training. Here we announce a machine learning tool called ‘ToxClassifier’ that enables simple and consistent discrimination of toxins from non-toxin sequences with >99% accuracy and compare it to commonly used toxin annotation methods. ‘ToxClassifer’ also reports the best-hit annotation allowing placement of a toxin into the most appropriate toxin protein family, or relates it to a non-toxic protein having the closest homology, giving enhanced curation of existing biological databases and new venomics projects. ‘ToxClassifier’ is available for free, either to download (https://github.com/rgacesa/ToxClassifier) or to use on a web-based server (http://bioserv7.bioinfo.pbf.hr/ToxClassifier/).",Bioinformatics; Computational Biology; Data Mining and Machine Learning,Protein sequences; Biological function; Animal venom; Automatic annotation; Functional prediction,PeerJ Computer Science,2,36,2,4,2016
Investigating the correspondence between driver head position and glance location,"Joonbum, Lee; Mauricio, Muñoz; Lex, Fridman; Trent, Victor; Bryan, Reimer; Bruce, Mehler",10.7717/peerj-cs.146,"The relationship between a driver’s glance orientation and corresponding head rotation is highly complex due to its nonlinear dependence on the individual, task, and driving context. This paper presents expanded analytic detail and findings from an effort that explored the ability of head pose to serve as an estimator for driver gaze by connecting head rotation data with manually coded gaze region data using both a statistical analysis approach and a predictive (i.e., machine learning) approach. For the latter, classification accuracy increased as visual angles between two glance locations increased. In other words, the greater the shift in gaze, the higher the accuracy of classification. This is an intuitive but important concept that we make explicit through our analysis. The highest accuracy achieved was 83% using the method of Hidden Markov Models (HMM) for the binary gaze classification problem of (a) glances to the forward roadway versus (b) glances to the center stack. Results suggest that although there are individual differences in head-glance correspondence while driving, classifier models based on head-rotation data may be robust to these differences and therefore can serve as reasonable estimators for glance location. The results suggest that driver head pose can be used as a surrogate for eye gaze in several key conditions including the identification of high-eccentricity glances. Inexpensive driver head pose tracking may be a key element in detection systems developed to mitigate driver distraction and inattention.",Human-Computer Interaction; Data Mining and Machine Learning,Head movements; Glance classification; Head-glance correspondence; Driver distraction,PeerJ Computer Science,4,29,7,2,2018
Enhancing discovery in spatial data infrastructures using a search engine,"Paolo, Corti; Athanasios Tom, Kralidis; Benjamin, Lewis",10.7717/peerj-cs.152,"A spatial data infrastructure (SDI) is a framework of geospatial data, metadata, users and tools intended to provide an efficient and flexible way to use spatial information. One of the key software components of an SDI is the catalogue service which is needed to discover, query and manage the metadata. Catalogue services in an SDI are typically based on the Open Geospatial Consortium (OGC) Catalogue Service for the Web (CSW) standard which defines common interfaces for accessing the metadata information. A search engine is a software system capable of supporting fast and reliable search, which may use ‘any means necessary’ to get users to the resources they need quickly and efficiently. These techniques may include full text search, natural language processing, weighted results, fuzzy tolerance results, faceting, hit highlighting, recommendations and many others. In this paper we present an example of a search engine being added to an SDI to improve search against large collections of geospatial datasets. The Centre for Geographic Analysis (CGA) at Harvard University re-engineered the search component of its public domain SDI (Harvard WorldMap) which is based on the GeoNode platform. A search engine was added to the SDI stack to enhance the CSW catalogue discovery abilities. It is now possible to discover spatial datasets from metadata by using the standard search operations of the catalogue and to take advantage of the new abilities of the search engine, to return relevant and reliable content to SDI users.",Human-Computer Interaction; Spatial and Geographic Information Systems,Data discovery; Catalogue Service for the Web; Metadata; WorldMap; Geoportal; Search engine; Spatial Data Infrastructure; pycsw; Solr; GeoNode,PeerJ Computer Science,4,20,7,0,2018
Decomposed multi-objective bin-packing for virtual machine consolidation,"Eli M., Dow",10.7717/peerj-cs.47,"In this paper, we describe a novel solution to the problem of virtual machine (VM) consolidation, otherwise known as VM-Packing, as applicable to Infrastructure-as-a-Service cloud data centers. Our solution relies on the observation that virtual machines are not infinitely variable in resource consumption. Generally, cloud compute providers offer them in fixed resource allocations. Effectively this makes all VMs of that allocation type (or instance type) generally interchangeable for the purposes of consolidation from a cloud compute provider viewpoint. The main contribution of this work is to demonstrate the advantages to our approach of deconstructing the VM consolidation problem into a two-step process of multidimensional bin packing. The first step is to determine the optimal, but abstract, solution composed of finite groups of equivalent VMs that should reside on each host. The second step selects concrete VMs from the managed compute pool to satisfy the optimal abstract solution while enforcing anti-colocation and preferential colocation of the virtual machines through VM contracts. We demonstrate our high-performance, deterministic packing solution generation, with over 7,500 VMs packed in under 2 min. We demonstrating comparable runtimes to other VM management solutions published in the literature allowing for favorable extrapolations of the prior work in the field in order to deal with larger VM management problem sizes our solution scales to.",Autonomous Systems; Operating Systems,Virtual machine management; Consolidation; IaaS,PeerJ Computer Science,2,26,0,9,2016
Achieving human and machine accessibility of cited data in scholarly publications,"Joan, Starr; Eleni, Castro; Mercè, Crosas; Michel, Dumontier; Robert R., Downs; Ruth, Duerr; Laurel L., Haak; Melissa, Haendel; Ivan, Herman; Simon, Hodson; Joe, Hourclé; John Ernest, Kratz; Jennifer, Lin; Lars Holm, Nielsen; Amy, Nurnberger; Stefan, Proell; Andreas, Rauber; Simone, Sacchi; Arthur, Smith; Mike, Taylor; Tim, Clark",10.7717/peerj-cs.1,"Reproducibility and reusability of research results is an important concern in scientific communication and science policy. A foundational element of reproducibility and reusability is the open and persistently available presentation of research data. However, many common approaches for primary data publication in use today do not achieve sufficient long-term robustness, openness, accessibility or uniformity. Nor do they permit comprehensive exploitation by modern Web technologies. This has led to several authoritative studies recommending uniform direct citation of data archived in persistent repositories. Data are to be considered as first-class scholarly objects, and treated similarly in many ways to cited and archived scientific and scholarly literature. Here we briefly review the most current and widely agreed set of principle-based recommendations for scholarly data citation, the Joint Declaration of Data Citation Principles (JDDCP). We then present a framework for operationalizing the JDDCP; and a set of initial recommendations on identifier schemes, identifier resolution behavior, required metadata elements, and best practices for realizing programmatic machine actionability of cited data. The main target audience for the common implementation guidelines in this article consists of publishers, scholarly organizations, and persistent data repositories, including technical staff members in these organizations. But ordinary researchers can also benefit from these recommendations. The guidance provided here is intended to help achieve widespread, uniform human and machine accessibility of deposited data, in support of significantly improved verification, validation, reproducibility and re-use of scholarly/scientific data.",Data Science; Digital Libraries; Human-Computer Interaction; World Wide Web and Web Science,Data citation; Machine accessibility; Data archiving; Data accessibility,PeerJ Computer Science,1,69,0,2,2015
Efficient online detection of temporal patterns,"Shlomi, Dolev; Jonathan, Goldfeld; Rami, Puzis; Muni, Venkateswarlu K.",10.7717/peerj-cs.53,"Identifying a temporal pattern of events is a fundamental task of online (real-time) verification. We present efficient schemes for online monitoring of events for identifying desired/undesired patterns of events. The schemes use preprocessing to ensure that the number of comparisons during run-time is minimized. In particular, the first comparison following the time point when an execution sub-sequence cannot be further extended to satisfy the temporal requirements halts the process that monitors the sub-sequence.",Algorithms and Analysis of Algorithms; Theory and Formal Methods,Pattern matching; Verification; Time series,PeerJ Computer Science,2,27,9,0,2016
PhilDB: the time series database with built-in change logging,"Andrew, MacDonald",10.7717/peerj-cs.52,"PhilDB is an open-source time series database that supports storage of time series datasets that are dynamic; that is, it records updates to existing values in a log as they occur. PhilDB eases loading of data for the user by utilising an intelligent data write method. It preserves existing values during updates and abstracts the update complexity required to achieve logging of data value changes. It implements fast reads to make it practical to select data for analysis. Recent open-source systems have been developed to indefinitely store long-period high-resolution time series data without change logging. Unfortunately, such systems generally require a large initial installation investment before use because they are designed to operate over a cluster of servers to achieve high-performance writing of static data in real time. In essence, they have a ‘big data’ approach to storage and access. Other open-source projects for handling time series data that avoid the ‘big data’ approach are also relatively new and are complex or incomplete. None of these systems gracefully handle revision of existing data while tracking values that change. Unlike ‘big data’ solutions, PhilDB has been designed for single machine deployment on commodity hardware, reducing the barrier to deployment. PhilDB takes a unique approach to meta-data tracking; optional attribute attachment. This facilitates scaling the complexities of storing a wide variety of data. That is, it allows time series data to be loaded as time series instances with minimal initial meta-data, yet additional attributes can be created and attached to differentiate the time series instances when a wider variety of data is needed. PhilDB was written in Python, leveraging existing libraries. While some existing systems come close to meeting the needs PhilDB addresses, none cover all the needs at once. PhilDB was written to fill this gap in existing solutions. This paper explores existing time series database solutions, discusses the motivation for PhilDB, describes the architecture and philosophy of the PhilDB software, and performs an evaluation between InfluxDB, PhilDB, and SciDB.",Data Science; Databases,Time series; Database; Logging; Python; Data science; Temporal,PeerJ Computer Science,2,19,5,1,2016
Computational drug repositioning based on side-effects mined from social media,"Timothy, Nugent; Vassilis, Plachouras; Jochen L., Leidner",10.7717/peerj-cs.46,"Drug repositioning methods attempt to identify novel therapeutic indications for marketed drugs. Strategies include the use of side-effects to assign new disease indications, based on the premise that both therapeutic effects and side-effects are measurable physiological changes resulting from drug intervention. Drugs with similar side-effects might share a common mechanism of action linking side-effects with disease treatment, or may serve as a treatment by “rescuing” a disease phenotype on the basis of their side-effects; therefore it may be possible to infer new indications based on the similarity of side-effect profiles. While existing methods leverage side-effect data from clinical studies and drug labels, evidence suggests this information is often incomplete due to under-reporting. Here, we describe a novel computational method that uses side-effect data mined from social media to generate a sparse undirected graphical model using inverse covariance estimation with ℓ1-norm regularization. Results show that known indications are well recovered while current trial indications can also be identified, suggesting that sparse graphical models generated using side-effect data mined from social media may be useful for computational drug repositioning.",Bioinformatics; Data Mining and Machine Learning; Computational Biology; Social Computing,Drug repositioning; Drug repurposing; Side-effect; Adverse drug reaction; Social media; Graphical model; Graphical lasso; Inverse covariance estimation,PeerJ Computer Science,2,92,6,1,2016
An algorithm for calculating top-dimensional bounding chains,"J. Frederico, Carvalho; Mikael, Vejdemo-Johansson; Danica, Kragic; Florian T., Pokorny",10.7717/peerj-cs.153,"We describe the Coefficient-Flow algorithm for calculating the bounding chain of an $(n-1)$-boundary on an $n$-manifold-like simplicial complex $S$. We prove its correctness and show that it has a computational time complexity of O(|S(n−1)|) (where S(n−1) is the set of $(n-1)$-faces of $S$). We estimate the big- $O$ coefficient which depends on the dimension of $S$ and the implementation. We present an implementation, experimentally evaluate the complexity of our algorithm, and compare its performance with that of solving the underlying linear system.",Algorithms and Analysis of Algorithms; Data Science; Scientific Computing and Simulation,Homology; Computational algebraic topology,PeerJ Computer Science,4,19,2,1,2018
Journal of Open Source Software (JOSS): design and first-year review,"Arfon M., Smith; Kyle E., Niemeyer; Daniel S., Katz; Lorena A., Barba; George, Githinji; Melissa, Gymrek; Kathryn D., Huff; Christopher R., Madan; Abigail, Cabunoc Mayes; Kevin M., Moerman; Pjotr, Prins; Karthik, Ram; Ariel, Rokem; Tracy K., Teal; Roman, Valls Guimera; Jacob T., Vanderplas",10.7717/peerj-cs.147,"This article describes the motivation, design, and progress of the Journal of Open Source Software (JOSS). JOSS is a free and open-access journal that publishes articles describing research software. It has the dual goals of improving the quality of the software submitted and providing a mechanism for research software developers to receive credit. While designed to work within the current merit system of science, JOSS addresses the dearth of rewards for key contributions to science made in the form of software. JOSS publishes articles that encapsulate scholarship contained in the software itself, and its rigorous peer review targets the software components: functionality, documentation, tests, continuous integration, and the license. A JOSS article contains an abstract describing the purpose and functionality of the software, references, and a link to the software archive. The article is the entry point of a JOSS submission, which encompasses the full set of software artifacts. Submission and review proceed in the open, on GitHub. Editors, reviewers, and authors work collaboratively and openly. Unlike other journals, JOSS does not reject articles requiring major revision; while not yet accepted, articles remain visible and under review until the authors make adequate changes (or withdraw, if unable to meet requirements). Once an article is accepted, JOSS gives it a digital object identifier (DOI), deposits its metadata in Crossref, and the article can begin collecting citations on indexers like Google Scholar and other services. Authors retain copyright of their JOSS article, releasing it under a Creative Commons Attribution 4.0 International License. In its first year, starting in May 2016, JOSS published 111 articles, with more than 40 additional articles under review. JOSS is a sponsored project of the nonprofit organization NumFOCUS and is an affiliate of the Open Source Initiative (OSI).",Data Science; Digital Libraries; Scientific Computing and Simulation; Software Engineering,Research software; Code review; Computational research; Software citation; Open-source software; Scholarly publishing,PeerJ Computer Science,4,48,8,0,2018
Give the people what they want: studying end-user needs for enhancing the web,"Tak Yeon, Lee; Benjamin B., Bederson",10.7717/peerj-cs.91,"End-user programming (EUP) is a common approach for helping ordinary people create small programs for their professional or daily tasks. Since end-users may not have programming skills or strong motivation for learning them, tools should provide what end-users want with minimal costs of learning –i.e., they must decrease the barriers to entry. However, it is often hard to address these needs, especially for fast-evolving domains such as the Web. To better understand these existing and ongoing challenges, we conducted two formative studies with Web users –a semi-structured interview study, and a Wizard-of-Oz study. The interview study identifies challenges that participants have with their daily experiences on the Web. The Wizard-of-Oz study investigates how participants would naturally explain three computational tasks to an interviewer acting as a hypothetical computer agent. These studies demonstrate a disconnect between what end-users want and what existing EUP systems support, and thus open the door for a path towards better support for end user needs. In particular, our findings include: (1) analysis of challenges that end-users experience on the Web with solutions; (2) seven core functionalities of EUP for addressing these challenges; (3) characteristics of non-programmers describing three common computation tasks; (4) design implications for future EUP systems.",Human-Computer Interaction,End-user programming; User study; Wizard-of-Oz study; World-Wide Web,PeerJ Computer Science,2,49,3,4,2016
GRNsight: a web application and service for visualizing models of small- to medium-scale gene regulatory networks,"Kam D., Dahlquist; John David N., Dionisio; Ben G., Fitzpatrick; Nicole A., Anguiano; Anindita, Varshneya; Britain J., Southwick; Mihir, Samdarshi",10.7717/peerj-cs.85,"GRNsight is a web application and service for visualizing models of gene regulatory networks (GRNs). A gene regulatory network (GRN) consists of genes, transcription factors, and the regulatory connections between them which govern the level of expression of mRNA and protein from genes. The original motivation came from our efforts to perform parameter estimation and forward simulation of the dynamics of a differential equations model of a small GRN with 21 nodes and 31 edges. We wanted a quick and easy way to visualize the weight parameters from the model which represent the direction and magnitude of the influence of a transcription factor on its target gene, so we created GRNsight. GRNsight automatically lays out either an unweighted or weighted network graph based on an Excel spreadsheet containing an adjacency matrix where regulators are named in the columns and target genes in the rows, a Simple Interaction Format (SIF) text file, or a GraphML XML file. When a user uploads an input file specifying an unweighted network, GRNsight automatically lays out the graph using black lines and pointed arrowheads. For a weighted network, GRNsight uses pointed and blunt arrowheads, and colors the edges and adjusts their thicknesses based on the sign (positive for activation or negative for repression) and magnitude of the weight parameter. GRNsight is written in JavaScript, with diagrams facilitated by D3.js, a data visualization library. Node.js and the Express framework handle server-side functions. GRNsight’s diagrams are based on D3.js’s force graph layout algorithm, which was then extensively customized to support the specific needs of GRNs. Nodes are rectangular and support gene labels of up to 12 characters. The edges are arcs, which become straight lines when the nodes are close together. Self-regulatory edges are indicated by a loop. When a user mouses over an edge, the numerical value of the weight parameter is displayed. Visualizations can be modified by sliders that adjust the force graph layout parameters and through manual node dragging. GRNsight is best-suited for visualizing networks of fewer than 35 nodes and 70 edges, although it accepts networks of up to 75 nodes or 150 edges. GRNsight has general applicability for displaying any small, unweighted or weighted network with directed edges for systems biology or other application domains. GRNsight serves as an example of following and teaching best practices for scientific computing and complying with FAIR principles, using an open and test-driven development model with rigorous documentation of requirements and issues on GitHub. An exhaustive unit testing framework using Mocha and the Chai assertion library consists of around 160 automated unit tests that examine nearly 530 test files to ensure that the program is running as expected. The GRNsight application (http://dondi.github.io/GRNsight/) and code (https://github.com/dondi/GRNsight) are available under the open source BSD license.",Bioinformatics; Graphics; Software Engineering,Gene regulatory networks; Visualization; Web application; Web service; Automatic graph layout; Best practices for scientific computing; FAIR principles; Open source; Open development,PeerJ Computer Science,2,44,5,1,2016
